{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5HBCSUd4Vca",
        "outputId": "a7f74884-7f81-4252-b740-e6e12a85802c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.11.12)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=297140119d38d6c8031eaf2ea38fc89c030a2e562e6c2d9f89d76d1077191827\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/0f/04/b17a72024b56a60e499ce1a6313d283ed5ba332407155bee03\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.16.0\n",
            "    Uninstalling h11-0.16.0:\n",
            "      Successfully uninstalled h11-0.16.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.11\n",
            "    Uninstalling idna-3.11:\n",
            "      Successfully uninstalled idna-3.11\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.3.0\n",
            "    Uninstalling h2-4.3.0:\n",
            "      Successfully uninstalled h2-4.3.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.9\n",
            "    Uninstalling httpcore-1.0.9:\n",
            "      Successfully uninstalled httpcore-1.0.9\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langgraph-sdk 0.2.10 requires httpx>=0.25.2, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 2.8.1 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "mcp 1.22.0 requires httpx>=0.27.1, but you have httpx 0.13.3 which is incompatible.\n",
            "google-genai 1.52.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "gradio-client 1.14.0 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "gradio 5.50.0 requires httpx<1.0,>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
            "langsmith 0.4.47 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "bilgi_guvenligi_qa_master.csv -> TÃ¼rkÃ§e Ã§eviri + JSONL Ã¼retim script'i\n",
        "\n",
        "YapÄ±lanlar:\n",
        "1) GitHub raw URL'den CSV indir\n",
        "2) question & answer kolonlarÄ±nÄ± TÃ¼rkÃ§eye Ã§evir (googletrans ile)\n",
        "3) question_tr & answer_tr kolonlarÄ±nÄ± ekle ve yeni CSV kaydet\n",
        "4) dataset_split'e gÃ¶re:\n",
        "   - bilgi_guvenligi_finetune_train_tr.jsonl\n",
        "   - bilgi_guvenligi_finetune_test_tr.jsonl\n",
        "   dosyalarÄ±nÄ± oluÅŸtur\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "# EÄŸer googletrans yoksa Ã¶nce:\n",
        "# pip install googletrans==4.0.0-rc1\n",
        "from googletrans import Translator\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1. Ayarlar\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "# Senin verdiÄŸin URL\n",
        "CSV_URL = \"https://raw.githubusercontent.com/guzelTemli/YapayZekaChatbot/refs/heads/main/bilgi_guvenligi_dataset/bilgi_guvenligi_qa_master.csv\"\n",
        "\n",
        "# Ã‡Ä±ktÄ± klasÃ¶rÃ¼ (istersen deÄŸiÅŸtir)\n",
        "OUTPUT_DIR = Path(\"./bilgi_guvenligi_tr_outputs\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MASTER_TR_CSV_PATH = OUTPUT_DIR / \"bilgi_guvenligi_qa_master_tr.csv\"\n",
        "TRAIN_JSONL_PATH   = OUTPUT_DIR / \"bilgi_guvenligi_finetune_train_tr.jsonl\"\n",
        "TEST_JSONL_PATH    = OUTPUT_DIR / \"bilgi_guvenligi_finetune_test_tr.jsonl\"\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2. CSV'yi indir ve oku\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "print(\"ðŸ”¹ CSV indiriliyor ve okunuyor...\")\n",
        "\n",
        "df = pd.read_csv(CSV_URL)\n",
        "\n",
        "print(\"Veri kÃ¼mesi boyutu:\", df.shape)\n",
        "print(\"Kolonlar:\", df.columns.tolist())\n",
        "print(df.head(3))\n",
        "\n",
        "# Beklenen Ã¶nemli kolonlar:\n",
        "# question, answer, question_type, is_impossible, dataset_split\n",
        "\n",
        "required_cols = {\"question\", \"answer\", \"dataset_split\"}\n",
        "missing = required_cols - set(df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Eksik kolonlar: {missing}. CSV yapÄ±sÄ±nÄ± kontrol et.\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3. Ã‡eviri fonksiyonu (Ä°ngilizce -> TÃ¼rkÃ§e)\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "translator = Translator()\n",
        "translation_cache = {}  # AynÄ± cÃ¼mleyi tekrar Ã§evirmemek iÃ§in\n",
        "\n",
        "def translate_to_tr(text: str) -> str:\n",
        "    \"\"\"Ä°ngilizce metni TÃ¼rkÃ§eye Ã§evirir. Basit cache ve hata yÃ¶netimi iÃ§erir.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "    text = text.strip()\n",
        "    if text == \"\":\n",
        "        return text\n",
        "\n",
        "    # Cache kontrolÃ¼\n",
        "    if text in translation_cache:\n",
        "        return translation_cache[text]\n",
        "\n",
        "    # Google Translate isteÄŸi\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            result = translator.translate(text, src=\"en\", dest=\"tr\")\n",
        "            tr_text = result.text\n",
        "            translation_cache[text] = tr_text\n",
        "            # Ã‡ok hÄ±zlÄ± gitmemek iÃ§in hafif bekleme\n",
        "            time.sleep(0.3)\n",
        "            return tr_text\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Ã‡eviri hatasÄ± (deneme {attempt+1}/3): {e}\")\n",
        "            time.sleep(1.0)\n",
        "\n",
        "    # 3 denemede de Ã§evirilemediyse, orijinali dÃ¶ndÃ¼r\n",
        "    print(\"âŒ Ã‡evrilemeyen metin, orijinali bÄ±rakÄ±lÄ±yor:\", text[:80], \"...\")\n",
        "    translation_cache[text] = text\n",
        "    return text\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 4. Soru & cevap kolonlarÄ±nÄ± TÃ¼rkÃ§eye Ã§evir ve yeni sÃ¼tunlara yaz\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "print(\"ðŸ”¹ Soru ve cevaplar TÃ¼rkÃ§eye Ã§evriliyor... (biraz sÃ¼rebilir)\")\n",
        "\n",
        "question_tr_list = []\n",
        "answer_tr_list = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    q_en = row[\"question\"]\n",
        "    a_en = row[\"answer\"]\n",
        "    q_tr = translate_to_tr(q_en)\n",
        "    a_tr = translate_to_tr(a_en)\n",
        "\n",
        "    question_tr_list.append(q_tr)\n",
        "    answer_tr_list.append(a_tr)\n",
        "\n",
        "    if idx % 50 == 0:\n",
        "        print(f\"  -> {idx} satÄ±r iÅŸlendi...\")\n",
        "\n",
        "df[\"question_tr\"] = question_tr_list\n",
        "df[\"answer_tr\"]   = answer_tr_list\n",
        "\n",
        "print(\"Ä°lk 3 satÄ±r (TR):\")\n",
        "print(df[[\"question\", \"question_tr\", \"answer\", \"answer_tr\"]].head(3))\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 5. TÃ¼rkÃ§eleÅŸtirilmiÅŸ master CSV'yi kaydet\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "df.to_csv(MASTER_TR_CSV_PATH, index=False, encoding=\"utf-8-sig\")\n",
        "print(f\"âœ… TÃ¼rkÃ§eleÅŸtirilmiÅŸ master CSV kaydedildi: {MASTER_TR_CSV_PATH}\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 6. Train / Test split'e gÃ¶re JSONL Ã¼ret\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def row_to_jsonl_line_tr(row) -> str:\n",
        "    \"\"\"\n",
        "    Fine-tune iÃ§in kullanÄ±lacak tek satÄ±rlÄ±k JSON:\n",
        "    {\n",
        "      \"instruction\": \"TÃ¼rkÃ§e soru\",\n",
        "      \"input\": \"\",\n",
        "      \"output\": \"TÃ¼rkÃ§e cevap\"\n",
        "    }\n",
        "    \"\"\"\n",
        "    record = {\n",
        "        \"instruction\": row[\"question_tr\"],\n",
        "        \"input\": \"\",\n",
        "        \"output\": row[\"answer_tr\"],\n",
        "    }\n",
        "    # ensure_ascii=False -> TÃ¼rkÃ§e karakterler bozulmaz\n",
        "    return json.dumps(record, ensure_ascii=False)\n",
        "\n",
        "print(\"ðŸ”¹ Train / Test JSONL dosyalarÄ± Ã¼retiliyor...\")\n",
        "\n",
        "train_df = df[df[\"dataset_split\"] == \"train\"]\n",
        "test_df  = df[df[\"dataset_split\"] == \"test\"]\n",
        "\n",
        "print(\"Train satÄ±r sayÄ±sÄ±:\", len(train_df))\n",
        "print(\"Test satÄ±r sayÄ±sÄ±:\", len(test_df))\n",
        "\n",
        "# TRAIN JSONL\n",
        "with open(TRAIN_JSONL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    for _, row in train_df.iterrows():\n",
        "        line = row_to_jsonl_line_tr(row)\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "# TEST JSONL\n",
        "with open(TEST_JSONL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    for _, row in test_df.iterrows():\n",
        "        line = row_to_jsonl_line_tr(row)\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "print(f\"âœ… Train JSONL: {TRAIN_JSONL_PATH}\")\n",
        "print(f\"âœ… Test JSONL : {TEST_JSONL_PATH}\")\n",
        "\n",
        "print(\"ðŸŽ‰ Ä°ÅŸlem tamamlandÄ±!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6t6z52V2-iT",
        "outputId": "cfcaeb1f-93b4-48e6-b694-a22b0c42b807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¹ CSV indiriliyor ve okunuyor...\n",
            "Veri kÃ¼mesi boyutu: (614, 9)\n",
            "Kolonlar: ['question_id', 'question', 'answer', 'question_type', 'source_filenames', 'source_sections', 'source_chunk_ids', 'is_impossible', 'dataset_split']\n",
            "  question_id                                           question  \\\n",
            "0      QA0001  What are some examples of activities that are ...   \n",
            "1      QA0002  What are the potential consequences for indivi...   \n",
            "2      QA0003  According to the policy, what specific securit...   \n",
            "\n",
            "                                              answer question_type  \\\n",
            "0  The policy prohibits violations of intellectua...        simple   \n",
            "1  Non-compliance with this policy may result in ...        simple   \n",
            "2  To ensure the security of wireless networks, a...       complex   \n",
            "\n",
            "                                    source_filenames        source_sections  \\\n",
            "0         SANS_Acceptable_Use_Standard_April2025.pdf             Safeguards   \n",
            "1  SANS_Safeguard_Implementation_Management_Polic...             Safeguards   \n",
            "2  SANS_Internal_Network_Access_Management_Policy...  Safeguards|Safeguards   \n",
            "\n",
            "  source_chunk_ids  is_impossible dataset_split  \n",
            "0                3          False         train  \n",
            "1                2          False         train  \n",
            "2              0|1          False         train  \n",
            "ðŸ”¹ Soru ve cevaplar TÃ¼rkÃ§eye Ã§evriliyor... (biraz sÃ¼rebilir)\n",
            "  -> 0 satÄ±r iÅŸlendi...\n",
            "  -> 50 satÄ±r iÅŸlendi...\n",
            "  -> 100 satÄ±r iÅŸlendi...\n",
            "  -> 150 satÄ±r iÅŸlendi...\n",
            "  -> 200 satÄ±r iÅŸlendi...\n",
            "  -> 250 satÄ±r iÅŸlendi...\n",
            "  -> 300 satÄ±r iÅŸlendi...\n",
            "  -> 350 satÄ±r iÅŸlendi...\n",
            "  -> 400 satÄ±r iÅŸlendi...\n",
            "  -> 450 satÄ±r iÅŸlendi...\n",
            "  -> 500 satÄ±r iÅŸlendi...\n",
            "  -> 550 satÄ±r iÅŸlendi...\n",
            "  -> 600 satÄ±r iÅŸlendi...\n",
            "Ä°lk 3 satÄ±r (TR):\n",
            "                                            question  \\\n",
            "0  What are some examples of activities that are ...   \n",
            "1  What are the potential consequences for indivi...   \n",
            "2  According to the policy, what specific securit...   \n",
            "\n",
            "                                         question_tr  \\\n",
            "0  Politikaya gÃ¶re kesinlikle yasak olan bazÄ± faa...   \n",
            "1  Bu politikayÄ± ihlal eden kiÅŸiler aÃ§Ä±sÄ±ndan pot...   \n",
            "2  Politikaya gÃ¶re, tÃ¼m kablosuz aÄŸlar iÃ§in hangi...   \n",
            "\n",
            "                                              answer  \\\n",
            "0  The policy prohibits violations of intellectua...   \n",
            "1  Non-compliance with this policy may result in ...   \n",
            "2  To ensure the security of wireless networks, a...   \n",
            "\n",
            "                                           answer_tr  \n",
            "0  Politika, telif hakkÄ±yla korunan materyalin iz...  \n",
            "1  Bu politikaya uyulmamasÄ±, zorunlu tazeleme eÄŸi...  \n",
            "2  Kablosuz aÄŸlarÄ±n gÃ¼venliÄŸini saÄŸlamak iÃ§in tÃ¼m...  \n",
            "âœ… TÃ¼rkÃ§eleÅŸtirilmiÅŸ master CSV kaydedildi: bilgi_guvenligi_tr_outputs/bilgi_guvenligi_qa_master_tr.csv\n",
            "ðŸ”¹ Train / Test JSONL dosyalarÄ± Ã¼retiliyor...\n",
            "Train satÄ±r sayÄ±sÄ±: 521\n",
            "Test satÄ±r sayÄ±sÄ±: 93\n",
            "âœ… Train JSONL: bilgi_guvenligi_tr_outputs/bilgi_guvenligi_finetune_train_tr.jsonl\n",
            "âœ… Test JSONL : bilgi_guvenligi_tr_outputs/bilgi_guvenligi_finetune_test_tr.jsonl\n",
            "ðŸŽ‰ Ä°ÅŸlem tamamlandÄ±!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"bilgi_guvenligi_tr_outputs/bilgi_guvenligi_qa_master_tr.csv\")\n",
        "files.download(\"bilgi_guvenligi_tr_outputs/bilgi_guvenligi_finetune_train_tr.jsonl\")\n",
        "files.download(\"bilgi_guvenligi_tr_outputs/bilgi_guvenligi_finetune_test_tr.jsonl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "9hLpKn3f_eIT",
        "outputId": "1d8726a8-babd-489e-8aae-26412d7e92b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_823b45c2-56f3-4b0d-bbb6-58a311509a8f\", \"bilgi_guvenligi_qa_master_tr.csv\", 490390)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3e59c878-4c1b-4686-af67-0aea4ed735a2\", \"bilgi_guvenligi_finetune_train_tr.jsonl\", 211370)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c7b41e52-a2a5-4239-8dec-dd5ec31fa0c5\", \"bilgi_guvenligi_finetune_test_tr.jsonl\", 37906)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}