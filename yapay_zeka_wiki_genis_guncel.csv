baslik,icerik
Yapay zekâ,"Yapay zekâ (İngilizce: Artificial intelligence, AI); hesaplama sistemlerinin öğrenme, akıl, problem çözme, algılama ve karar verme gibi insan zekâsıyla tipik olarak ilişkilendirilen görevleri yerine getirme yeteneğidir. Dördüncü Sanayi Devrimi'nin en yaygın özelliklerinden biri olarak kabul edilir. İlk ve ikinci kategoriler arasındaki ayrım genellikle seçilen kısaltmayla ortaya çıkar. Güçlü yapay zeka genellikle yapay genel zekâ (İngilizce: Artificial general intelligence kelimelerinin kısaltılmışı olarak: AGI) olarak etiketlenirken, doğal zekayı taklit etme girişimleri yapay biyolojik zekâ (İngilizce: Artificial biological intelligence: ABI) olarak adlandırılır. Önde gelen yapay zekâ ders kitapları, alanı zeki etmenlerin çalışması olarak tanımlar: Çevresini algılayan ve hedeflerine başarıyla ulaşma şansını en üst düzeye çıkaran eylemleri gerçekleştiren herhangi bir cihaz. Halk arasında, yapay zekâ kavramı genellikle insanların insan zihni ile ilişkilendirdiği öğrenme ve problem çözme gibi bilişsel eylemleri taklit eden makineleri tanımlamak için kullanılır.
Makineler daha becerikli hâle geldikçe, zekâ gerektirdiği düşünülen görevler genellikle yapay zekâ etkisi olarak bilinen bir fenomen olan yapay zekâ tanımından çıkarılır. Tesler'in teoremindeki bir espri, ""Yapay zekâ henüz yapılmamış şeydir"" der. Örneğin, optik karakter tanıma yapay zekâ olarak değerlendirilen şeylerin dışında tutulur, rutin teknoloji hâline gelir.
Genellikle yapay zekâ olarak sınıflandırılan modern makine yetenekleri satranç ve Go gibi stratejik oyun sistemlerinde, en üst düzeyde rekabet eden insan konuşmasını anlama, poker ya da otonom arabalar gibi kusurlu-bilgi oyunlarını içerik dağıtım ağındaki akıllı yönlendirmeyi ve askeri simülasyonları kapsar.
Yapay zekâ çalışmaları sıklıkla insanın düşünme yöntemlerini taklit eden yapay algoritmalar geliştirmeye yöneliktir, ancak bununla sınırlı değildir. Öğrenebilen ve gelecekte insan zekâsından bağımsız gelişebilecek bir yapay zekâ kavramına doğru yeni yönelimler oluşmaktadır. Bu yönelim, insanın evreni ve doğayı anlama çabasında kendisine yardımcı olabilecek belki de kendisinden daha zeki, insan ötesi varlıklar meydana getirme düşünün bir ürünüdür. Bu düş, 1920'li yıllarda yazılan ve sonraları Isaac Asimov'u etkileyen modern bilimkurgu edebiyatının öncü yazarlarından Karel Čapek'in eserlerinde dışa vurmuştur. Karel Čapek, R.U.R. adlı tiyatro oyununda yapay zekâya sahip robotlar ile insanlığın ortak toplumsal sorunlarını ele alarak 1920 yılında yapay zekânın insan aklından bağımsız gelişebileceğini öngörmüştür.

Tanım
Yapay zekâ, idealleştirilmiş bir perspektife göre insan zekâsına özgü yüksek bilişsel fonksiyonları veya otonom davranışları sergileyen bir yapay işletim sistemidir. Bu sistem, algılama, öğrenme, çoğul kavramları bağlama, düşünme, fikir yürütme (belirtme), sorun çözme, iletişim kurma ve karar verme gibi yeteneklere sahip olmalıdır. Ayrıca, bu yapay zekâ sistemi düşüncelerinden tepkiler üretebilmeli (eyleyici yapay zekâ) ve bu tepkileri fiziksel olarak dışa vurabilmelidir.

Hedefler
Zekâyı simüle etme (veya oluşturma) genel problemi, alt problemlere ayrılmıştır. Bu alt problemler, araştırmacıların zeki bir sistemden sergilemesini beklediği belirli özellikler veya yeteneklerden oluşur. Aşağıda açıklanan özellikler, yapay zekâ araştırmalarında en çok dikkat çeken konular arasında yer almakta ve bu araştırmaların kapsamını belirlemektedir.

Akıl yürütme ve problem çözme
Yapay zekânın ilk araştırmacıları, insanların bulmacaları çözerken veya mantıksal çıkarımlar yaparken kullandığı adım adım akıl yürütmeyi taklit eden algoritmalar geliştirmiştir. 1980'lerin sonları ve 1990'larda, belirsiz veya eksik bilgiyle başa çıkmak için olasılık ve ekonomi kavramlarını kullanan yöntemler geliştirilmiştir.
Bu algoritmaların birçoğu, büyük ölçekli akıl yürütme problemlerini çözmek için yetersizdir, çünkü ""kombinatoryal patlama"" denilen bir durum yaşarlar: Problemler büyüdükçe bu algoritmaların çalışması üstel bir şekilde yavaşlar. Hatta insanlar bile erken dönem yapay zekânın modelleyebildiği adım adım akıl yürütmeyi nadiren kullanır. İnsanlar, çoğu sorunlarını hızlı ve sezgisel yargılarla çözerler. Doğru ve verimli akıl yürütme, hâlâ çözülememiş bir problemdir.

Bilgi temsili
Bilgi temsili ve bilgi mühendisliği, yapay zekâ programlarının sorulara akıllıca yanıt vermesini ve gerçek dünya ile ilgili çıkarımlarda bulunmasını sağlar. Formel bilgi temsilleri; içerik tabanlı indeksleme ve bilgi erişimi, sahne yorumlama, klinik karar destek sistemleri, bilgi keşfi (büyük veritabanlarından ""ilginç"" ve eyleme geçirilebilir çıkarımlarda bulunma) gibi birçok alanda kullanılmaktadır.

Tarihçe
Yapay zekâ tarihi, antik çağlarda, usta zanaatkarlar tarafından zeka veya bilinç kazandırılan yapay varlıklara ilişkin mitler, hikâyeler ve söylentilerle başladı. Mekanik ya da ""formel"" akıl yürütme üzerine çalışmalar, antik çağda filozoflar ve matematikçilerle birlikte devam etti. Mantık alanındaki bu çalışmalar, Alan Turing'in algoritmalar teorisinin temelini oluşturdu. Turing'in teorisi, ""0"" ve ""1"" gibi basit sembolleri kullanarak bir makinenin, insan aklının gerçekleştirebileceği her türlü matematiksel akıl yürütmeyi simüle edebileceğini öne sürdü. Bu teori, sibernetik, bilgi teorisi ve nörobiyoloji gibi alanlardaki keşiflerle birleşerek, araştırmacılara ""elektronik bir beyin"" inşa etme olasılığını düşünmeye yöneltti. Öne çıkan çalışmalar arasında, 1943'teki Warren McCullouch ve Walter Pitts'in ""yapay nöronlar"" tasarımı ve Turing'in 1950'de yayımladığı, ""makine zekâsının"" mümkün olduğunu gösterdiği Turing testini tanıtan Bilgi İşlem Makineleri ve Zekâ adlı makalesi bulunmaktaydı. Bu dönemde yapılan bu tür araştırmalar, yapay zekânın temel alanlarının ortaya çıkardı.
1956'da Dartmouth College'deki bir atölye çalışmasında yapay zeka araştırma alanı kuruldu. Bu atölyeye katılan araştırmacılar, 1960'larda yapay zekâ alanına liderlik ettiler. Öğrencileriyle beraber bu araştırmacılar, basının ""şaşırtıcı"" olarak nitelendirdiği projeler ürettiler. Bu dönemde bilgisayarlar, dama stratejileri öğrenmiş, cebir problemlerini çözmüş, mantıksal teoremleri kanıtlamış ve İngilizce konuşmuştu. 1950'lerin sonu ve 1960'ların başlarında, hem İngiltere'deki hem de Amerika Birleşik Devletleri'ndeki birçok üniversitede yapay zekâ laboratuvarları kuruldu.
1960'lar ve 1970'lerdeki araştırmacılar, genel zekâya sahip bir makine üretmenin mümkün olduğuna inanıyor ve bunu alanlarının nihai hedefi olarak görüyordu. 1965'te Herbert Simon, ""makinelerin yirmi yıl içinde bir insanın yapabileceği her işi yapabilir hâle geleceği"" tahmininde bulunmuş, 1967'de Marvin Minsky, ""bir nesil içinde ... 'yapay zekâ' sorununun büyük ölçüde çözüleceği"" yorumunu yapmıştı. Ancak bu öngörüler, yapay zekânın geliştirilmesindeki zorlukları göz ardı etmişti. 1974'te Sir James Lighthill'ın eleştirileri ve ABD Kongresi'nin daha somut projelere fon sağlama baskısı nedeniyle, ABD ve İngiltere hükûmetleri keşif amaçlı yapay zekâ araştırmalarına verilen desteği durdurma kararı aldı. Minsky ve Seymour Papert'ın Perceptrons adlı kitabı, yapay sinir ağlarının gerçek dünya problemlerini çözmede etkisiz olduğunu savunmuş ve bu yaklaşım, itibarını kaybetmişti. Bunun sonucunda, yapay zekâ projelerine fon bulmanın zorlaştığı ""yapay zekâ kışı"" adı verilen bir dönem başladı.
1980'lerin başında yapay zekâ araştırmaları, uzman sistemlerin ticari başarısıyla yeniden canlandı. Uzman sistemler, uzmanların bilgi ve analiz yeteneklerini taklit eden bir tür yapay zekâ programıydı. 1985 yılına gelindiğinde, yapay zekâ pazarı 1 milyar doları aşmıştı. Aynı dönemde Japonya'nın beşinci nesil bilgisayar projesi, ABD ve Britanya hükûmetlerini akademik araştırmalara yeniden fon sağlamaya teşvik etti. Ancak, 1987'de Lisp makinesi pazarının çöküşüyle yapay zekâ tekrar itibar kaybetti ve daha uzun süren ikinci bir ""yapay zekâ kışı"" başladı.
O zamana kadar yapay zekâ projelerinin büyük kısmı, planlar, hedefler, inançlar ve bilinen gerçekler gibi zihinsel kavramları temsil etmek için yüksek seviyede semboller kullanmaya odaklanmıştı. Ancak 1980'lerde bazı araştırmacılar, bu yaklaşımın insan bilişinin tüm süreçlerini, özellikle algı, robotik, öğrenme ve örüntü tanıma gibi karmaşık süreçleri taklit edemeyeceği konusunda şüphe duymaya başladı. Bu nedenle, ""alt sembolik"" yöntemlere yöneldiler. Rodney Brooks, genel olarak ""temsil"" fikrini reddederek hayatta kalabilen ve hareket edebilen makineler geliştirmeye odaklandı. Judea Pearl ve Lofti Zadeh gibi isimler, eksik ve belirsiz bilgileri mantıksal kesinlik yerine mâkul tahminlerle işleyebilen yöntemler geliştirdi. Ayrıca Geoffrey Hinton ve diğer araştırmacılar, ""bağlantısallık"" ve sinir ağ araştırmalarını yeniden canlandırarak bu alanın gelişimine katkıda bulundu. 1990'da Yann LeCun, evrişimli sinir ağlarının el yazısı ile yazılmış rakamları tanıyabildiğini gösterdi; bu, sinir ağlarının birçok başarılı uygulamasından ilkiydi.
Yapay zekâ, 1990'ların sonları ve 21. yüzyılın başlarında, formal matematik yöntemlerinden yararlanarak ve belirli problemlere özel çözümler geliştirerek itibarını yeniden kazanmaya başladı. Bu ""dar"" ve ""formal"" yaklaşım, araştırmacıların doğrulanabilir sonuçlar üretmesini ve istatistik, ekonomi ve matematik gibi diğer alanlarla iş birliği yapmasını sağladı. 2000'li yıllara gelindiğinde, yapay zekâ araştırmacılarının geliştirdiği çözümler yaygın olarak kullanılmaya başlanmıştı. Ancak 1990'larda bu çözümler genellikle ""yapay zekâ"" olarak adlandırılmıyordu (bu eğilim yapay zekâ etkisi olarak bilinir).
Buna karşın, bazı akademisyenler yapay zekânın ilk hedefinden, yani çok yönlü ve tam anlamıyla zeki makineler yaratma amacından uzaklaştığını düşünmeye başladı. 2002'nin başlarında, bu hedefi yeniden canlandırmak amacıyla yapay genel zekâ (""AGI"") adı verilen bir alt alan kuruldu. 2010'lara gelindiğinde, yapay genel zekâ araştırmaları için iyi finanse edilen birçok kurum ortaya çıktı.
Derin öğrenme, 2012'de endüstri standartlarında önemli bir üstünlük sağlayarak yapay zekâ alanında yaygın bir şekilde benimsendi.
Birçok spesifik görevde diğer yöntemler terk edildi.
Bu başarının arkasında daha hızlı bilgisayarlar, grafik işlem birimleri, bulut bilişim gibi donanım geliştirmeleriyle ImageNet gibi dikkatle oluşturulmuş veri kümelerinin de dâhil olduğu büyük miktarda veriye erişim yatıyordu. Derin öğrenmenin bu yükselişi, yapay zekâya olan ilgi ve finansmanda büyük bir artışı tetikledi. 2015-2019 yılları arasında makine öğrenimi araştırmalarındaki toplam yayımlanma sayısında %50'lik bir artış görüldü.
2016'da, adalet ve teknolojinin yanlış kullanımı gibi etik sorunlar, makine öğrenimi konferanslarında ana gündem maddesi hâline geldi. Bu konularda yapılan yayımlar hızla artarken, yeni finansman kaynakları sağlandı ve birçok araştırmacı kariyerlerini bu alanlara yönlendirdi. Aynı dönemde, yapay zekânın insan çıkarlarına uygun şekilde geliştirilmesini konu alan ""uyum problemi"" akademik bir çalışma alanı olarak ciddiyet kazandı.
2010'ların sonları ve 2020'lerin başlarında, yapay genel zekâ üzerine çalışan şirketler dikkat çeken yazılımlar üretmeye başladı. 2015 yılında DeepMind tarafından geliştirilen AlphaGo, oyunun kuralları öğretilerek kendi stratejisini geliştirdi ve dünya şampiyonu Go oyuncusunu yendi. 2020'de OpenAI tarafından çıkarılan GPT-3, insan benzeri yüksek kaliteli metinler üretebilen bir geniş dil modeli olarak dikkat çekti. 30 Kasım 2022'de kullanıma sunulan ChatGPT ise iki ay içinde 100 milyon kullanıcıya ulaşarak tarihin en hızlı büyüyen tüketici yazılımı oldu. 2022 yılı, yapay zekânın geniş kitleler tarafından fark edildiği ve yaygın bir şekilde konuşulmaya başlandığı bir dönüm noktası olarak kabul edildi. Bu yazılımlar, büyük bir yapay zeka patlamasını tetikledi. Büyük ölçekli şirketler yapay zekâ araştırmalarına milyarlarca dolar yatırım yapmaya başladı. AI Impacts'in verilerine göre, 2022 yılında sadece ABD'de yapay zekâya yıllık yaklaşık $50 milyar yatırım yapıldı. ABD'deki yeni bilgisayar bilimi doktora mezunlarının yaklaşık %20'si yapay zekâ üzerine uzmanlaştı. Aynı yıl, ABD'de yapay zekâ ile ilgili 800.000 iş ilanı bulundu. PitchBook araştırmasına göre, 2024'te yeni fon alan girişimlerin %22'si kendilerini yapay zekâ şirketi olarak tanımlıyordu.

Gelişim süreci
İlk araştırmalar ve yapay sinir ağları
İdealize edilmiş tanımıyla yapay zekâ konusundaki ilk çalışmalardan biri McCulloch ve Pitts tarafından yapılmıştır. Bu araştırmacıların önerdiği, yapay sinir hücrelerini kullanan hesaplama modeli, önermeler mantığı, fizyoloji ve Turing'in hesaplama kuramına dayanıyordu. Herhangi bir hesaplanabilir fonksiyonun sinir hücrelerinden oluşan ağlarla hesaplanabileceğini ve mantıksal ve ve veya işlemlerinin gerçekleştirilebileceğini gösterdiler. Bu ağ yapılarının uygun şekilde tanımlanmaları hâlinde öğrenme becerisi kazanabileceğini de ileri sürdüler. Hebb, sinir hücreleri arasındaki bağlantıların şiddetlerini değiştirmek için basit bir kural önerince, öğrenebilen yapay sinir ağlarını gerçekleştirmek de olası hale gelmiştir.
1950'lerde Shannon ve Turing bilgisayarlar için satranç programları yazıyorlardı. İlk yapay sinir ağı temelli bilgisayar SNARC, MIT'de Minsky ve Edmonds tarafından 1951'de yapıldı. Çalışmalarını Princeton Üniversitesi'nde sürdüren Mc Carthy, Minsky, Shannon ve Rochester'le birlikte 1956 yılında Dartmouth'da iki aylık bir açık çalışma düzenledi. Bu toplantıda birçok çalışmanın temelleri atılmakla birlikte, toplantının en önemli özelliği Mc Carthy tarafından önerilen yapay zekâ adının konmasıdır. İlk kuram ispatlayan programlardan Logic Theorist (Mantık kuramcısı) burada Newell ve Simon tarafından tanıtılmıştır.

Yeni yaklaşımlar
Daha sonra Newell ve Simon, insan gibi düşünme yaklaşımına göre üretilmiş ilk program olan Genel Sorun Çözücü (General Problem Solver)'ı geliştirmişlerdir. Simon, daha sonra fiziksel simge varsayımını ortaya atmış ve bu kuram, insandan bağımsız zeki sistemler yapma çalışmalarıyla uğraşanların hareket noktasını oluşturmuştur. Simon'ın bu tanımlaması bilim adamlarının yapay zekâya yaklaşımlarında iki farklı akımın ortaya çıktığını belirginleştirmesi açısından önemlidir: Sembolik Yapay Zekâ ve Sibernetik Yapay Zekâ.

Yaklaşımlar ve eleştiriler
Sembolik yapay zekâ
Simon'ın sembolik yaklaşımından sonraki yıllarda mantık temelli çalışmalar egemen olmuş ve programların başarımlarını göstermek için bir takım yapay sorunlar ve dünyalar kullanılmıştır. Daha sonraları bu sorunlar gerçek yaşamı hiçbir şekilde temsil etmeyen oyuncak dünyalar olmakla suçlanmış ve yapay zekânın yalnızca bu alanlarda başarılı olabileceği ve gerçek yaşamdaki sorunların çözümüne ölçeklenemeyeceği ileri sürülmüştür.
Geliştirilen programların gerçek sorunlarla karşılaşıldığında çok kötü bir başarım göstermesinin ardındaki temel neden, bu programların yalnızca sentaktik süreçleri benzeşimlendirerek anlam çıkarma, bağlantı kurma ve fikir yürütme gibi süreçler konusunda başarısız olmasıydı. Bu dönemin en ünlü programlarından Weizenbaum tarafından geliştirilen Eliza, karşısındaki ile sohbet edebiliyor gibi görünmesine karşın, yalnızca karşısındaki insanın cümleleri üzerinde bazı işlemler yapıyordu. İlk makine çevirisi çalışmaları sırasında benzeri yaklaşımlar kullanılıp çok gülünç çevirilerle karşılaşılınca bu çalışmaların desteklenmesi durdurulmuştu. Bu yetersizlikler aslında insan beynindeki semantik süreçlerin yeterince incelenmemesinden kaynaklanmaktaydı.

Sibernetik yapay zekâ
Yapay sinir ağları çalışmalarının dahil olduğu sibernetik cephede de durum aynıydı. Zeki davranışı benzeşimlendirmek için bu çalışmalarda kullanılan temel yapılardaki bazı önemli yetersizliklerin ortaya konmasıyla birçok araştırmacılar çalışmalarını durdurdular. Buna en temel örnek, Yapay sinir ağları konusundaki çalışmaların Marvin Minsky ve Seymour Papert'in 1969'da yayınlanan Perceptrons adlı kitaplarında tek katmanlı algaçların bazı basit problemleri çözemeyeceğini gösterip aynı kısırlığın çok katmanlı algaçlarda da beklenilmesi gerektiğini söylemeleri ile bıçakla kesilmiş gibi durmasıdır.
Sibernetik akımın uğradığı başarısızlığın temel sebebi de benzer şekilde Yapay Sinir Ağının tek katmanlı görevi başarması fakat bu görevle ilgili vargıların veya sonuçların bir yargıya dönüşerek diğer kavramlar ile bir ilişki kurulamamasından kaynaklanmaktadır. Bu durum aynı zamanda semantik süreçlerin de benzeşimlendirilememesi gerçeğini doğurdu.

Uzman sistemler
Her iki akımın da uğradığı başarısızlıklar, her sorunu çözecek genel amaçlı sistemler yerine belirli bir uzmanlık alanındaki bilgiyle donatılmış programları kullanma fikrinin gelişmesine sebep oldu ve bu durum yapay zekâ alanında yeniden bir canlanmaya yol açtı. Kısa sürede Uzman sistemler adı verilen bir metodoloji gelişti.
Uzman sistemler bir konuda belli ön koşullar aynı anda var olduğunda konunun bir uzmanın (bazen ne olasılıkla) ne karar alacağını belirleyen kuralların tümünü içeren bir programı gelen problemlere uygulamak temellidir. Bunun bir avantajı her verilen kararın hangi kurallar uygulanarak verildiğinin kolayca bilinmesi idi. Bu birçok kuralcı bürokratik karar örgütleri için kolayca uygulamalar geliştirilebilmesi demekti. Bu doğal olarak bir otomobilin tamiri için önerilerde bulunan uzman sistem programının otomobilin ne işe yaradığından haberi olmaması da demekti. Buna rağmen uzman sistemlerin başarıları beraberinde ilk ticari uygulamaları da getirdi.
Yapay zekâ yavaş yavaş bir endüstri hâline geliyordu. DEC tarafından kullanılan ve müşteri siparişlerine göre donanım seçimi yapan R1 adlı uzman sistem şirkete bir yılda 40 milyon dolarlık tasarruf sağlamıştı. Birden diğer ülkeler de yapay zekâyı yeniden keşfettiler ve araştırmalara büyük kaynaklar ayrılmaya başlandı. 1988'de yapay zekâ endüstrisinin cirosu 2 milyar dolara ulaşmıştı.

Doğal dil işleme
Antropoloji bilimi, gelişmiş insan zekâsı ile dil arasındaki bağlantıyı gözler önüne serdiğinde, dil üzerinden yürütülen yapay zekâ çalışmaları tekrar önem kazandı. İnsan zekâsının doğrudan doğruya kavramlarla düşünmediği, dil ile düşündüğü, dil kodları olan kelimeler ile kavramlar arasında bağlantı kurduğu anlaşıldı. Bu sayede insan aklı kavramlar ile düşünen Hayvan beyninden daha hızlı işlem yapabilmekteydi ve dil dizgeleri olan cümleler yani şablonlar ile etkili bir öğrenmeye ve bilgisini soyut olarak genişletebilme yeteneğine sahip olmuştu. İnsanların iletişimde kullandıkları Türkçe, İngilizce gibi doğal dilleri anlayan bilgisayarlar konusundaki çalışmalar hızlanmaya başladı. Önce, yine Uzman sistemler olarak karşımıza çıkan doğal dil anlayan programlar, daha sonra Sembolik Yapay Zekâ ile ilgilenenler arasında ilgiyle karşılandı ve yazılım alanındaki gelişmeler sayesinde İngilizce olan A.I.M.L (Artificial intelligence Markup Language) ve Türkçe T.Y.İ.D (Türkçe Yapay Zekâ İşaretleme Dili) gibi bilgisayar dilleri ile sentaktik (Örüntü) işlemine uygun veri erişim metotları geliştirilebildi. Bugün Sembolik Yapay Zekâ araştırmacıları özel Yapay Zekâ dillerini kullanarak verileri birbiri ile ilişkilendirebilmekte, geliştirilen özel prosedürler sayesinde anlam çıkarma ve çıkarımsama yapma gibi ileri seviye bilişsel fonksiyonları benzetimlendirmeye çalışmaktadırlar.
Bütün bu gelişmelerin ve süreçlerin sonunda bir grup yapay zekâ araştırmacısı, insan gibi düşünebilen sistemleri araştırmaya devam ederken diğer bir grup ise ticari değeri olan rasyonel karar alan sistemler (Uzman sistemler) üzerine yoğunlaştı.

Diyalog bazlı yapay zeka
Doğal dil işleme ve makine öğrenmesi gibi yapay zeka teknolojileri kullanılarak insan ve makine (yazılım) arasında bir diyaloğun sürdürülmesini sağlayan yapay zeka alt dalına ""diyalog bazlı yapay zeka"" (conversational artificial intelligence) denir. Daha önce insanların bilgisayara komut vermesinde kullanılan web, mobil uygulama gibi grafiksel arayüzlerin (GUI) yerine geçmeyi amaçlayan diyalog bazlı arayüzler (CUI) insanların bilgisayara günlük dilde yazarak veya konuşarak komut verebilmesini amaçlar. Günümüzde, chatbotlar ve sesli asistanlar diyalog bazlı yapay zeka alanında sıkça kullanılan teknolojik ürünler olarak karşımıza çıkmaktadır.

Chatbotlar
Chatbotlar, diyalog bazlı yapay zekanın günlük hayatta kullanılan bir örneğidir. Kullanıcılar, Türkçeye sohbet robotları olarak geçmiş bu dijital ürünler ile yazışarak belirli bir konuda bilgi alabilir veya uçak bileti almak, banka havalesi yapmak veya bir kitap satın almak gibi günlük işlerini yapabilirler. Chatbotlar, şirketlerin web sitesinde veya mobil uygulamasında yer alabilirler. Bunun dışında chatbotlar, WhatsApp, Facebook Messenger gibi genel mesajlaşma platformlarında veya Google Assistant, Siri gibi sesli asistanlarda da yer alabilirler.
Chatbotlar kullanıcı ile etkileşim kurma yöntemini, arkasında yer alan teknolojik altyapıya göre farklı çeşitlerde oluşturulabilir. Örneğin bir chatbot kullanıcı ile, sadece kullanıcı onunla etkileşime girdiğinde iletişim kuruyorsa reaktif bir chatbottur, eğer bir uyarıcı ile tetiklenerek kullanıcı ile olan diyaloğu başlatan taraf oluyorsa buna proaktif bir chatbot denir. Teknoloji açısından bakılacak olursa, yapay zeka tabanlı chatbotların yanında, doğal dil işleme, makine öğrenmesi gibi yapay zeka teknolojileri kullanılmadan geliştirilen kural tabanlı chatbotlar da kullanılmaktadır. Ancak bu iki tür chatbotun davranışı farklıdır. Kural bazlı chatbotlarda genellikle kullanıcıya belirli seçenekler sunulur ve yaratılan deneyim bu seçeneklerle sınırlı kalır. Yapay zeka tabanlı chatbotlarda ise kullanıcı serbest bir metin yazabilir, chatbotun doğal dil işleme teknolojisi bu metni anlamlandırıp doğru yanıtı belirleyerek kullanıcıya sunar.

Gelecekte yapay zekâ
Süper zekâ ve tekillik
Süper zekâ, en parlak ve en yetenekli insan zihninin zekâsını çok aşan bir zekâya sahip varsayımsal bir etkendir. Yapay genel zekâ üzerine yapılan araştırmalar yeterince zeki bir yazılım üretirse, yazılım kendini yeniden programlayabilir ve geliştirebilir. Gelişen yazılım kendini geliştirmede daha da iyi olur ve I. J. Good'un ""zekâ patlaması"" ve Vernor Vinge'in ""tekillik"" olarak adlandırdığı şeye yol açar.
Ancak teknolojiler sonsuza kadar üstel olarak gelişemez ve genellikle S şeklinde bir eğri izler, teknolojinin yapabileceği şeyin fiziksel sınırlarına ulaştığında yavaşlar.

Transhümanizm
Robot tasarımcısı Hans Moravec, sibernetikçi Kevin Warwick ve mucit Ray Kurzweil, gelecekte insanların ve makinelerin birleşerek her ikisinden de daha yetenekli ve güçlü siborglara dönüşebileceğini öngördüler. Transhümanizm olarak adlandırılan bu fikrin kökleri, Aldous Huxley ve Robert Ettinger'ın yazılarında bahsedilmektedir.
Edward Fredkin, ""yapay zekânın evrimin bir sonraki adımı"" olduğunu savunmaktaydı; bu fikir ilk olarak Samuel Butler'ın ""Darwin among the Machines"" adlı mektubunda 1863'te ortaya atıldı ve George Dyson tarafından 1998'deki Darwin Among the Machines: The Evolution of Global Intelligence adlı kitabında geliştirildi.

Yapay zekânın gücü
Bilişim uzmanları, bir insanın hepsi aynı anda paralel olarak çalışan 100 milyar nöron bağlantısının toplam hesap gücünün alt sınırı olan saniyede 10 katrilyon (1.000.000.000.000.000 = 
  
    
      
        
          10
          
            15
          
        
      
    
    {\displaystyle 10^{15}}
  
) hesap düzeyine 2025'te erişeceğini düşünüyorlar.
Beynin bellek kapasitesine gelince, 100 trilyon bağlantının her birine 10.000 bit bilgi depolama gereksinimi tanınırsa, toplam kapasite 10^18 düzeyine çıkıyor. 2020'ye gelindiğinde insan beyninin işlevselliğine erişmiş bir bilgisayarın fiyatının 1000 dolar olacağı tahmin ediliyor. 2030'da 1000 dolarlık bir bilgisayarın bellek kapasitesi 1000 insanın belleğine eşit olacak.

Uygulama alanları
Yapay zekanın uygulama alanlarının bazı örnekleri şu şekildedir:

Önerici sistemler: Kullanıcıların geçmiş davranışlarına dayanarak yeni içerik önerilmesi. Örneğin, sosyal medya sitelerinde yeni arkadaş, mağazalarda başka bir ürün, gazetede başka bir haber önerileri.
Makine çevirisi: Bir dilde ifade edilen cümleyi farklı bir dile çevirmek. Örneğin, Google Translate, Microsoft Tercüman ve Yandex.Çeviri gibi çevrimiçi araçlar.
Sinyal işleme: Ses ve görüntü gibi sinyallerin işlenerek bilgi çıkarımı. Örneğin, yüz ve ses tanıma.
Prosedürel içerik üretimi: Rassal yöntemler kullanarak yapay içerik üretme. Örneğin, üretimsel müzik ve video oyunlarında prosedürel dünyalar.
Regresyon analizi: Geçmiş verilere dayanılarak bir değişkenin gelecekteki değerinin tahmin edilmesi. Örneğin, ekonomik öngörüler, üretim miktarı öngörüleri.
Görüntü işleme: Dijital görüntülerde bulunan objeleri tanıma, yerini bulma, sınıflandırma gibi işlemlerin tümü. Yapay zekadan önce bu işlemler Hough dönüşümü gibi kurala dayalı algoritmalar ile sürdürülürken, günümüzde bu kurallar veriden öğrenilmektedir. Görüntülemenin sık kullanıldığı tıp, biyoloji, otomotiv, üretim gibi alanlarda kullanılmaktadır.
Makale yazma: Dünyada yapay zeka ile yazılan ilk köşe yazısı 8 Eylül 2020 tarihinde The Guardian gazetesinde yayınlanmıştır. Türkiye'de yapay zekanın yazdığı ilk haber ise Şalom gazetesinde 2018 yılında yayınlanmıştır.

Alt dallar
Makine Zekâsı (Sembolik Yapay Zekâ)
Yapay Sinir Ağları (Sibernetik Yapay Zekâ)
Doğal Dil işleme (Dil ile düşünme)
Görüntü İşleme
Konuşma Sentezi (Yapay Konuşma)
Konuşma Anlama (Konuşma Analizi)
Tümevarımlı mantık programlama
Bilgi-tabanlı sistem
Bilgisayarlı görü
Doğal dil üretme
Uzman sistemler
Sinirsel şebeke
Makine öğrenimi
Örüntü Tanıma
Genetik Algoritmalar
Genetik Programlama
Bulanık Mantık
Çoklu Örnekle Öğrenme (Multiple Instance Learning)

Ayrıca bakınız
Notlar


== Kaynakça =="
Makine öğrenimi,"Makine öğrenimi (ML), veriden öğrenebilen ve görünmeyen verilere genelleştirebilen ve dolayısıyla açık talimatlar olmadan görevleri yerine getirebilen istatistiksel algoritmaların geliştirilmesi ve incelenmesiyle ilgilenen, yapay zekâda akademik bir disiplindir. Makine öğrenimi, bilgisayarların deneyimlerinden öğrenerek karmaşık görevleri otomatikleştirmeyi sağlayan bir yapay zeka alanıdır. Bu, veri analizi yaparak örüntüler tespit etme ve tahminlerde bulunma yeteneğine dayanır. Son zamanlarda yapay sinir ağları, performans açısından önceki birçok yaklaşımı geride bırakmayı başardı.
Makine öğrenimi yaklaşımları, doğal dil işleme, bilgisayar görüşü, konuşma tanıma, e-posta filtreleme, tarım ve tıp dahil olmak üzere birçok alana uygulanmıştır. Bu teknikler, genellikle tahmine dayalı analitik olarak tanımlanan iş sorunlarına yönelik uygulamalarda önemli bir rol oynamaktadır. ML, iş sorunlarına yönelik uygulamasında tahmine dayalı analitik denir. Makine öğreniminin tümü istatistiksel temelli olmasa da, hesaplamalı istatistiksel yöntemlerinin önemli bir kaynağıdır.
ML'nin matematiksel temelleri matematiksel optimizasyon (matematiksel programlama) yöntemleriyle sağlanır. Veri madenciliği, gözetimsiz öğrenme yoluyla keşifsel veri analizine (EDA) odaklanan ilgili (paralel) bir bilim dalıdır.
Teorik bir bakış açısından bakıldığında, muhtemelen yüksek olasılıklı doğru (PAC) öğrenme, makine öğrenimini tanımlamak için bir çerçeve sağlar.
Makine öğrenimi araştırmalarının odaklandığı konu bilgisayarlara karmaşık örüntüleri algılama ve veriye dayalı akılcı kararlar verebilme becerisi kazandırmaktır. Bu, makine öğreniminin istatistik, olasılık kuramı, veri madenciliği, örüntü tanıma, yapay zekâ, uyarlamalı denetim ve kuramsal bilgisayar bilimi gibi alanlarla yakından ilintili olduğunu gösterir.

Özet
Makine öğrenimi, bilgisayarların, açıkça programlanmadan karmaşık görevleri otomatikleştirmeyi öğrenmelerini sağlar. Belirli görevleri yerine getirmeleri için sağlanan verilerdeki örüntüleri ve ilişkileri keşfederek bu örüntüleri yeni durumlara uygulama yeteneği kazandırır. Bilgisayarlara atanan basit görevler için, makineye eldeki sorunu çözmek için gereken tüm adımları nasıl uygulayacağını bildiren algoritmalar programlamak mümkündür; bilgisayar tarafında öğrenmeye gerek yoktur. Daha gelişmiş görevlerde insan için gerekli algoritmaları elle yapmak zor olabilir. Uygulamada, insan programcıların gerekli her adımı belirlemesinden ziyade, makinenin kendi algoritmasını geliştirmesine yardımcı olmak daha etkili olabilir.
Makine öğrenimi disiplini, bilgisayarlara tam olarak tatmin edici bir algoritmanın bulunmadığı görevleri gerçekleştirmeyi öğretmek için çeşitli yaklaşımlar kullanır. Çok sayıda olası yanıtın olduğu durumlarda, doğru yanıtlardan bazılarını geçerli olarak etiketlemek bir yaklaşımdır. Bu, daha sonra bilgisayarın doğru yanıtları bulmak için kullandığı algoritmayı/algoritmaları geliştirmede eğitim verisi olarak kullanılabilir. Örneğin, sayısal karakter tanıma görevinde sistemi eğitmek için el yazısıyla yazılmış rakamların MNIST veri kümesi sıklıkla kullanılır.

Tarihçe
Makine öğrenimi terimi 1959'da bilgisayar oyunları ve yapay zeka alanında öncü ve IBM çalışanı olan Amerikalı Arthur Samuel tarafından icat edildi. 1960'larda makine öğrenimi araştırmasının temsili bir kitabı, Nilsson'un Öğrenme Makineleri hakkındaki kitabıydı ve çoğunlukla örüntü sınıflandırması için makine öğrenimi ile ilgiliydi. Model tanıma ile ilgili ilgi, 1973'te Duda ve Hart tarafından tanımlandığı gibi 1970'lerde de devam etti.
1981'de, bir sinir ağı 'nın bilgisayar terminalinden 40 karakteri (26 harf, 10 rakam ve 4 özel sembol) tanımayı öğrenmesi için öğretme stratejilerinin kullanımına ilişkin bir rapor verildi.
Tom M. Mitchell, makine öğrenimi alanında incelenen algoritmaların geniş ölçüde alıntılanan daha resmi bir tanımını yaptı: ""Bir bilgisayar programının performans ölçüsü ""P"" ve bazı ""T"" görev sınıflarıyla ilgili olarak ""T"" görevlerindeki performansı ""E"" deneyimiyle iyileşiyorsa ""P"" ile ölçüldüğü gibi E deneyiminden öğrendiği söylenir.
Makine öğreniminin söz konusu olduğu görevlerin bu tanımı, alanı bilişsel terimlerle tanımlamak yerine temelde operasyonel tanım sunar. Bu, Alan Turing'in ""Computing Machinery and Intelligence"" adlı makalesinde ""Makineler düşünebilir mi?"" ""Makineler bizim (düşünen varlıklar olarak) yapabildiğimizi yapabilir mi?"" sorusuyla değiştirilir.
Günümüzün modern makine öğreniminin iki amacı vardır, biri verileri geliştirilen modellere göre sınıflandırmak, diğer amaç ise bu modellere dayalı olarak gelecekteki sonuçlar için tahminler yapmaktır. Verileri sınıflandırmaya özgü varsayımsal bir algoritma, kanserli benleri sınıflandırmada onu eğitmek için denetimli öğrenmeyle birleştirilen mollerin bilgisayar görüşü kullanabilir. Hal böyle olunca, hisse senedi ticareti için makine öğrenme algoritması, tüccara gelecekteki olası tahminler hakkında bilgi verebilir.

Diğer alanlarla ilişkiler
Yapay zeka
Bilimsel bir çaba olarak makine öğrenimi, yapay zeka arayışından doğdu. Yapay zekanın akademik disiplin olarak ilk günlerinde bazı araştırmacılar makinelerin verilerden öğrenmesini sağlamakla ilgileniyordu. Soruna çeşitli sembolik yöntemlerle ve daha sonra ""sinir ağları"" denilen yöntemlerle yaklaşmaya çalıştılar; bunlar çoğunlukla perceptronlar ve diğer modellerdi daha sonra istatistiklerin genelleştirilmiş doğrusal modellerin yeniden icatları oldukları anlaşıldı.
Olasılık muhakeme de özellikle otomatik tıbbi tanı için kullanıldı. Ancak, mantıksal, bilgiye dayalı yaklaşım üzerindeki artan vurgu, yapay zeka ile makine öğrenimi arasında bir sürtüşmeye neden oldu. Olasılıklı sistemler, veri toplama ve gösteriminin teorik ve pratik problemleriyle boğuşuyordu.
1980 yılına gelindiğinde, uzman sistemler yapay zekaya hâkim oldu ve istatistik gözden düştü.
Sembolik/bilgiye dayalı öğrenme üzerine çalışmalar AI içinde devam etti ve endüktif mantık programlama'ya yol açtı ancak daha istatistiksel araştırma hattı artık örüntü tanıma da ve bilgi erişimdeydi.
Sinir ağları araştırması, yapay zeka ve bilgisayar bilimi tarafından aynı zamanlarda terk edildi. Bu çizgi de diğer disiplinlerden John Hopfield, Rumelhart ve Hinton‘i içeren araştırmacılar tarafından AI/CS alanının dışında ""bağlantısallık"" olarak devam ettirildi. Ana başarıları, 1980'lerin ortasında geri yayılımın yeniden buluşuyla ortaya çıktı.
Ayrı bir alan olarak yeniden düzenlenen makine öğrenimi (ML), 1990'larda gelişmeye başladı. Alan, amacını yapay zeka elde etmekten ziyade pratik nitelikteki çözülebilir problemlerle mücadele etmek olarak değiştirdi. Odağı, AI'dan miras aldığı sembolik yaklaşımlar'dan, istatistik ve olasılık teorisi’nden ödünç alınan yöntem ve modellere kaydırdı.
2020 itibarıyla birçok kaynak, makine öğreniminin yapay zekanın bir alt alanı olmaya devam ettiğini iddiasını sürdürüyor.
Ana anlaşmazlık, tüm makine öğreniminin Yapay zeka(YZ)'nın (AI) bir parçası olup olmadığıdır çünkü bu, makine öğrenimini kullanan herhangi birinin YZ kullandığını iddia edebileceği anlamına gelir. Diğerlerinin görüşü, tüm makine öğreniminin yapay zekanın bir parçası olmadığıdır ki burada, makine öğreniminin yalnızca 'akıllı' bir alt kümesi YZ'nin bir parçasıdır.
Makine öğrenimi ve yapay zeka arasındaki farkın ne olduğu sorusu, ""The Book of Why"" adlı kitabında Judea Pearl tarafından yanıtlanır. Buna göre, makine öğrenimi pasif gözlemlere dayanarak öğrenir ve tahmin eder, oysa AI, hedeflerine başarılı bir şekilde ulaşma şansını en üst düzeye çıkaran eylemleri öğrenmek ve gerçekleştirmek için çevre ile etkileşime giren aracı ifade eder.

Veri sıkıştırma
Makine öğrenimi ile sıkıştırma arasında yakın bir bağlantı vardır. Tüm geçmişi göz önüne alındığında bir dizinin sonsal olasılıklarını tahmin eden bir sistem, optimum veri sıkıştırması için (çıkış dağılımında aritmetik kodlama kullanılarak) kullanılabilir. Tersine, tahmin için en uygun sıkıştırıcı (önceki geçmiş göz önüne alındığında en iyi sıkıştıran sembolü bularak) kullanılabilir. Bu eşdeğerlik, veri sıkıştırmanın ""genel zeka"" için ölçüt olarak kullanılmasının gerekçesi olarak kullanılmıştır.
Alternatif bir görünüm, sıkıştırma algoritmalarının dizeleri örtülü özellik alanı vektörlerine örtülü olarak eşlediğini gösterebilir ve sıkıştırmaya dayalı benzerlik ölçümleri, bu özellik alanlarındaki benzerliği hesaplar. Her sıkıştırıcı C(.) için ilişkili bir vektör uzayı ℵ tanımlarız, öyle ki C(.), ||~x|| vektör normuna karşılık gelen giriş dizesi x'i eşler. Tüm sıkıştırma algoritmalarının altında yatan özellik uzaylarının kapsamlı incelemesi uzay nedeniyle engellenir; bunun yerine, özellik vektörleri üç temsili kayıpsız sıkıştırma yöntemini yani LZW, LZ77 ve PPM'yi incelemeyi seçer.
Hutter Ödülü'nde daha doğrudan açıklanan bir bağlantı olan AIXI teorisine göre, x'in mümkün olan en iyi sıkıştırılması, x'i üreten mümkün olan en küçük yazılımdır. Örneğin, bu modelde bir zip dosyasının sıkıştırılmış boyutu hem zip dosyasını hem de zip açma yazılımını içerir. Çünkü her ikisi olmadan zip dosyasını açamazsınız ancak daha da küçük birleştirilmiş bir form olabilir.
Yapay zeka destekli ses/video sıkıştırma yazılımı örnekleri olarak VP9, NVIDIA Maxine, AIVC, AccMPEG sayılabilir. Yapay zeka destekli görüntü sıkıştırma gerçekleştirebilen yazılımına OpenCV, TensorFlow, MATLAB'ın Image Processing Toolbox (IPT) ve High-Fidelity Generative Image Compression örnek olarak verilebilir.
Denetimsiz makine öğreniminde, benzer veri noktalarını kümeler halinde gruplandırarak verileri sıkıştırmak için k-ortalama kümelemesi kullanılabilir. Bu teknik, önceden tanımlanmış etiketlerin bulunmadığı kapsamlı veri kümelerinin işlenmesini basitleştirir ve görüntü sıkıştırma gibi alanlarda çok kullanılır.
Geniş dil modelleri aynı zamanda kayıpsız veri sıkıştırma özelliğiklidir.

Veri madenciliği
Makine öğrenimi ve veri madenciliği sıklıkla aynı yöntemleri kullanır ve önemli ölçüde örtüşür, ancak makine öğrenimi, eğitim verilerinden öğrenilen bilinen özelliklere dayalı olarak tahmine odaklanırken, veri madenciliği verilerdeki (daha önce) bilinmeyen özelliklerin keşfedilmesine odaklanır (bu, veritabanlarında bilgi keşfinin analiz adımıdır).
Veri madenciliği farklı amaçlarla birçok makine öğrenimi yöntemini kullanır. Öte yandan makine öğrenimi, ""denetimsiz öğrenme"" olarak veya öğrenen doğruluğunu artırmak için bir ön işleme adımı olarak veri madenciliği yöntemlerini de kullanır. Bu iki araştırma topluluğu (çoğunlukla ayrı konferansları ve ayrı dergileri olan, ECML PKDD önemli bir istisnadır) arasındaki kafa karışıklığının büyük kısmı, birlikte çalıştıkları temel varsayımlardan kaynaklanmaktadır: Makine öğreniminde performans genellikle bilinen bilgiyi yeniden üretme becerisine göre değerlendirilirken, bilgi keşfi ve veri madenciliği (KDD)'de temel görev önceden bilinmeyen bilginin keşfidir. Bilinen bilgilere göre değerlendirildiğinde, bilgi verilmeyen (denetlenmeyen) bir yöntem, diğer denetlenen yöntemlere göre kolayca daha iyi performans gösterecektir, oysa tipik bir KDD görevinde, eğitim verilerinin mevcut olmaması nedeniyle denetlenen yöntemler kullanılamaz.
Makine öğreniminin optimizasyonla da yakın bağları vardır: birçok öğrenme problemi, eğitim set örneklerinde bazı kayıp fonksiyonların en aza indirilmesi olarak formülleştirilir. Kayıp fonksiyonları, eğitilen modelin tahminleri ile gerçek problem örnekleri arasındaki tutarsızlığı ifade eder (örneğin, sınıflandırmada örneklere bir etiket atamak istenir ve modeller, bir dizi örnek için önceden atanmış etiketleri doğru şekilde tahmin edecek şekilde eğitilir).

Genelleme
Optimizasyon ve makine öğrenimi arasındaki fark, genelleştirme hedefinden kaynaklanır: Optimizasyon algoritmaları bir eğitim setindeki kaybı en aza indirebilirken, makine öğrenimi, görünmeyen örneklerdeki kaybı en aza indirmekle ilgilenir. Çeşitli öğrenme algoritmalarının genelleştirilmesinin karakterize edilmesi, özellikle derin öğrenme algoritmaları için güncel araştırmaların aktif bir konusudur.

İstatistik
Makine öğrenimi ve istatistik, yöntemler açısından birbiriyle yakından ilişkili alanlardır ancak temel hedefleri bakımından farklıdır: istatistik bir örnekten nüfus çıkarımları yaparken, makine öğrenimi genelleştirilebilir tahmin kalıpları bulur. Michael I. Jordan'a göre, metodolojik ilkelerden teorik araçlara kadar makine öğrenimi fikirlerinin istatistik alanında uzun bir geçmişi vardır. Ayrıca genel alanı adlandırmak için veri bilimi terimini yer tutucu olarak önerdi.
Geleneksel istatistiksel analizler, çalışma veri seti için en uygun modelin önsel seçimini gerektirir. Ayrıca analize yalnızca önceki deneyimlere dayanan önemli veya teorik olarak ilgili değişkenler dahil edilir. Bunun aksine, makine öğrenimi önceden yapılandırılmış bir model üzerine kurulmamıştır; bunun yerine veriler, altta yatan kalıpları tespit ederek modeli şekillendirir. Modeli eğitmek için ne kadar çok değişken (girdi) kullanılırsa nihai model o kadar doğru olur.
Leo Breiman iki istatistiksel modelleme paradigmasını birbirinden ayırdı: veri modeli ve algoritmik model,; burada ""algoritmik model"", Rastgele orman gibi az çok makine öğrenimi algoritmaları anlamına gelir.
Bazı istatistikçiler, makine öğreniminden yöntemleri benimseyerek istatistiksel öğrenme adını verdikleri birleşik bir alana yol açtılar.

İstatistiksel fizik
Düzensiz sistemlerin köklü fiziğinden türetilen analitik ve hesaplamalı teknikler, örneğin derin sinir ağ'larının ağırlık uzayını analiz etmek için makine öğrenimi de dahil olmak üzere büyük ölçekli sorunlara genişletilebilir. İstatistiksel fizik bu nedenle tıbbi teşhis alanında da uygulama alanları bulmaktadır.

Teori
Bir öğrencinin temel amacı, deneyiminden genelleme yapmaktır.  Bu bağlamda genelleme, öğrenen bir makinenin, bir öğrenme veri kümesini deneyimledikten sonra yeni, görülmemiş örnekler/görevler üzerinde doğru bir şekilde performans gösterme yeteneğidir. Eğitim örnekleri, genel olarak bilinmeyen bazı olasılık dağılımlarından gelir (oluşma uzayını temsil ettiği kabul edilir) ve öğrencinin, yeni durumlarda yeterince doğru tahminler üretmesini sağlayan bu alan hakkında genel bir model oluşturması gerekir. Bu bağlamda genelleme, öğrenen bir makinenin, bir öğrenme veri kümesini deneyimledikten sonra yeni, görülmemiş örnekler/görevler üzerinde doğru şekilde performans gösterme yeteneğidir. Eğitim örnekleri, genel olarak bilinmeyen bazı olasılık dağılımlarından gelir (oluşma uzayını temsil ettiği kabul edilir) ve öğrencinin, yeni durumlarda yeterince doğru tahminler üretmesini sağlayan bu alan hakkında genel bir model oluşturması gerekir.
Makine öğrenimi algoritmalarının ve performanslarının hesaplamalı analizi, Yüksek Olasılıklı Doğru Öğrenme (PAC) modeli aracılığıyla hesaplamalı öğrenme teorisi olarak bilinen teorik bilgisayar biliminin bir dalıdır. Eğitim kümeleri sınırlı olduğundan ve gelecek belirsiz olduğundan, öğrenme teorisi genellikle algoritmaların performansına dair garanti vermez. Bunun yerine performansa ilişkin olasılıksal sınırlar oldukça yaygındır. Önyargı-varyans ayrıştırması, genelleme hatasını ölçmenin bir yoludur.
Genelleme bağlamında en iyi performansı elde etmek için hipotezin karmaşıklığı, verilerin altında yatan işlevin karmaşıklığıyla eşleşmelidir. Hipotezin fonksiyondan daha az karmaşık olması durumunda model, verilere gereğinden az uyum sağlamıştır. Yanıt olarak modelin karmaşıklığı artarsa eğitim hatası azalır. Ancak hipotez çok karmaşıksa, model aşırı uyumdan etkilenir ve genelleme daha zayıf olur.
Performans sınırlarına ek olarak, öğrenme teorisyenleri öğrenmenin zaman karmaşıklığını ve fizibilitesini de inceler. Hesaplamalı öğrenme teorisinde, bir hesaplamanın polinom zamanında yapılması mümkünse mümkün olduğu kabul edilir. İki tür zaman karmaşıklık sonucu vardır: Pozitif sonuçlar, belirli bir fonksiyon sınıfının polinom zamanda öğrenilebileceğini gösterir. Negatif sonuçlar bazı sınıfların polinom zamanında öğrenilemeyeceğini göstermektedir.

Uygulamalar
Makine öğreniminin başlıca uygulamaları makine algılaması, bilgisayarlı görme, doğal dil işleme, sözdizimsel örüntü tanıma, arama motorları, tıbbi tanı, biyoinformatik, beyin-makine arayüzleri ve kiminformatik, kredi kartı dolandırıcılığı denetimi, borsa çözümlemesi, DNA dizilerinin sınıflandırılması, konuşma ve elyazısı tanıma, bilgisayarlı görmede nesne tanıma, oyun oynama, yazılım mühendisliği, uyarlamalı web siteleri ve robot gezisidir.

İnsan etkileşimi
Makine öğrenimi sistemlerinin bir bölümü insan sezgisine olan gereksinimi tümüyle ortadan kaldırmaya çalışırken bazıları insan ve makine arasında işbirliğine dayalı bir yaklaşım benimsemektedir. Ne var ki, sistemi tasarlayan kişinin verinin kodlanma biçimi üzerinde tümüyle egemen oluşu insan sezgisinin tümüyle ortadan kaldırılmasını olanaksızlaştırmaktadır. Makine öğrenimi deneysel yöntemin otomatikleştirilmesi çabası olarak görülmektedir.
Bazı istatistiksel makine öğrenimi araştırmacıları Bayes istatistiği çerçevesi kapsamında kullanılabilen yöntemler geliştirmektedirler.

Öğrenme yaklaşımları
Makine öğrenimi algoritmaları hedeflenen sonuca göre birkaç sınıfa ayrılabilmektedir:

Gözetimli öğrenme - Gözetimli öğrenme, bilgisayarların etiketlenmiş örnek verilerden öğrenme yeteneğini içerir. Bu yaklaşım, her örneğin bir girdi ve ona karşılık gelen bir çıktıya sahip olduğu durumlar için idealdir. Örneğin, bir görüntünün üzerindeki nesnelerin tanımlanması gibi bir problemde, etiketlenmiş veri setleri kullanılır. Makine, bu veri setlerini analiz ederek girdileri çıktılara eşleme yeteneğini geliştirir.
Gözetimsiz öğrenme - Gözetimsiz öğrenme, bilgisayarların etiketlenmemiş verilerden örüntüleri keşfetme yeteneğini içerir. Bu yaklaşım, verilerdeki yapıları anlamak ve veri setlerindeki gizli ilişkileri keşfetmek için kullanılır. Örneğin, bir pazarlama analizi yapılırken, müşteri segmentlerini belirlemek için gözetimsiz öğrenme teknikleri kullanılabilir. Bu sayede, belirli bir önceden tanımlanmış etikete gerek kalmadan veri setindeki doğal gruplamaları keşfedebiliriz.
Pekiştirmeli öğrenme - Pekiştirmeli öğrenme, bir ajanın çevresiyle etkileşime girerek deneme-yanılma yoluyla optimal davranışı öğrenme yeteneğini içerir. Bu yaklaşım genellikle karar alma ve kontrol problemleri için kullanılır. Ajan, bir ortamda belirli eylemler gerçekleştirir ve bu eylemlerin sonuçlarına göre ödüller veya cezalar alır. Bu ödüller ve cezalar, ajanın davranışını optimize etmesine yardımcı olur. Örneğin, bir robotun belirli bir ortamda dengede kalmasını öğrenmesi veya bir oyun oynarken en yüksek skoru elde etmesi için pekiştirmeli öğrenme kullanılabilir.
Yarı gözetimli öğrenme - Uygun işlev ya da sınıflandırıcılar oluşturmak için etiketli ve etiketsiz örnekleri birlikte ele alır.
Öğrenmeyi öğrenme - Önceki deneyimlerden yararlanır.

Ayrıca bakınız
Kaynakça
Konuyla ilgili yayınlar
Dış bağlantılar
""Makine öğrenimi algoritmalarının Ruby uygulamaları"". 25 Haziran 2012 tarihinde kaynağından arşivlendi. 
""Andrew Ng'in Stanford ders notları"". 31 Ağustos 2009 tarihinde kaynağından arşivlendi. 
""Berimsel Zekâ Ansiklopedisi"". 11 Ekim 2007 tarihinde kaynağından arşivlendi. 
""Uluslararası Makine Öğrenimi Topluluğu"". 9 Mart 2012 tarihinde kaynağından arşivlendi. 
""Makine öğrenimi, veri madenciliği ve KDD bilimsel konferansları"". 3 Eylül 2009 tarihinde kaynağından arşivlendi. 
""Açık kaynak kodlu makine öğrenimi yazılımları"". 3 Ekim 2009 tarihinde kaynağından arşivlendi. 
""Görüntülü makine öğrenimi dersleri"". 16 Eylül 2009 tarihinde kaynağından arşivlendi. 
""Berimsel Zekâ ve Makine Öğrenimi Sanal Topluluğu"". 4 Ekim 2009 tarihinde kaynağından arşivlendi."
Derin öğrenme,"Derin öğrenme (aynı zamanda derin yapılandırılmış öğrenme, hiyerarşik öğrenme ya da derin makine öğrenmesi) bir veya daha fazla gizli katman içeren yapay sinir ağları ve benzeri makine öğrenme algoritmalarını kapsayan çalışma alanıdır.
Yani en az bir adet yapay sinir ağının (YSA) kullanıldığı ve birçok algoritma ile, bilgisayarın eldeki verilerden yeni veriler elde etmesidir.
Derin öğrenme gözetimli, yarı gözetimli veya gözetimsiz olarak gerçekleştirilebilir. Derin yapay sinir ağları pekiştirmeli öğrenme yaklaşımıyla da başarılı sonuçlar vermiştir.
Yapay  sinir ağları, biyolojik sistemlerdeki bilgi işleme ve dağıtılmış iletişim düğümlerinden esinlenilmiştir.
Yapay sinir ağlarının biyolojik beyinlerden çeşitli farklılıkları vardır. Özellikle, sinir ağları statik ve sembolik olma eğilimindeyken, çoğu canlı organizmanın biyolojik beyni dinamik(plastik) ve analogtur.

Tarihçe
Derin öğrenme kavramı, 1940'lardan beri geliştirilen sinir ağlarına dayanmaktadır. 1980'ler ve 1990'lar boyunca, araştırmacılar geri yayılım (backpropagation) ve destek vektör makineleri gibi daha gelişmiş tekniklerle sinir ağları üzerinde çalıştılar. 2000'lerde, büyük miktarda etiketli verinin ve daha güçlü donanımların kullanılabilir hale gelmesiyle, derin öğrenme alanında büyük ilerlemeler kaydedildi. Bu dönemde, yapay sinir ağları ve derin öğrenme, tanıma ve sınıflandırma görevlerinde insan seviyesinde performans sergilemeye başladı.

Derin Öğrenme Modelleri ve Teknikleri
Derin öğrenme modelleri, farklı yapı ve işlevlere sahip çeşitli sinir ağlarından oluşur. Başlıca derin öğrenme modelleri şunlardır:

Yapay Sinir Ağları (ANN)
Evrişimli Sinir Ağları (CNN)
Tekrarlayan Sinir Ağları (RNN)
Uzun Kısa Vadeli Bellek (LSTM)
Generative Adversarial Networks (GAN)
Derin öğrenme algoritmaları, büyük veri kümesi üzerinde eğitilerek başarılı tahminler yapabilirler. Bu süreçte sıkça kullanılan teknikler şunlardır:

Geri yayılım (Backpropagation)
Aktivasyon fonksiyonları
Dropout ve düzenlileştirme (Regularization)
Stokastik Gradyan İnişi (SGD)
Adam optimizasyonu

Uygulama Alanları
Derin öğrenme, bilgisayarlı görü ve ses tanıma, doğal dil işleme, tıbbi görüntü analizi ve oyun stratejileri gibi çeşitli alanlarda başarıyla kullanılmaktadır. Ayrıca, otomotiv, eğlence, finans ve sağlık gibi sektörlerde önemli rol oynar.
Örnek kullanım alanları:

Yüz tanıma sistemleri,
Ses tanıma sistemleri,
Araçlarda otopilot özelliği ve sürücüsüz kendi kendine giden araçlar,
Alarm sistemleri, (kamera kayıtlarını sürekli kontrol etmek yerine, yalnızca olağandışı hareketlerde alarm sisteminin devreye girmesi gibi teknolojiler derin öğrenme sayesinde mümkün olmaktadır.)
Sağlık sektöründe kanser araştırmaları, (Kanserli hücre örneklerinin tanıtıldığı derin öğrenme algoritmaları, yeni hücrelerin kanserli olup olmadığı tanısını koymakta hem daha hızlı hem de daha başarılı oluyor.)
Görüntü iyileştirilmeleri,
Tavsiye sistemleri (örneğin beğenilebilecek ürün, müzik ve film önerileri sunmada)
Siber tehdit analizleri


== Kaynakça =="
Doğal dil işleme,"Doğal Dil İşleme, yaygın olarak NLP (Natural Language Processing) olarak bilinen yapay zekâ ve dilbilim alt kategorisidir. Türkçe, İngilizce, Almanca, Fransızca gibi doğal dillerin işlenmesi ve kullanılması amacı ile araştırma yapan bilim dalıdır.

Uzman Sistemler ve Doğal Dil İşleme
NLP yani Doğal Dil İşleme, doğal dillerin kurallı yapısının çözümlenerek anlaşılması veya yeniden üretilmesi amacını taşır.Bu çözümlemenin insana getireceği kolaylıklar, yazılı dokümanların otomatik çevrilmesi, soru-cevap makineleri, otomatik konuşma ve komut anlama, konuşma sentezi, konuşma üretme, otomatik metin özetleme, bilgi sağlama gibi birçok başlıkla özetlenebilir. Bilgisayar teknolojisinin yaygın kullanımı, bu başlıklardan üretilen uzman yazılımların gündelik hayatımızın her alanına girmesini sağlamıştır. Örneğin, tüm kelime işlem yazılımları birer imlâ düzeltme aracı taşır. Bu araçlar aslında yazılan metni çözümleyerek dil kurallarını denetleyen doğal dil işleme yazılımlarıdır.
Batı dillerinde SAPI (Microsoft şirketinin konuşma sentezleyici üretmek amacı ile satışa sunduğu geliştirici program) tabanlı Konuşma sentezleyici bileşenleri, yazılımcıların multimedia (çoklu ortam) sunuları hazırlamaları için hizmete sunulmuştur.
Konuşma ve komut anlama yazılımları ise gelecekte insan ve bilgisayar arasındaki klavye, fare gibi veri girişi aygıtlarını ortadan kaldıracak yazılımlardır. Bu gelişmeler makine-insan iletişiminde yeni ve devrimci değişimlere yol açacak ve bilgisayarların daha çok insan tarafından kabul görmesine yol açacaktır.

Yapay Zekâ ve Doğal Dil İşleme
Gelecekte, konuşma sentezleyiciler ve konuşma anlama alanındaki gelişmeler ve makine-insan iletişiminin gelişmesi, insanın makineden beklentilerini yükseltecektir. İnsanlar makinelerin kendisini anlamalarını isteyecek, karmaşık kullanımı olan makineler pazar bulamayacaktır. Giderek gelişen ve insanı anlayan makinelerin daha zeki olması insanın yaşam kalitesini yükselteceğinden, vazgeçilmez olması kaçınılmazdır. Zeki makine kavramı, yapay zekâ çalışmalarının hızlanmasına yol açmıştır. Geleceğin en önemli sektörlerinden biri olan yapay zekâ ile insanın iletişim kuracağı tek araç dildir.
Dil, insanoğlunun uygarlaşmasını sağlamakla kalmamış, onun zekâsının doğada daha önce görülmemiş şekilde parlamasını sağlamıştır. Kültür dediğimiz insanlık birikimi, dil kullanan ve iletişim kuran insanın sosyalleşme sürecinin ürünüdür.

Dilin Matematik Modeli
Dilin işlenmek üzere çözümlenebilmesi için, matematik modelinin oluşturulması gerekmekteydi.

Genişletilmiş Geçiş Ağları
ATN Genişletilmiş Geçiş Ağları (Augmented Transition Network),Woods tarafından 1970 ve 1973 yılları arasında geliştirilmiş bir yaklaşımdır.
Genişletilmiş geçiş ağları (GGA) üç bileşenden oluşur:

En az başlangıç ve son (/s) durumları olan sonlu sayıdaki durumlar kümesi,
Belli bir metindeki mümkün olan harflerden oluşan alfabe (e),
Sonlu sayıdaki bir durumdan diğer bir duruma geçişi sağlayacak geçişler kümesi.
Genişletilmiş geçiş ağlarında, bir durumdan diğer bir duruma geçmek için gerekli harf okunur ve bu harf geçilecek olan duruma geçmek için gereken harfle karşılaştırılır; uygun ise diğer duruma geçilir. Geçiş ağlarında doğru bir yol, bir başlangıç durumundan başlayıp, son duruma ulaşan geçişler sağlandığında tamamlanır. Harflerin birbirine eklenmesiyle oluşan metin, ağın kabul etmesi için verilen metin ise, bu metin ağ tarafından kabul edilmiş demektir.
Yanda: ""Bal"" metnini kabul eden Genişletilmiş Geçiş Ağı.

Fonetik ve fonoloji
Fonetik, konuşulurken, dil, gırtlak, ses telleri, damak, dişler ve dudaklar ile çıkarılan sesleri ve bu seslerin dil ile olan ilişkilerini tanımlamak için kullanılan bir terimdir.Doğal dillerde anlam ayırıcı olarak kullanılan en küçük ses fondur (phon) dur. Fonetik terimi bu kökten gelmektedir.
Fon kavramı evrensel değildir ve her dilde farklı seslere kaşılık gelir. Farklı dillerdeki fonların tek ortak özelliği ayırıcı temel sesler olmalarıdır.Sesle ifade edilen dili, yani konuşmayı kaydetmek için yazı icat edilmişti.Konuşmayı yazı ile ifade etmek için ses birim veya fonları harflerle eşleştirmek gerekmekteydi. Bazı dillerde, örneğin Türkçe, Fince ve Japoncada, sesbirimler doğrudan harflere karşılık gelmektedir. Bu tip dillere fonetik diller denir.İngilizce, Almanca, Fransızca gibi dillerde ise Fonlar harflere kaşılık gelmezler.Bu yaklaşımın yerine uluslararası olarak geçerliliği olan fonetik bir alfabe ses birimleri ifade etmek için kullanılır. Ses birimlerin simgesel olarak ifade edilmesi sonucu olusan simgeler fonem (phoneme) olarak adlandırılır. Bir başka deyişle aslında fonemlerin seslendirilmesiyle ses birimler (phon) oluşur.
Dildeki ses birimler belirlenirken iki yaklaşım kullanılır.Bunlar,

Parçalı sesbirimler (segmental) ve,
Parçalarüstü ses birimler (supra-segmental, prosodic) dir.

Dilin morfolojisi
Dil bilime terim olarak 1859 yılında August Schleicher tarafından kazandırılan morfoloji, dilde biçimi oluşturan ögelerin türlerini tanımlamak ve özetle dil bilgisi kuralları denen biçimsel ögelerin sınıflandırmasını yapmaktır.

Morfolojik çözümlemede analitik yaklaşımlar
Doğal dil işleme çalışmalarında anlam bütünsel çözümleme yapabilmek için, bazı yaklaşımlar belirmiştir. Bu yaklaşımlar aşağıdaki süreçlerden oluşur.

Sözdizimsel (sentaktik) analiz
Sözdizimsel analiz, sözdizimini (syntax) veya cümleyi oluşturan morfolojik ögelerin hiyerarşik kurallara uyumunu karşılaştırarak ölçümlemektir. Böylece söz dizimin anlamlı olup olmadığının ölçülebilmesi için düzenleyici bir süreç gerçekleşmiş olur.
Türkçede cümleler en genel şekliyle özne, nesne ve yüklem bileşenlerinden oluşur. Cümleye eklenmek istenen anlamlar arttıkça cümleler, özne, yer tamlayıcısı, zarf tamlayıcısı, nesne ve yüklem gibi bileşenleri içerir.Ayrıca cümlenin anlamını kuvvetlendiren cümle dışı bileşenler de (bağlaç, edat, vb) cümlede bulunabilir.Bunlara örnek olarak ""ile, için, ama, çünkü"" kelimeleri verilebilir. Türkçede özne ile yüklem cümlenin temel bileşenleridir ve genelde tüm cümlelerde yer alırlar. Yer tamlayıcısı, zarf tamlayıcısı, nesne gibi bileşenler bazı cümlelerde yer almayabilirler veya bazı cümlelerde sadece biri, bazılarında sadece ikisi bulunabilir. Bu bileşenlerin cümle içindeki sıralanışları da değişebilir.
Bilgisayarla doğal dilin modellenmesinde anlamsal analizden önce kelimelerden oluşturulan yapının cümle olup olmadığının test edilmesi faydalıdır.Bu işlem sentaktik eşleştirme işleminde anlamsız eşleşmelerin önlenmesine faydalı olur.
Yandaki Şekil : Sözdizimsel Analiz.
Simgeler: Ö: özne, D: dolaylı tümleç, Z: zarf tümleci, N: nesne, Y: yüklem, İG: isim grubu, SG: sıfat grubu, İN: isim nesnesi, SN: sıfat nesnesi, DZ: diğer zarflar, S: sıfat, İ: isim, ZB: zaman belirteçleri, T: tamlayan, TN: tamlanan, ZM: zamir, NE: nesne eki, TE: tamlayan eki, TNE: tamlanan eki, KE: kip eki, ZE: zaman eki, DE: dolaylı tümleç eki, EF: ek fiil

Anlambilimsel (semantik) analiz
Anlambilimsel analiz, sözdizimini oluşturan morfolojik ögelerin ayrılması, yani sözdizimsel analiz ile anlam taşıyan kelimelerin sınıflandırılması işleminden sonra gelen anlamlandırma veya anlama sürecidir.Bu süreçte anlam taşıyan kelimelerin, ekler ve cümle hiyerarşisi içindeki konumlarının saptanması sayesinde birbirleri ile ilişkileri kurulabilir. Bu ilişkiler anlam çıkarma, fikir yürütme gibi ileri seviye bilişsel fonksiyonların oluşturulmasında ham bilgi olarak kullanılacaktır.

Yapay konuşma
Morfolojik çözümleme aşamalarından sonra sözdizimsel kurgu veya yapay konuşma süreci ile yapay zekâ ya veya uzman sistemlere iletişim becerisi kazandırılacaktır. Sözdizimsel çözümlemenin tersi süreçlerden oluşan birleştirme sürecinde, önceki süreçlerde ele geçen bilgi yine morfolojik kurallar dahilinde birleştirilir.

Ayrıca bakınız
Doğal dil üretme

Kaynakça
Vasif Nabiyev - Yapay Zekâ: Problemler, Yöntemler, Algoritmalar, 764 say., Seçkin, Ankara, 2005
Devrim Çamoğlu - D.U.Y.G.U. Projesi araştırma tezleri.
Ünal Çakıroğlu - (KTU) Şekiller, Sözdizimsel Analiz ve matematik model bölümü

Dış bağlantılar
İTÜ Doğal Dil İşleme Takımı 23 Ekim 2020 tarihinde Wayback Machine sitesinde arşivlendi.
Stanford Üniversitesi Doğal Dil İşleme Öbeği 29 Kasım 2005 tarihinde Wayback Machine sitesinde arşivlendi.
Survey of the State of the Art in Human Language Technology
Natural Language Processing Group at the Johns-Hopkins University
DNLP - Dalhousie Natural Language Processing Group
2004 International Workshop on Natural Language Understanding and Cognitive Science
CLAC: Computational Linguistics At Concordia 4 Aralık 2005 tarihinde Wayback Machine sitesinde arşivlendi.
TCC: Cognitive and Communication Technologies (TCC) at ITC-Irst
YTÜ Doğal Dil İşleme Araştırma Grubu
Fatih Ü. Doğal Dil İşleme Grubu
Cognitive Science Society of Trakya (CSST) 10 Mart 2012 tarihinde Wayback Machine sitesinde arşivlendi."
Yapay sinir ağı,"Yapay sinir ağları (YSA), insan beyninin bilgi işleme tekniğinden esinlenerek geliştirilmiş bir sinir ağı ve bilgi işlem teknolojisidir. YSA ile basit biyolojik sinir sisteminin çalışma şekli taklit edilir. Yani biyolojik sinir hücrelerinin ve bu hücrelerin birbirleri ile arasında kurduğu sinaptik bağın dijital olarak modellenmesidir. Nöronlar çeşitli şekillerde birbirlerine bağlanarak ağlar oluştururlar. Bu ağlar öğrenme, hafızaya alma ve veriler arasındaki ilişkiyi ortaya çıkarma kapasitesine sahiptirler. Diğer bir ifadeyle, YSA'lar, normalde bir insanın düşünme ve gözlemlemeye yönelik doğal yeteneklerini gerektiren problemlere çözüm üretmektedir. Bir insanın, düşünme ve gözlemleme yeteneklerini gerektiren problemlere yönelik çözümler üretebilmesinin temel sebebi ise insan beyninin ve dolayısıyla insanın sahip olduğu yaşayarak veya deneyerek öğrenme yeteneğidir.
Biyolojik sistemlerde öğrenme, nöronlar arasındaki sinaptik bağlantıların ayarlanması ile olur. Yani, insanlar doğumlarından itibaren bir yaşayarak öğrenme süreci içerisine girerler. Bu süreç içinde beyin sürekli bir gelişme göstermektedir. Yaşayıp tecrübe ettikçe sinaptik bağlantılar ayarlanır ve hatta yeni bağlantılar oluşur. Bu sayede öğrenme gerçekleşir. Bu durum YSA için de geçerlidir. Öğrenme, eğitme yoluyla örnekler kullanarak olur; başka bir deyişle, gerçekleşme girdi/çıktı verilerinin işlenmesiyle, yani eğitme algoritmasının bu verileri kullanarak bağlantı ağırlıklarını bir yakınsama sağlanana kadar, tekrar tekrar ayarlamasıyla olur.
YSA'lar, sinir hücrelerine benzer şekilde, birbiriyle bağlantılı işlem birimlerinden oluşan matematiksel ağlardır. Bir işlem birimi, aslında sık sık transfer fonksiyonu olarak anılan bir denklemdir. Bu işlem birimi, diğer nöronlardan sinyalleri alır; bunları birleştirir, bir aktivasyon fonksiyonuna sokarak dönüştürür ve sayısal bir sonuç ortaya çıkartır. Genelde, işlem birimleri kabaca gerçek nöronlara karşılık gelirler ve bir ağ içinde birbirlerine bağlanırlar; bu yapı da sinir ağlarını oluşturmaktadır.
YSA'lar, geleneksel mikroişlemcilerden farklı şekilde işlem yapmaktadırlar. Geleneksel mikroişlemcilerde, tek bir merkezi işlem birimi her komutları sırasıyla gerçekleştirir. YSA'lar ise her biri büyük bir problemin sadece bir parçası ile ilgilenen çok sayıda müstakil mikroişlemciden oluşmaktadır. En basit şekilde, müstakil bir işlem birimi, belirli bir girdiyi işler, sözkonusu girdiyi dönüştürür ve bir çıktı değeri oluşturur. İlk bakışta, işlem birimlerinin çalışma şekli yanıltıcı şekilde basittir. Sinirsel hesaplamanın gücü, toplam işlem yükünü paylaşan işlem birimlerinin birbirleri arasındaki yoğun bağlantı yapısından gelmektedir. Bu sistemlerde geri yayılım metoduyla daha sağlıklı öğrenme sağlanmaktadır.
Çoğu YSA'da, benzer karakteristiğe sahip nöronlar tabakalar halinde yapılandırılırlar ve transfer fonksiyonları eşzamanlı olarak çalıştırılırlar. Hemen hemen tüm ağlar, veri alan nöronlara ve çıktı üreten nöronlara sahiptirler.
YSA'nın ana öğesi olan matematiksel fonksiyon, ağın mimarisi tarafından şekillendirilir. Daha açık bir şekilde ifade etmek gerekirse, fonksiyonun temel yapısını ağırlıkların büyüklüğü ve işlem elemanlarının işlem şekli belirler. YSA'ların davranışları, yani girdi veriyi çıktı veriye nasıl ilişkilendirdikleri, ilk olarak nöronların transfer fonksiyonlarından, nasıl birbirlerine bağlandıklarından ve bu bağlantıların ağırlıklarından etkilenir.
Yapay sinir ağlarının üstünlüklerinin yanı sıra bazı sakıncaları da vardır. Bu sakıncalar şu şekilde listelenebilir:

Sistem içerisinde ne olduğu bilinemez.
Bazı ağlar hariç kararlılık analizleri yapılamaz.
Farklı sistemlere uygulanması zor olabilir.

Nörobilgisayar
Nörobilgisayar, genellikle çok hız gerektiren ve büyük boyuttaki problemlerin çözülmesi için tamamen yapay sinir ağı teknolojisine dayanarak geliştirilmiş bilgisayar türüdür.
Siemens tarafından geliştirilmiş synapse 1 nörobilgisayarı, bir ana bilgisayara ethernet kartı ile bağlanıp çalışabilen bir bilgisayardır. 8 Tane MA-16 sistolik dizi yongası kullanmaktadır. Bu bilgisayarın performansı 25 mhz ile saniyede 3.2 milyar çarpım (16 bit*16 bit) ve toplama işlemi yapabilecek bir güce sahiptir.

Ayrıca bakınız
Bilişsel bilim
Bulanık mantık
Derin öğrenme yazılımlarının karşılaştırılması
Yinelemeli sinir ağı
Evrişimli sinir ağları
Hücresel yapay sinir ağları
Genetik algoritma
Genetik programlama
Naive Bayes sınıflandırıcısı
Sinirbilim
Yapay yaşam
Yapay zeka
Tensorflow

Kaynakça
Çetin Elmas (2012), Yapay Zeka Uygulamaları, Yapay Sinir Ağları – Bulanık Mantık– Genetik Algoritma, Ankara: Seçkin Yayinevi ISBN 9789750216961
Ercan Öztemel (2006), Yapay Sinir Ağları, Istanbul: Papatya ISBN 9789756797396
Şeref Sağıroğlu, ""Mühendislikte Yapay Zeka Uygulamaları I : Yapay Sinir Ağları"", Ufuk Kitabevi, Ağustos 2003.

Dış bağlantılar
Yapay Sinir Ağlar websitesi"
Denetimli öğrenme,"Gözetimli öğrenme ya da denetimli öğrenme (İngilizce: supervised learning), bilinen etiketler ve özellikler kullanarak bir fonksiyon öğrendiğimiz, makine öğreniminin önemli bir alt dalıdır. Bu yöntem, eğitim veri seti kullanılarak öğrenilen modelin, yeni ve bilinmeyen veri noktalarını doğru bir şekilde tahmin etmesini amaçlar.
Bilgisayar mühendisliğinin, makine öğrenmesi alanının bir konusu olan gözetimli öğrenme, verilen 
  
    
      
        X
      
    
    {\displaystyle X}
  
 girdi kümesinden istenen 
  
    
      
        Y
      
    
    {\displaystyle Y}
  
 çıktı kümesinin elde edilmesi için bir fonksiyon öğrenilmesidir.
Gözetimli öğrenmede, öğrenilmek istenen kavram ile ilgili toplanan gözlemler bir eğitim kümesi olarak öğreniciye verilir. Eğitim kümesinde her örnek için istenen çıktı değerleri de verilir. Bu bilgiler kullanılarak giriş ve çıkış arasında bir ilişki oluşturulur. Oluşturulan ilişki kullanılarak gelecekte karşılaşılacak 
  
    
      
        
          X
          ′
        
      
    
    {\displaystyle X'}
  
gözlemlerinin karşılık geldiği 
  
    
      
        
          Y
          ′
        
      
    
    {\displaystyle Y'}
  
çıktıları tahmin edilebilir.

Tarihçe ve Gelişim
Gözetimli öğrenme, 1950'lerde istatistiksel sınıflandırma ve regresyon analizinin gelişimi ile ortaya çıkmıştır. Alan, yapay sinir ağlarının ve destek vektör makinelerinin geliştirilmesi ile önemli ilerlemeler kaydetmiştir. 1990'larda büyük veri kümelerinin ve hesaplama gücünün artmasıyla, gözetimli öğrenme algoritmaları daha sofistike hale gelmiştir.

Uygulama Alanları
Gözetimli öğrenme, çeşitli alanlarda başarılı bir şekilde uygulanmaktadır:
Tıp: Hastalık teşhisi ve tedavi önerileri.
Finans: Kredi risk analizi ve piyasa tahminleri.
Pazarlama: Müşteri segmentasyonu ve hedefli pazarlama.
Görüntü İşleme: Nesne tanıma ve yüz tanıma.

Avantaj ve Dezavantajlar
Gözetimli öğrenme yöntemlerinin avantajları ve dezavantajları şunlardır:
Avantajlar:
Yüksek doğruluk: Etiketlenmiş veri setleri kullanarak yüksek doğrulukta modeller oluşturabilir.
Uygulama çeşitliliği: Birçok farklı problem türünde uygulanabilir.
Dezavantajlar:
Veri bağımlılığı: Etiketlenmiş veri gereksinimi yüksek ve maliyetlidir.
Aşırı öğrenme riski: Model, eğitim verisine aşırı uyum sağlayarak genelleme yeteneğini kaybedebilir.


== Kaynakça =="
Denetimsiz öğrenme,"Gözetimsiz öğrenme, gözetimli öğrenmeden farklı olarak, verileri sebep-sonuç ya da giriş-çıkış şeklinde etiketlemeden, veri içerisinde var olan ilişkilerin ve yapıların öğrenilmesidir. Veri örneklerinin birbirine olan uzaklıklarını, komşuluk ilişkilerini ve yoğunluklarını kullanarak veriyle ilgili çıkarımlar yapılmasını sağlar. Gözetimsiz öğrenmenin iki önemli yaklaşımı boyut indirgeme ve kümelemedir.
Örneğin, bir sosyal ağda tanınan kişiler arkadaş olarak eklenir. Sosyal ağ sitesi ise üyelerini ekledikleri kişilere göre sınıflandırarak belirli arkadaş grupları oluşturur ve kullanıcılara “tanıyor olabileceğiniz kişiler” diyerek önerilerde bulunur. Sistemin kullanıcıya sunduğu “sadece tanıyor olabileceği kişiler arkadaşlık isteği gönderebilsin” seçeneği de gözetimsiz öğrenmeye örnek gösterilebilir.
Kısaca gözetimsiz öğrenmede veriler üzerinde bir ayrım yapmadan sisteme yüklenip, algoritma ile onu kendisinin ayırıp, kendisinin öğrenmesi beklenir.


== Kaynakça =="
Veri bilimi,"Veri bilimi, yapılandırılmış ve yapılandırılmamış verilerden bilgi ve öngörü elde etmek için bilimsel yöntemleri, süreçleri, algoritmaları ve sistemleri kullanan çok disiplinli bir alandır. Veri bilimi veri madenciliği ve büyük verilerle ilişkilidir.
Veri bilimi, “gerçek olayları verilerle anlamak ve analiz etmek” için “ istatistikleri, veri analizini, makine öğrenimini ve ilgili yöntemlerini birleştirmek için kullanılan bir kavramdır”. Matematik, istatistik, bilgisayar bilimi ve bilgi bilimi bağlamından birçok teknik ve teori kullanır. Turing ödüllü Jim Gray, veri bilimini bir ""dördüncü paradigma"" bilimi (ampirik, teorik, hesaplamalı ve şimdi veri odaklı) olarak tanımlar. 2015 yılında Amerikan İstatistik Kurumu veritabanı yönetimi, istatistik ve makine öğrenimi ve dağıtılan ve paralel sistemleri üç temel meslek topluluğu olarak tanımladı.
Veri bilimi, bir dizi ilkeyi, çeşitli algoritmaları, olayları ve büyük veri kümelerinden gelen kullanışlı kalıpları ayıklamak için gerekli süreçleri kapsamaktadır. Bununla birlikte veri bilimi, bu süreçlerde; veri analizini, istatistikleri, makine öğrenmesi ve veri madenciliği gibi alanları ve bunlarla ilgili birçok yöntemi birleştirmek için kullanılan bir kavram olarak belirtilir.
Veri bilimi, makine öğrenmesi ve veri madenciliği kavramları sıklıkla birbirleri yerine kullanılmaktadır. Bu disiplinler arasındaki ortaklık, verilerin analizi yoluyla karar vermenin iyileştirilmesini sağlamaktır. Veri bilimi bu alanlardan beslenmekle birlikte, daha geniş bir kapsama alanına sahiptir. Makine öğrenmesi, veriden örüntü çıkarma algoritmalarının tasarımı ve değerlendirmesine de odaklanır. Veri madenciliği genellikle yapılandırılmış verilerin analizi ile ilgilenir ve ticari uygulamalara vurgu yapar. Veri bilimi ise, tüm bu hususları dikkate almaktadır.
Veri bilimi ile ilgili önemli bazı kavramlardan;
Yapılandırılmış veri, en basit anlamıyla bir Excel tablosu olarak düşünülebilir. Başka bir deyişle buradaki her bir sütundaki veri sütun başlığının içeriğine mutlaka uygun bir biçimde yapısı belli olan bir değer içerir. Yapılandırılmış verilerde girdi verileri, sayısal veya kategorik olan belirli bir değişkenler kümesi için veri noktalarından oluşur.
Yapılandırılmamış veriler, herhangi bir dilde yazılan metin, dil bilgisi kurallarına tabi olsa da yapılandırılmış verileri analiz ederken sahip olduğumuz açıkça tanımlanmış değerlerden yoksundur. Günümüzdeki verilerin çoğu yapılandırılmamış biçimdedir. Resim dosyaları, ses dosyaları, PDF dosyalar, Word gibi metin tabanlı dosyalar, elektronik postalar gibi veriler yapılandırılmamış biçimdeki verilerdir.
Diğer yandan, metin veritabanılarında saklanan veriler, örneğin doküman başlığı, yazar isimleri, tarih, tür gibi bir kısım yapısal olan, ancak içerik gibi büyük oranda yapısal olmayan alanlar içerebilir. Bu durumda ise veri yarı yapılandırılmış bir hâl alır.
Veri analisti kavramı ise, iş dünyasına yapılandırılmış veri biçimi ile girmiştir. Görev tanımı, veritabanılarından belirli araçlar yardımıyla sonuçlar üretmek ve bunları raporlamaktır. Veri bilimciler ise, artık büyük veri olarak adlandırılan veri kümesi ile birlikte yapılandırılmamış veri biçimi de organizasyonların veritabanılarında yer almaktadır. Günümüzde Facebook, Twitter gibi sosyal ağlar; Google, Yahoo gibi bazı portaller veri bilimcilerle çalışmaktadır. Veri bilimciler, farklı veri kaynaklarından beslenen büyük veri yönetimi için hipotezler kurup, bu hipotezlerin doğruluğu ya da yanlışlığını test etmek için araştırmalar yapar. Bu doğrultuda veri odaklı önemli uygulamalar geliştirirler.  Veri bilimciler, birçok disipline dayanan eşsiz bir beceri setine sahip olmalıdırlar. Sektörden gelen taleplere dayanarak yeni bir iş profili olarak ""veri bilimci"" ismi, çeşitli endüstriler arasında yaygın olarak farklılaşan bir biçimde ortaya çıkmıştır. Veri bilimcilerin beceri seti çok yönlüdür. Analitikler, veri yönetimi, sanat ve tasarım, girişimcilik, bilgisayar bilimi gibi alanlarda tecrübeyi içermektedir.

Veri biliminin tarihi
Veri biliminin istatistikle yakından bir ilişkisi söz konusudur. 20. yüzyılın önemli istatistikçilerinden birisi olan John W. Tukey, İstatistik matematiğine daha fazla odaklanıldığını ama verilerin analizi konusunda yeteri kadar odaklanılmadığını düşünmüş ve buna karşı bir hareket öngörmüştür. 1977 yılında Tukey, test etmek için ve hipotezler önermek için verilerin kullanılmasına daha fazla vurgu yapılması gerektiğini ve Keşifsel Veri Analizi ile Doğrulayıcı Veri Analizi'nin ""yan yana ilerleyebileceğini ve ilerlemesi gerektiğini"" savunarak Keşif Verileri Analizini yayınlamıştır.
""Veri Bilimi"" kavramı günümüzde daha yaygın bir şekilde kullanılmaya başlanmıştır. Bu yaygın kullanımı sağlayan bazı kaynaklar vardır ve bu kaynaklarla birlikte kitlelerin erişimi de daha kolay hâle gelmiştir. Bunlardan ikisi; 2002'de Bilim ve Teknoloji Veri Komitesi tarafından başlatılan Data Science Journal ve Columbia Üniversitesi tarafından 2003 yılında başlatılan The Journal of Data Science'dır.

Veri bilimi süreci
Veri bilimi, veriden elde edilen bilginin genelleştirilebilir çıkarımlarının araştırılmasıdır. Ya da veri toplama, ayıklama ve analiz etme gibi soruları formülleştirme sürecine dayanır. Genel olarak veri bilimi süreci ardışıktır ve farklı bileşenlerle birlikte devam eder. İzlenen adımlar ise şu şekildedir:

İlgilenilen soruyu tanımla,
Veriyi elde et,
Veriyi ayıkla,
Veriyi kontrol et,
İstatistiksel modeller uydur,
Sonuçları duyur,
Analizi yeniden oluşturabilecek şekilde yap.
Veri bilimi süreci döngüsel ve amaç, işlem sonrasında elde edilen bilgiler sonucunda karar verme aşamasına ulaşmaktır.

Veri biliminin etkileri
Veri Bilimi alanı, stratejik bir avantaj için problemleri çözmek amacıyla en son veri teknolojilerinin nasıl kullanıldığı açısından önemli bir geçiş noktasındadır. Veri bilimciler, son zamanlarda ve özellikle gelecekte işlerini çok farklı şekilde yürütmeye devam etmişlerdir. Büyük veri, algoritma ekonomisi, Bulut küresel işletmelerde ana akım olmaya devam ederken, işletmeler eğrinin önünde kalmak için en son rekabet stratejilerini benimsemeye devam etmektedir. Bu geçişin en çarpıcı iki özelliği, veri süreçlerinin artan otomasyonu ve anlık analitik çözümlerinin sunulmasıdır. Veri odaklı işletmeler, 2015 yılından 2021 yılına kadar geçen 6 yıllık periyotta etkisini çok büyük bir şekilde arttırarak göstermektedir. (yaklaşık 333 milyar ABD dolarından 1,2 trilyon ABD dolarına) Veri bilimcileri, şirketlerin büyüklüklerine bakmaksızın her büyüklükteki şirketin kuruluşlarını, çeşitli bulgulara dayanarak analiz etmelerine yardımcı olmak için bir veri okyanusundan yararlı bilgileri çıkarmanın yollarını bulmalarına yardımcı olmakla birlikte, ilgili sonuçları bulmak için veri merkezli sorular sormaya, verileri analiz etmeye ve istatistik ve matematiği uygulamaya odaklanır.

Veri bilimi görevleri
Kümeleme
Veriler, bir canlı türünün özelliklerini tanımlar veya bir makinenin ne gibi sistemi olduğunu kaydeder. Her türlü nesne ve olgunun anlaşılması için ileri analiz, kararlar ve nihayetinde bir temel oluşturur. Bu veri analizlerinin sonucunda da bir gruplaşma veya kümeleme işlemi yapılması şarttır. Aynı grupta sınıflandırılan cisimler belirli benzer özellikler göstermelidir. Yeni bir nesneyi öğrenmek veya tanımlamak özelliklerini sınıflandırma konusunda insanlar için büyük önem taşır. Bu özellikleri benzerlik ve farklılıklarına göre diğer nesnelerle karşılaştırma yapılır. Diğer yandan, kümeleme tanımı üzerine bir fikir birliği olmamakla birlikte, ""benzer özellikler gösteren nesnelerin birlikte gruplanması"" şeklinde yorumlanabilir. Küme analizi, veri madenciliğinde önemli bir yere sahiptir. Benzerlik ölçüsüne dayanan koleksiyonların kümeler halinde örgütlenmesi problemini ele alır.

İlişkilendirme kuralları madenciliği
Veri madenciliğinde en çok kullanılan yöntemlerden biridir. Veri kümelerinde gizli olan örüntüleri ortaya çıkarmak için kullanılır. İlişkilendirme kuralları madenciliği, biyomedikal araştırmacılar içinde ""Keşifsel Veri Analizi yapmak için ve veri kümelerindeki değişkenler nelerdir?"" gibi soruların cevaplarını bulmak için yaygın olarak bu yöntemi kullanmaktadır.

Anomali algılama
Anomali algılama, bir veri kümesindeki tipik verilere uymayan örneklerin aranmasını ve tanımlanmasını içerir. Bu uygun olmayan örneklere genellikle anomaliler veya aykırı değerler denilir. Anormal durum tespiti genellikle, potansiyel dolandırıcılık faaliyetlerini tanımlamak ve soruşturmaları tetiklemek için finansal işlemlerin analizinde kullanılır. Anomali algılama akıllı telefonlar üzerinden bir örnekle belirtilebilir. Akıllı telefonların yaygınlaşması, kötü amaçlı uygulamaları da beraberinde getirmiştir. Son yıllarda kötü amaçlı yazılımlar Android telefonlar için büyük bir tehdit haline gelmiştir. Kötü amaçlı ağ davranışını tanımlamak üzere ağ trafiği analizi, veri madenciliği ile birleştirilebilir. Ağ trafiği özelliklerini ağ verilerinden çıkarmak için geliştirilen Apriori algoritması ile genel olarak operasyonel davranış tetikleyicileri aracılığıyla kötü amaçlı yazılım işlevleri ortaya çıkabilir. Oluşturulan model, bir anomaliyi etkili bir şekilde tespit edebilir, günlük akıllı telefon güvenlik kontrolü ve değerlendirmesi için kullanılabilir.

Tahmin
Bir tahmin modeli, bir girdi için bir etiket veya kategori olarak geri döndürdüğünde bir sınıflama modeli olarak bilinir. Sınıflama modelini eğitmek, her bir örneğin hedef olayın bu örnekte olup olmadığını belirtmek üzere etiketlendiği tarihi verileri gerektirir. Örneğin, müşteri sınıflandırması her müşteriye bir etiketin atandığı bir veri kümesi gerektirir. Veri seti, her bir müşteri için bu etiketi listeleyen, hedef özellik olarak bilinen bir öznitelik içerecektir.

Veri biliminin uygulandığı bazı alanlar
Sağlık alanındaki veri bilimi uygulamaları
Günümüzde klinik vakalara stratejik bir karar desteği sağlayabilmek için büyük veri yığınları içinden değerli verilerin kullanılmasına yönelik modeller geliştirilmekte ve bunların kullanım alanları gittikçe büyük verinin analiz edildiği yeni nesil klinik karar destek sistemleri, sağlık uzmanlarının kullanımına sunularak faydalı uygulamalar geliştirilebilmektedir.

İşletmecilik alanındaki veri bilimi uygulamaları
Perakende satış ve pazarlama için veri madenciliği uygulamalarının çoğu kolaylıkla kullanılabilmektedir. Tipik bir yaklaşım, satın alma ve işlem geçmişlerinin tanımları gibi verilerden yararlanarak müşterileri sınıflandırmak ya da kümelere ayırmaktır. Bu kümeler, bugün bile pratikte sıklıkla görülebilen A-B-C segmentlerinden çok daha iyi optimize edilmiş veri odaklı bölümler oluşturabilir. Müşterilerin segmentlere ayırmak, örneğin belirli satış veya pazarlama kanalları için müşterileri seçmek veya bu müşterilere veya potansiyel müşterilere yaklaşmak için en uygun sonraki en iyi eylemin hangisi olduğunu tahmin etmek gibi daha ileri analizler için önemli bir önkoşuldur.

Eğitim alanındaki veri bilimi uygulamaları
""Eğitimde veri madenciliği, eğitim araştırmaları içerisinde yer alan bilgisayar bilimi, istatistik, matematik gibi pek çok alanla ilişkisi bulunan disiplinler arası bir çalışma alanı olup eğitimde kullanılan bilgi ve iletişim teknolojileri ile üretilen verilerin, analiz edilerek eğitimde olan kişiler için anlamlı bilgilere dönüştürülmesini amaç edinmiştir. Oldukça yeni olan bu alanın eğitimciler için önemi büyüktür.""

Finans alanındaki veri bilimi uygulamaları
Veri bilimi, finans sektöründe karar alma süreçlerini kökten dönüştüren temel araçlardan biri haline gelmiştir. Finansal işlemler büyük hacimli ve yüksek frekansta veri ürettiğinden, bu verilerin analizinde istatistiksel modelleme, makine öğrenmesi ve yapay zekâ tekniklerinin kullanımı kaçınılmaz olmuştur.
Risk yönetimi ve kredi skorlama süreçlerinde lojistik regresyon, ve gradient boosting gibi sınıflandırma algoritmaları, müşterilerin temerrüt riskini öngörmede geleneksel yöntemlere kıyasla daha yüksek doğruluk sağlar. Dolandırıcılık tespitinde ise  DBSCAN ve autoencoder gibi anomali tespiti algoritmaları, gerçek zamanlı işlem verilerini analiz ederek olağandışı aktiviteleri belirler ve yeni tehditlere karşı adaptif çözümler sunar.
Algoritmik ticarette RNN, LSTM ve Transformer tabanlı modeller, OHLC gibi zaman serisi verileri üzerinden fiyat hareketlerini tahmin eder. Özellikle yüksek frekanslı ticarette gecikmenin minimize edilmesi kritik öneme sahiptir. Yatırım stratejileri geliştirilirken Markowitz ve Black-Litterman gibi modellerin yanı sıra ARIMA, GARCH ve Prophet gibi zaman serisi analizleriyle portföy getirileri öngörülür.
Müşteri segmentasyonu için K-means ve Gaussian Mixture Models gibi kümeleme algoritmaları kullanılır; bu segmentlere özel ürün önerileri, öneri sistemleri ile kişiselleştirilir. Regülasyon ve uyum süreçlerinde ise denetimli öğrenme ve doğal dil işleme teknikleri, kara para aklama tespiti ve şüpheli işlem raporlamasında etkin biçimde kullanılmakta; statik kurallar yerine dinamik, öğrenen sistemler tercih edilmektedir.""

Veri bilimi alanındaki eğitim faaliyetleri
Eğitim ile veri bilimi arasında özel bir ilişki söz konusudur. Öğrenme süreci ve eğitim kurumları, zengin veriler içermektedir ve toplum için büyük bir önem taşımaktadır. Bu nedenle eğitim, özellikle veri bilimi için çok uygun bir alandır. Veri bilimi, işletmelerin bilgisayarları ve bilişim teknolojilerinin iş süreçlerini değiştirmesiyle benzer bir şekilde devrim yapma potansiyeline sahip yeni bir paradigmadır. Bu yeni zorluklar, teknolojik ilerlemelere ayak uydurmak ve onların toplumun yararına şekillendirmek için büyük fırsatlar sunarken aynı zamanda üniversitelere ve genel olarak yükseköğrenime sorumluluklar yükler. Aşağıda Türkiye'de ve Dünyada veri bilimi alanındaki bazı eğitim faaliyetleri görülmektedir.

Veri biliminde etik ve veri koruma
İnternetten veya mağazalardan alışveriş yaparken satıcı firmalar tarafından kullanıcılara belirtilen, ancak kullanıcılar tarafından pek üstünde durulmayan, şahsi verilerin aktarıldığı bir sistem mevcuttur. Bununla ilgili 7 Nisan 2016 tarihinde Kişisel Verilerin Korunması Kanunu Resmî Gazete’de yayınlanmıştır. Özellikle web üzerinden, e-ticaret yoluyla üyelerden veya mağazalar yoluyla ziyaretçilerden gelen pek çok veri toplanmaktadır. Bu gizlilik ve üyelik bilgilendirme sözleşmeleri okunmalı ve bilgilerimizin depolandığı unutulmamalıdır.
“Türkiye’de 24.03.2016 tarihinde TBMM Genel Kurulu’nda kabul edilen “6698 sayılı Kişisel Verilerin Korunması Kanunu” 07.04.2016 tarihli 29677 sayılı Resmi Gazete’de yayımlanarak yürürlüğe girmiştir.” Kanun kapsamında, kişisel verilerin işlenmesinde başta özel hayatın gizliliği olmak üzere kişilerin temel hak ve özgürlüklerinin korunması ile ilgili esasların düzenlenmesi amaçlanmaktadır. Kişisel verilerin korunması konusundaki çalışmalar devam etmektedir. Bilgi ve iletişim teknolojilerdeki gelişmeler nedeniyle dünyada sürekli olarak yön değiştirmekte ve sosyal, iktisadi açılardan uluslararası bir konuma gelmektedir. Başta sosyal ağlar, bulut bilişim, büyük veri analizi ve küreselleşmenin getirdiği etkilerle pek çok etken kişisel verilere erişimi ve verilerin toplanıp kullanılmasını değiştirmiş ve kolaylaştırmıştır.

Ayrıca bakınız
Veri mühendisliği


== Kaynakça =="
Yapay zekâ etiği,"Yapay zekâ etiği, yapay zeka sistemlerinin tasarım ve kullanım süreçlerinde ortaya çıkan etik sorunları inceleyerek, bu sorunlarla mücadele yöntemleri geliştirmeye odaklanan disiplindir. Yapay zeka etiği  robotlara ve diğer yapay zeka sistemlerine özgü teknoloji etiğinin bir parçasıdır.  Yapay zeka sistemlerini tasarlarken, inşa ederken, kullanırken ve onlara karşı davranırken insanların etik davranışları ile ilgili bir roboetiğe ve yapay ahlaki etkenlerin ahlaki davranışlarıyla ilgilenen makine etiği şeklinde ikiye ayrılabilir. Yapay genel zekâ sistemlerine (YGZ) ilişkin olarak, tam etik aracı olan YGZ'lerin mevcut yasal ve sosyal çerçevelerle bütünleştirilmesine yönelik yaklaşımlar üzerinde ön çalışmalar yapılmıştır. Bu yaklaşımlar yasal konumlarının ve haklarının iki yönlü olarak ele alınmasına odaklanmıştır. YGZ sistemlerinin adil ve tarafsız olması, etik açıdan büyük önem taşır. YGZ algoritmalarının karar alma süreçlerinde hiçbir grubu haksız bir şekilde avantajlı veya dezavantajlı hale düşürmeden tasarlanmış olması gerekir. Ancak, YGZ sistemleri, geliştirildikleri veri setlerinden etkilenir ve bu veri setleri, mevcut toplumsal önyargıları yansıtabilir. Bu nedenle, YGZ'nin adalet ve tarafsızlık prensiplerine uygun olarak geliştirilmesi kritik öneme sahiptir.

Avrupa Birliği'nin Yapay Zekâ Yasası
Avrupa Birliği'nin Yapay Zekâ Yasası 13 Haziran 2024 tarihinde Avrupa Parlamentosu ve Konseyi tarafından kabul edilen Yapay Zekâ Yasası (AI Act), 2 Ağustos 2026 tarihinde tam anlamıyla yürürlüğe girecektir. Bu yasa, yapay zekânın etik ve güvenilir bir şekilde kullanılmasını sağlamak amacıyla risk temelli bir yaklaşım benimsemektedir. Yasaklı sistemler arasında insan davranışını manipüle eden teknikler, kırılgan grupların sömürüsünü içeren sistemler, sosyal puanlama sistemleri ve kamu alanlarında gerçek zamanlı biyometrik tanımlama yapan sistemler bulunmaktadır.

Ayrıca bakınız
SimSimi


== Kaynakça =="
Chatbot,"Chatbot, kullanıcı ile genellikle metin, bazı durumlarda ise konuşma yoluyla diyalog kurarak bilgi veren veya bir işlemi gerçekleştiren bir yazılımdır.
90'lı yıllarda kullanıcıların günlük hayatına giren web siteleri ve 2000 sonrasında kullanmaya başlanan mobil uygulamalar gibi chatbotlar da günümüzün ve yakın geleceğin, teknoloji ile insan arasındaki etkileşimi sağlayan arayüzleri olarak adlandırılmaktadır. Web siteleri ve mobil uygulamalar grafiksel arayüzler (GUI), chatbotlar ise diyalog bazlı arayüzlerdir (CUI). Bu sebeple, chatbotların teknoloji ile insan arasındaki etkileşimi mevcut durumdan farklı bir boyuta getirdiği ve arayüz anlamında devrimsel bir özellik taşıdığı görüşü yaygınlık kazanmıştır. Chatbotlar, ilgili kurumun web sitesinde, mobil uygulamasında yer alabilecekleri gibi yaygın mesajlaşma platformlarında (WhatsApp, WeChat, Facebook Messenger) veya sesli asistanlarda (Google Assistant, Siri, Amazon Alexa) yer alabilirler. En yaygın kullanım alanları finans, e-ticaret, havayolu, sağlık gibi kullanıcı ile sık etkileşim gerektiren sektörlerdedir.

İlk Chatbot Örneğinden Günümüze
Chatbotlar yaklaşık 2016’dan bu yana yoğun şekilde konuşulmaktadır. Ancak yapay zeka konusundaki bilimsel araştırmalar 1950’lere kadar gitmektedir. 1950 yılında Alan Turing tarafından Mind dergisinde bir seminer raporu yayınlanmış, yazıda bugün yapay zekanın ilk örneği olarak kabul edilen Turing testinden söz edilmiştir. Turing testi bir bilgisayar programının, insan ve yazılım ile iletişim kurmasına ve hangisinin insan, hangisinin yazılım olduğunu ayırt etmesine dayanan bir testtir. Eğer denekler, insan ve yazılımı ayırt edemezse yazılım testi geçmiş sayılmaktadır.  
İlk chatbot olarak ise 1966 yılında bir MIT profesörü olan Joseph Weizenbaum tarafından geliştirilen “ELIZA” kabul edilmektedir. ELIZA bir psikolog olarak tasarlanmış ve insani diyaloglar oluşturması amaçlanmıştır. Ancak, doğal dil işleme ve makine öğrenmesi teknolojileri henüz yeterince gelişmediği için, o dönemde sadece kelime eşleştirme yöntemiyle yanıt oluşturabilmiştir. Chatbot terimi ilk olarak “chatter bot” olarak Michael L. Mauldin tarafından 1994 yılında kullanılmıştır
Günümüzde mevcut yazılım ve donanımların veri toplama ve veriyi işleme konusunda yetkin hale gelmesi, doğal dil işleme ve makine öğrenmesi alanlarının gerçek kullanım alanları bulmasını sağlamıştır. Mesajlaşmanın insanların günlük iletişim alışkanlığı haline gelmesi de chatbotlara ilginin artmasında önemli bir etken olmuştur. 2016 yılında Facebook tarafından Facebook Messenger’ın chatbotlara açıldığını duyurması ile dünyanın her yerinden geliştiriciler bu alana odaklanmış, chatbotlar hem arz hem talep açısından popüler hale gelmiştir.
OpenAI, 2022'nin sonbaharında, şirketin GPT-3 modeline dayanan ChatGPT sohbet botunu başlattı. ChatGPT, şu anda mevcut olan en gelişmiş sohbet botlarından biri olarak kabul edilir ve konuşma tabanlı yapay zeka geliştirme sürecinde önemli bir kilometre taşı olarak görülür. Model, büyük miktarda insan konuşmasıyla eğitildiği için kullanıcılarla doğal ve insan gibi iletişim kurabilir. ChatGPT genellikle müşteri hizmetleri amaçları için kullanılır ve geniş bir konu yelpazesi hakkında soruları cevaplayabilir. Yüksek trafik nedeniyle resmi ChatGPT sık sık kullanılamadı, bu yüzden OpenAI'nin resmi API'sini kullanan sohbet uygulamaları popüler hale geldi.

Chatbot Türleri
Chatbotlar birçok farklı açıdan değerlendirilerek sınıflandırılabilirler.

Yerine getirdikleri fonksiyona göre chatbotlar
Bu açıdan bakıldığında SSS (Sıkça Sorular Sorular) chatbotlar, işlem yapan chatbotlar ve kullanıcıya herhangi bir konuda önderi sunan chatbotlar söz konusudur. SSS chatbotlar sadece bilgi vererek kullanıcıya yardımcı olurlar, hem kamu kurumları hem de özel şirketler tarafından kullanıcıya hızlı ve anlık şekilde doğru bilgiye erişmesini sağlamak için kullanılabilirler. Günümüzde en fazla bu tür chatbot geliştirilmektedir, bu tür chatbotların temel fonksiyonu kullanıcıya bir konuda bilgi vermek ve sorularını yanıtlamaktır. İşlem yapan chatbotlar bir işlemi kullanıcı için başka bir arayüze yönlendirme yapmadan gerçekleştirebilirler. SSS chatbotların kurumun iç sistemleri ile entegre olması gerekmezken, işlem yapan chatbotların bu sistemlerle entegre şekilde çalışması, kurum ile bilgi alışverişi yapması gerekir. Öneri veren chatbotlar, kullanıcının ihtiyaçlarını, kullanım alışkanlıklarını, önceliklerini ve içinde bulundukları durumu anlayıp buna uygun ürün ve hizmet önerileri sunabilirler. Bu tür de kurumun sistemleri ile entegre çalışmak zorundadır. Böyle bir chatbot kullanımı sunabilmek için ilgili kurumun yüksek veri analitiği yetkinliklerine sahip olması gerekir. Bu tür chatbotlar bir sanal kişisel asistan gibi de konumlandırılabilirler. En önemli örnekleri finansal hizmetler alanında kişisel bütçe yönetimi (PFM - Personal Finance Management) konusunda geliştirilmektedir.

Kullanılan teknolojiye göre chatbotlar
Chatbotların geliştirilmesinde karar ağacı sistemleri kullanılabildiği gibi makine öğrenmesi (ML - Machine Learning), derin öğrenme (DL - Deep Learning) ve doğal dil işleme (NLP - Natural Language Processing) gibi diyalog bazlı yapay zeka (Conversational AI) teknolojileri de kullanılabilir. Karar ağacı sistemleri ile kurgulanan chatbotlara kural (kelime veya menü) bazlı chatbotlar denir. Bu tür chatbotlar kullanıcıya seçenekler sunarak, butonlar yoluyla ilgili adımın seçilmesi ile ilerlerler. Kullanıcının doğal dil ile yazdığı metinleri anlayamazlar, yazılanların içinde belirli kelimeler arar ve bu kelimelere göre belirlenmiş olan karar ağacındaki adımları izlerler. Yapay zeka bazlı chatbotlar ise kullanıcıya daha serbest ve geniş bir deneyim sunarlar. Bu tür chatbotlarda kullanıcı doğal dille metinler yazdığında chatbot tarafından anlaşılabilir, yazılan metin bir cümle olarak anlaşılıp uygun yanıt sunulur veya işlem yapılır. Bu tür bir chatbotun, alışveriş, bankacılık, havayolu gibi herhangi bir alanda hizmet verebilmesi için önceden eğitilmesi (AI Training) gerekir. Bu tür gelişmiş chatbotlarıdan bazıları, diyalogdaki konu değişikliklerini takip edilip, konular arasındaki atlamalar yapılabilir, kullanıcının memnuniyet düzeyini yürütülen diyalogdan anlayabilir veya yazım hatalarını düzeltebilir.

Hedef kitlelerine göre chatbotlar
Chatbotlar, müşterilere hızlı ve doğal bir etkileşim imkânı sunmalarını sağlaması ve aynı zamanda verimliliği arttırmaları ile şirketlerin ilgisini çekmektedir. Günümüz müşterisinin beklentileri dijital ortamda ortaya çıkan (digital native) şirketlerin sunduğu kesintisiz, basit ve çok-kanallı deneyim ile artmıştır. Müşteriler artık her sektörden anında ve hatasız hizmet beklemektedir. Chatbotlar bunu sağlama vaadi ile şirketler tarafından denenmeye ve kullanılmaya başlanmıştır. Şirketlerinin gösterdiği ilginin bir diğer sebebi de chatbotların şirket verimliliğini artırma vaadidir. Günümüzde birçok sektörde daralan kar marjları ve yeni teknolojilerin sunduğu imkânlar şirketleri verimlilik arayışına itmiştir. Chatbotlar hızlı ve kolayca yanıt verebilen, ölçeklenebilir ve kesintisiz hizmet sunabilen yapılardır.
Temelde bu iki değer önerisine sahip chatbotların şirketler için iki ana hedef kitlesi  mevcuttur. Müşteriye yönelik chatbotlar, şirket müşterilerinin bilgi ve hizmete erişimini kolaylaştırır, mevcutta çağrı merkezleri, web siteleri ve mobil uygulamalarla aldıkları hizmeti, talep veya sorularını direkt olarak, daha basit şekilde almalarını sağlar. Ürün arama, bilgi alma, işlem yapma, satış sonrası destek alma gibi konular chatbotların belli başlı kullanım alanlardır. Chatbotların bir diğer hedef kitlesi ise şirket çalışanlarıdır. Şirketi çalışanlarının günlük işleyişte sıkça ihtiyaç duydukları teknik bilgiler, şirket yönetmelikleri, insan kaynakları gibi alanlardaki bilgiler, şifre değiştirme, sistemsel hata bilgilendirmeleri gibi basit bilgi işlem konuları için chatbotlar kullanılabilir. Gartner 2022’ye kadar beyaz yaka çalışanların %70’inin2 Nisan 2020 tarihinde Wayback Machine sitesinde arşivlendi. günlük olarak bir diyalog bazlı yapay zeka sistemi ile iletişime geçeceğini tahmin etmektedir.


== Kaynakça =="
Büyük dil modeli,"Geniş dil modeli (GDM) (İngilizce: Large Language Model - LLM), genel amaçlı olarak dili anlama ve üretme becerisiyle öne çıkan bir yapay zeka dil modelidir. GDM'ler bu yetenekleri, eğitimleri sırasında milyarlarca parametreyi öğrenebilmek için niceliksel olarak çok büyük miktarda veri kullanarak kazanır. Bu süreçte, aşırı derecede büyük hesaplama kaynakları tüketirler. GDM'ler, gelişmiş yapay sinir ağlarıdır (temelde dönüştürücüler ) ve özdenetimli öğrenme veya yarı denetimli öğrenme yöntemleri kullanılarak eğitilirler.
GDM'ler özbağlanımlı dil modelleri olarak, bir giriş metnini alıp bir sonraki belirteci veya sözcüğü tekrar tekrar tahmin ederek çalışır. 2020 yılına kadar, bir modelin belirli görevleri gerçekleştirebilmesi için uyarlanmasının tek yolu ince ayardı. Ancak günümümüzde GPT-3 gibi popüler olan daha büyük ölçekli modeller, benzer sonuçlar elde etmek için sufle mühendisliğini kullanacak şekilde tasarlanmaya başlandı. GDM'lerin, insan dili derleminde bulunan sözdizimi, anlambilim ve ontoloji hakkında somut bilgi edinebilmenin yanı sıra, aynı zamanda derlemde yer alan hataları ve önyargıları da öğrendikleri düşünülmektedir.

Uygulama Alanları
Geniş dil modelleri, aşağıdaki başlıca alanlarda kullanılmaktadır:

Chatbot ve dijital asistanların geliştirilmesi
Makine çevirisi ve çok dilli iletişim
Otomatik özetleme ve bilgi çıkarımı
Kod üretimi, hata tespiti ve otomasyon
Eğitim ve içerik üretiminde kişiselleştirilmiş yardımcı sistemler

Etik ve Eleştiriler
Veri gizliliği ve telif hakkı ihlalleri riskleri
Dil modeli tarafından öğrenilen önyargıların (bias) sonuçlara yansıması
Enerji tüketimi ve karbon ayak izi gibi çevresel etkiler
Yanlış bilgi üretme potansiyeli (hallucination) ve güvenilirlik sorunları

Gelecek Perspektifi
GDM'lerin çok modlu modeller ile birleşmesi, görsel, işitsel ve metinsel verilerle bütünleşik yapay zeka sistemlerinin oluşmasını sağlayacaktır. Ayrıca, küçük ve verimli modellerin artmasıyla yerel cihazlarda kullanım olanakları genişlemektedir. GDM'ler, yapay genel zekaya (AGI) yönelik kritik bir basamak olarak değerlendirilmektedir.


== Kaynakça =="
Yapay zekâ uygulamaları,"Yapay zeka uygulaması (YZ), yapay zekânın çeşitli alanlarda aktif olarak kullanılma yöntemidir.
Yapay zeka, modern toplumda birçok uygulamalara sahip genel amaçlı bir teknolojidir. Yapay zeka sistemleri, endüstriyel üretimde, bilimsel araştırmada, dilbilimde, sanatta, e-ticarette, muhasebe ve finans, mühendislik, sağlık, eğitim, ulaşım vb. alanlarda yaygın kullanılmaktadır.

Yapay zeka uygulamaları
Biyoenformatik'te makine öğrenimi
Deepfake
Yer bilimleri'nde makine öğrenimi
Üretken yapay zekâ
Yapay zekâ sanatı
Üretken ses
Müzik ve yapay zekâ
Hükûmet'te yapay zekâ
Sağlık hizmetlerinde yapay zekâ
Ruh sağlığında yapay zekâ
Sanayide yapay zekâ
Yapay zekâ destekli yazılım geliştirme
Makine çevirisi
Yapay zekâ silahlanma yarışı
Fizikte makine öğrenimi
Yapay zekâ projeleri

Kaynakça
Ayrıca bakınız
Yapay Zekâ Soğuk Savaşı
Yapay zekâ tarihi"
Yapay zekâ tarihi,"Yapay zeka tarihi, ilk önce yapay varlıklara ilişkin mitler, öyküler ve söylentilerle başladı. İlk yapay zekâ alanı, 1956 yazında ABD'de Dartmouth Konferansında düzenlenen bir seminerde ortaya çıktı. 1974 yılında, ABD ve İngiliz hükûmetleri yapay zeka alanındaki araştırmaları finanse etmeyi bıraktı. Bu olay daha sonra ""yapay zekâ kışı"" olarak anıldı.
Ama 2012 yılında Toronto Üniversitesi'nden bir araştırma ekibi ilk kez yapay sinir ağları ve derin öğrenme tekniklerini kullanarak bilgisayarlı görü alanında hata oranını %25'in altına düşürmüştür. Bu olay, dünyada on yıl içinde yapay zeka patlamasına öncülük etti.

Kaynakça
Ayrıca bakınız
Yapay zekâ alanında ilerleme
Yapay zekâ çağı
Yapay zekâ zaman çizelgesi
Yapay sinir ağları tarihi
Bilgi temsili ve akıl yürütme"
Yapay zekâ felsefesi,"Yapay zeka felsefesi, yapay zekayı ve yapay zekanın, etik, bilinç, epistemoloji ve özgür irade bilgi ve anlayışı üzerindeki etkilerini araştıran teknoloji felsefesinin bir dalıdır. Ayrıca teknoloji, yapay hayvanların veya yapay insanların yaratılmasıyla ilgilidir, bu nedenle disiplin, filozoflar için oldukça ilgi çekicidir. Bu faktörler yapay zeka felsefesinin ortaya çıkmasına katkıda bulunmuştur. Bazı akademisyenler, AI topluluğunun felsefeyi reddetmesinin zararlı olduğunu savunmaktadır.
Yapay zeka felsefesi, bu tür soruları şu şekilde yanıtlamaya çalışır:

Bir makine akıllıca hareket edebilir mi? Bir insanın düşünerek çözeceği herhangi bir sorunu çözebilir mi?
İnsan zekası ve makine zekası aynı mıdır? Makine zekası insan zekasından üstün olabilir mi? İnsan beyni aslında bir bilgisayar mı?
Bunun gibi sorular, sırasıyla AI araştırmacılarının, bilişsel bilim adamlarının ve filozofların farklı ilgi alanlarını yansıtır. Bu soruların bilimsel cevapları, ""akıl"" ve ""bilinç"" tanımlarına ve tam olarak hangi ""makinelerin"" tartışıldığına bağlıdır.
AI felsefesindeki önemli önermeler aşağıdakilerden bazılarını içerir:

Turing testi: Bir makine bir insan kadar akıllıca davranıyorsa, o zaman bir insan kadar zekidir.
Dartmouth Konferansı: ""Öğrenmenin her yönü veya zekanın diğer herhangi bir özelliği o kadar kesin bir şekilde tanımlanabilir ki, onu simüle etmek için bir makine yapılabilir.""
Allen Newell ve Herbert A. Simon'ın fiziksel sembol sistemi hipotezi: ""Fiziksel bir sembol sistemi, genel akıllı eylem için gerekli ve yeterli araçlara sahiptir.""
John Searle'nin güçlü AI hipotezi: ""Doğru girdi ve çıktılara sahip, uygun şekilde programlanmış bir bilgisayar, bu nedenle, tam olarak insanların sahip olduğu anlamda bir akla sahip olacaktır..""

Bir makine genel zeka gösterebilir mi?
İnsanların zekasını kullanarak çözdüğü tüm sorunları çözebilecek bir makine yaratmak mümkün müdür? Bu soru, makinelerin gelecekte neler yapabileceğinin kapsamını tanımlar ve AI araştırmasının yönünü gösterir. Yalnızca makinelerin davranışıyla ilgilidir ve psikologların, bilişsel bilim adamlarının ve filozofların ilgilendiği konuları görmezden gelir; Bu soruyu cevaplamak için, bir makinenin gerçekten (bir kişinin düşündüğü gibi) düşünmesi veya sadece düşünüyormuş gibi davranması önemli değildir.
Çoğu AI araştırmacısının temel konumu, 1956 Dartmouth Konferansı teklifinde yer alan bu açıklamada özetlenmiştir:

""Öğrenmenin her yönü veya zekanın diğer herhangi bir özelliği o kadar kesin bir şekilde tanımlanabilir ki, onu simüle etmek için bir makine yapılabilir.""
Temel önermeye karşı argümanlar, çalışan bir yapay zeka sistemi inşa etmenin imkansız olduğunu, çünkü bilgisayarların yeteneklerinde bazı pratik sınırlar olduğunu veya akıllı davranış için gerekli olan ve yine de bir başkası tarafından kopyalanamayan insan zihninin bazı özel kalitesinin olduğunu göstermelidir (Makine veya mevcut AI araştırmasının yöntemleriyle). Temel öncül lehine argümanlar, böyle bir sistemin mümkün olduğunu göstermelidir.

Zeka
Turing test
Alan Turing, zekayı tanımlama sorununu konuşma hakkında basit bir soruya indirgedi. Eğer bir makine kendisine sorulan herhangi bir soruyu sıradan bir insanla aynı kelimeleri kullanarak cevaplayabiliyorsa, o zaman o makineye akıllı diyebiliriz. Deneysel tasarımının modern bir versiyonu, katılımcılardan birinin gerçek bir kişi olduğu ve katılımcılardan birinin bir bilgisayar programı olduğu çevrimiçi bir sohbet odası kullanır. Program, iki katılımcıdan hangisinin insan olduğunu kimse söyleyemezse testi geçer. Turing, hiç kimsenin (filozoflar hariç) ""insanlar düşünebilir mi?"" sorusunu sormadığını not eder. ""Bu nokta üzerinde sürekli tartışmak yerine, herkesin düşündüğü kibar bir uzlaşıma sahip olmak olağandır"" diye yazıyor. Turing'in testi, bu kibar kuralı makinelere kadar genişletir:

Bir makine bir insan kadar zekiyse, o zaman bir insan kadar zekidir.
Turing testinin eleştirilerinden biri, davranışın ""zekasını"" değil, yalnızca makinenin davranışının ""insanlığını"" ölçmesidir. İnsan davranışı ve akıllı davranış tam olarak aynı şey olmadığından, test zekayı ölçemez. Stuart J. Russell ve Peter Norvig, ""havacılık mühendisliği metinleri, alanlarının amacını 'diğer güvercinleri kandırabilecek kadar güvercinler gibi uçan makineler yapmak' olarak tanımlamazlar"" diye yazıyorlar.

Akıllı ajan tanımı
Yirmi birinci yüzyıl AI araştırması, zekayı akıllı ajanlar açısından tanımlar. Bir ""ajan"", bir ortamda algılayan ve hareket eden bir şeydir. Bir ""performans ölçüsü"", aracı için neyin başarı sayılacağını tanımlar.

""Bir etmen, geçmiş deneyim ve bilgiye dayalı olarak bir performans ölçüsünün beklenen değerini maksimize edecek şekilde hareket ederse, o zaman akıllıdır.""
Bunun gibi tanımlar zekanın özünü yakalamaya çalışır. Turing testinden farklı olarak, yazım hataları yapma veya aşağılanma yeteneği gibi zeki olmayan insan özelliklerini test etmeme avantajına sahiptirler.""Düşünen şeyler"" ile ""düşünmeyen şeyler"" arasında ayrım yapamamak gibi bir dezavantaja sahiptirler. Bu tanıma göre, bir termostatın bile ilkel bir zekası vardır.

Bir makinenin genel zekayı gösterebileceğine dair argümanlar
Hubert Dreyfus, bu argümanı, ""sinir sistemi, fizik ve kimya yasalarına uyuyorsa, ki bunu varsaymak için her türlü nedenimiz varsa, o zaman sinir sisteminin davranışını bazı fiziksel araçlarla yeniden üretebilmeliyiz"" iddiası olarak tanımlar. İlk olarak 1943 gibi erken bir tarihte ortaya atılan ve 1988'de Hans Moravec tarafından canlı bir şekilde açıklanan bu argüman, bilgisayar gücünün 2029 yılına kadar eksiksiz bir beyin simülasyonu için yeterli olacağını tahmin eden fütürist Ray Kurzweil ile ilişkilendiriliyor. 2005 yılında insan beynine (1011 nöron) büyüklüğünde bir talamokortikal modelin simülasyonu yapıldı ve 27 işlemciden oluşan bir kümede 1 saniyelik beyin dinamiğinin simüle edilmesi 50 gün sürdü.

İnsan düşüncesi sembol işlemedir
1963'te Allen Newell ve Herbert A. Simon, ""sembol manipülasyonunun"" hem insan hem de makine zekasının özü olduğunu öne sürdüler.

""Fiziksel bir sembol sistemi, genel akıllı eylem için gerekli ve yeterli araçlara sahiptir.""
Bu iddia çok güçlüdür: hem insan düşüncesinin bir tür sembol manipülasyonu olduğunu (çünkü zeka için bir sembol sistemi gereklidir) hem de makinelerin zeki olabileceğini (çünkü zeka için bir sembol sistemi yeterlidir) ima eder.

Sembol işlemeye karşı argümanlar
Bu argümanlar, insan düşüncesinin (yalnızca) üst düzey sembol manipülasyonundan oluşmadığını göstermektedir. Yapay zekanın imkansız olduğunu göstermiyorlar, sadece sembol işlemeden fazlasının gerekli olduğunu gösteriyorlar.

Dreyfus: örtük becerilerin önceliği
Hubert Dreyfus, insan zekasının ve uzmanlığının, açık sembolik manipülasyondan ziyade öncelikle örtük becerilere bağlı olduğunu savundu ve bu becerilerin asla resmi kurallarda ele geçirilmeyeceğini savundu.

Bir makinenin zihni, bilinci ve zihinsel durumları olabilir mi?
Bu, diğer zihinlerin sorunu ve zor bilinç sorunuyla ilgili felsefi bir sorudur. Soru, John Searle tarafından ""güçlü AI"" olarak tanımlanan bir konum etrafında dönüyor:

Fiziksel bir sembol sisteminin bir zihni ve zihinsel durumları olabilir.
Searle, bu konumu ""zayıf yapay zeka"" dediği durumdan ayırdı:

Fiziksel bir sembol sistemi akıllıca hareket edebilir.
Searle, güçlü yapay zekayı zayıf yapay zekadan ayırmak için terimleri tanıttı, böylece daha ilginç ve tartışmalı olduğunu düşündüğü şeye odaklanabildi. Tam olarak insan zihni gibi hareket eden bir bilgisayar programımız olduğunu varsaysak bile, yine de cevaplanması gereken zor bir felsefi soru olacağını savundu.

Düşünmek bir tür hesaplama mıdır?
Hesaplamalı zihin teorisi veya ""bilgisayarcılık"", zihin ve beyin arasındaki ilişkinin (aynı değilse de) çalışan bir program ile bilgisayar arasındaki ilişkiye benzer olduğunu iddia eder. Bu fikrin felsefi kökleri Hobbes (akıl yürütmenin ""hesaplamadan başka bir şey olmadığını"" iddia eden), Leibniz (tüm insan fikirlerinin mantıksal bir hesabını oluşturmaya çalışan), Hume (algının ""atomik izlenimlere"" indirgenebileceğini düşünen) ve hatta (biçimsel kurallar tarafından kontrol edilen tüm deneyimleri analiz eden) Kant bile  (tüm deneyimleri biçimsel kurallarla kontrol edildiği şekilde analiz eden) son sürüm filozoflar Hilary Putnam ve Jerry Fodor ile ilişkilidir.

Diğer ilgili sorular
Bir makinenin duyguları olabilir mi?
Eğer ""duygular"" sadece davranış üzerindeki etkileri veya bir organizmanın içinde nasıl işlev gördükleri açısından tanımlanırsa, duygular akıllı bir ajanın eylemlerinin faydasını en üst düzeye çıkarmak için kullandığı bir mekanizma olarak görülebilir. Bu duygu tanımı göz önüne alındığında, Hans Moravec ""robotların genel olarak iyi insanlar olma konusunda oldukça duygusal olacağına"" inanıyor.

Bir makine kendinin farkında olabilir mi?
Yukarıda belirtildiği gibi, ""öz farkındalık"" bazen bilimkurgu yazarları tarafından bir karakteri tamamen insan yapan temel insan özelliğinin adı olarak kullanılır. Turing, insanın diğer tüm özelliklerini ortadan kaldırır ve soruyu ""bir makine kendi düşüncesinin konusu olabilir mi?"" sorusuna indirger. Kendini düşünebilir mi? Bu şekilde bakıldığında, hata ayıklayıcı gibi kendi iç durumları hakkında rapor verebilen bir program yazılabilir.

Bir makine orijinal veya yaratıcı olabilir mi?
Turing bunu bir makinenin ""bizi şaşırtıp götüremeyeceği"" sorusuna indirger ve herhangi bir programcının onaylayabileceği gibi bunun açıkça doğru olduğunu iddia eder. Yeterli depolama kapasitesine sahip bir bilgisayarın astronomik sayıda farklı şekilde davranabileceğini belirtiyor. Fikirleri temsil edebilen bir bilgisayarın onları yeni şekillerde birleştirmesi mümkün, hatta önemsiz olmalıdır. (Douglas Lenat'ın Otomatik Matematikçisi, bir örnek olarak, yeni matematiksel gerçekleri keşfetmek için fikirleri birleştirdi.) Kaplan ve Haenlein, makinelerin bilimsel yaratıcılığı sergileyebileceğini öne sürerken, sanatsal yaratıcılığın söz konusu olduğu yerde insanların üstünlüğünün olması muhtemel görünüyor.

Felsefenin rolü üzerine görüşler
Bazı akademisyenler, AI topluluğunun felsefeyi reddetmesinin zararlı olduğunu savunuyor.Stanford Felsefe Ansiklopedisi'nde, bazı filozoflar, felsefenin yapay zekadaki rolünün yeterince takdir edilmediğini savunuyorlar. Fizikçi David Deutsch, felsefeyi veya onun kavramlarını anlamadan, AI gelişiminin ilerleme eksikliğinden muzdarip olacağını savunuyor.

Kaynakça
Ayrıca bakınız
Notlar
Adam, Alison (1989). Artificial Knowing: Gender and the Thinking Machine. Routledge & CRC Press. https://www.routledge.com/Artificial-Knowing-Gender-and-the-Thinking-Machine/Adam/p/book/9780415129633
Benjamin, Ruha (2019). Race After Technology: Abolitionist Tools for the New Jim Code. Wiley. ISBN 978-1-509-52643-7 14 Temmuz 2021 tarihinde Wayback Machine sitesinde arşivlendi.
Blackmore, Susan (2005), Consciousness: A Very Short Introduction, Oxford University Press 
Bostrom, Nick (2014), Superintelligence: Paths, Dangers, Strategies, Oxford University Press , 978-0-19-967811-2
Brooks, Rodney (1990), ""Elephants Don't Play Chess"" (PDF), Robotics and Autonomous Systems, 6 (1–2), ss. 3-15, CiteSeerX 10.1.1.588.7539 $2, doi:10.1016/S0921-8890(05)80025-9, 9 Ağustos 2007 tarihinde kaynağından arşivlendi (PDF)30 Ağustos 2007 
Bryson, Joanna (2019). The Artiﬁcial Intelligence of the Ethics of Artiﬁcial Intelligence: An Introductory Overview for Law and Regulation, 34.
Chalmers, David J (1996), The Conscious Mind: In Search of a Fundamental Theory, Oxford University Press, New York, ISBN 978-0-19-511789-9 
Cole, David (Sonbahar 2004), ""The Chinese Room Argument"",  Zalta, Edward N. (Ed.), The Stanford Encyclopedia of Philosophy, 27 Şubat 2021 tarihinde kaynağından arşivlendi20 Ağustos 2021 .
Crawford, Kate (2021). The Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence. Yale University Press.
Şablon:Crevier 1993
Dennett, Daniel (1991), Consciousness Explained, The Penguin Press, ISBN 978-0-7139-9037-9 
Dreyfus, Hubert (1972), What Computers Can't Do, New York: MIT Press, ISBN 978-0-06-011082-6 
Dreyfus, Hubert (1979), What Computers Still Can't Do, New York: MIT Press .
Dreyfus, Hubert; Dreyfus, Stuart (1986), Mind over Machine: The Power of Human Intuition and Expertise in the Era of the Computer, Oxford, UK: Blackwell 
Fearn, Nicholas (2007), The Latest Answers to the Oldest Questions: A Philosophical Adventure with the World's Greatest Thinkers, New York: Grove Press 
Gladwell, Malcolm (2005), Blink: The Power of Thinking Without Thinking, Boston: Little, Brown, ISBN 978-0-316-17232-5 .
Harnad, Stevan (2001), ""What's Wrong and Right About Searle's Chinese Room Argument?"",  Bishop, M.; Preston, J. (Ed.), Essays on Searle's Chinese Room Argument, Oxford University Press, 26 Ekim 2011 tarihinde kaynağından arşivlendi20 Ağustos 2021 
Haraway, Donna (1985). A Cyborg Manifesto.
Haugeland, John (1985), Artificial Intelligence: The Very Idea, Cambridge, Mass.: MIT Press .
Hofstadter, Douglas (1979), Gödel, Escher, Bach: an Eternal Golden Braid .
Horst, Steven (2009), ""The Computational Theory of Mind"",  Zalta, Edward N. (Ed.), The Stanford Encyclopedia of Philosophy, Metaphysics Research Lab, Stanford University, 11 Eylül 2018 tarihinde kaynağından arşivlendi20 Ağustos 2021 .
Kaplan, Andreas; Haenlein, Michael (2018), ""Siri, Siri in my Hand, who's the Fairest in the Land? On the Interpretations, Illustrations and Implications of Artificial Intelligence"", Business Horizons, cilt 62, ss. 15-25, doi:10.1016/j.bushor.2018.08.004 
Kurzweil, Ray (2005), The Singularity is Near, New York: Viking Press, ISBN 978-0-670-03384-3 .
Lucas, John (1961), ""Minds, Machines and Gödel"",  Anderson, A.R. (Ed.), Minds and Machines, 19 Ağustos 2007 tarihinde kaynağından arşivlendi20 Ağustos 2021 .
Malabou, Catherine (2019). Morphing Intelligence: From IQ Measurement to Artificial Brains. (C. Shread, Trans.). Columbia University Press.
McCarthy, John; Minsky, Marvin; Rochester, Nathan; Shannon, Claude (1955), A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, 30 Eylül 2008 tarihinde kaynağından arşivlendi .
McDermott, Drew (14 Mayıs 1997), ""How Intelligent is Deep Blue"", New York Times, 4 Ekim 2007 tarihinde kaynağından arşivlendi10 Ekim 2007 
Moravec, Hans (1988), Mind Children, Harvard University Press 
Penrose, Roger (1989), The Emperor's New Mind: Concerning Computers, Minds, and The Laws of Physics, Oxford University Press, ISBN 978-0-14-014534-2 
Searle, John (1980), ""Minds, Brains and Programs"" (PDF), Behavioral and Brain Sciences, 3 (3), ss. 417-457, doi:10.1017/S0140525X00005756, 23 Eylül 2015 tarihinde kaynağından (PDF) arşivlendi 
Searle, John (1992), The Rediscovery of the Mind, Cambridge, Massachusetts: M.I.T. Press 
Searle, John (1999), Mind, language and society, New York, NY: Basic Books, ISBN 978-0-465-04521-1, OCLC 231867665 
Şablon:Turing 1950
Yee, Richard (1993), ""Turing Machines And Semantic Symbol Processing: Why Real Computers Don't Mind Chinese Emperors"" (PDF), Lyceum, 5 (1), ss. 37-59, 24 Şubat 2021 tarihinde kaynağından arşivlendi (PDF)20 Ağustos 2021"
Otomatik akıl yürütme,"Otomatik akıl yürütme, bilgisayar biliminin (bilgi temsilini ve akıl yürütmeyi içerir) ve akıl yürütmenin farklı yönlerini anlamaya çalışan bir alandır. Otomatik akıl yürütme çalışması, bilgisayarların tamamen veya neredeyse tamamen otomatik olarak akıl yürütmesine izin veren bilgisayar programlarının üretilmesine yardımcı olur. Otomatik akıl yürütme, yapay zekanın bir alt alanı olarak görülse de, teorik bilgisayar bilimi ve felsefesi ile de bağlantıları vardır.
Otomatik akıl yürütmenin en gelişmiş alt alanları, otomatik teorem kanıtlama (ve etkileşimli teorem kanıtlamanın daha az otomatik ancak daha pragmatik alt alanı) ve otomatik kanıt denetimidir (sabit varsayımlar altında garantili doğru akıl yürütme olarak görülür). Tümevarım ve çeşitli yöntemler kullanılarak benzetme yoluyla akıl yürütme konusunda da kapsamlı çalışmalar yapılmıştır.
Diğer önemli konular arasında belirsizlik altında akıl yürütme ve monoton olmayan akıl yürütme yer alır. Belirsizlik alanının önemli bir kısmı, daha standart otomatik kesintinin üzerine daha fazla minimallik ve tutarlılık kısıtlamalarının uygulandığı tartışma alanıdır. John Pollock'un OSCAR sistemi, yalnızca otomatik bir teorem ispatlayıcısı olmaktan daha spesifik olan otomatik bir tartışma sisteminin bir örneğidir.
Otomatik akıl yürütme araçları ve teknikleri arasında klasik mantık ve hesap, bulanık mantık, Bayes çıkarımı, maksimum entropi ile akıl yürütme ve daha daha az resmi ad hoc teknikleri bulunur.

İlk yıllar
Biçimsel mantığın gelişimi, yapay zekanın gelişmesine yol açan otomatik akıl yürütme alanında büyük bir rol oynadı. Biçimsel bir kanıt, her mantıksal çıkarımın matematiğin temel aksiyomlarına geri döndürüldüğü bir kanıttır. İstisnasız tüm ara mantıksal adımlar sağlanır. Sezgiden mantığa çeviri rutin olsa bile sezgiye başvurulmaz. Bu nedenle, resmi bir kanıt daha az sezgiseldir ve mantıksal hatalara daha az duyarlıdır.
Bazıları, birçok mantıkçıyı ve bilgisayar bilimcisini bir araya getiren 1957 Cornell Yaz toplantısını otomatik akıl yürütmenin veya otomatik tümdengelimin kaynağı olarak görüyor. Diğerleri, bundan önce, Newell, Shaw ve Simon'ın 1955 Mantık Teorisi programıyla veya Martin Davis'in 1954'te Presburger'in karar prosedürünü uygulamasıyla (ki bu, iki çift sayının toplamının çift olduğunu kanıtladı) başladığını söylüyor.
Otomatik akıl yürütme, önemli ve popüler bir araştırma alanı olmasına rağmen, seksenlerde ve doksanların başlarında bir ""Yapay zeka kışı"" geçirdi. Ancak alan daha sonra yeniden canlandı. Örneğin, 2005 yılında Microsoft, iç projelerinin çoğunda doğrulama teknolojisini kullanmaya başladı ve 2012 Visual C sürümüne mantıksal bir belirtim ve kontrol dili eklemeyi planlıyor.

Önemli katkılar
Principia Mathematica, Alfred North Whitehead ve Bertrand Russell tarafından yazılan biçimsel mantıkta bir dönüm noktası çalışmasıdır. Principia Mathematica - aynı zamanda Matematik İlkeleri anlamına gelir - matematiksel ifadelerin tamamını veya bir kısmını sembolik mantık açısından türetmek amacıyla yazılmıştır. Principia Mathematica ilk olarak 1910, 1912 ve 1913'te üç cilt halinde yayınlanmıştır.

Mantık Teorisi, 1956'da Allen Newell, Cliff Shaw ve Herbert A. Simon tarafından teoremleri ispatlamada ""insan akıl yürütmesini taklit etmek"" için geliştirilen ilk programdı ve Principia Mathematica'nın ikinci bölümünden elli iki teorem üzerinde gösterilmiştir. Bunlardan otuz sekiz tanesi kanıtlandı. Program, teoremleri kanıtlamaya ek olarak, Whitehead ve Russell tarafından sağlanandan daha zarif olan teoremlerden biri için bir kanıt bulmuştur. Sonuçlarını yayınlamak için başarısız bir girişimden sonra, Newell, Shaw ve Herbert 1958'deki The Next Advance in Operation Research adlı yayınlarında şunları bildirdiler:""Artık dünyada düşünen, öğrenen ve yaratan makineler var. Ayrıca, (görünür bir gelecekte) baş edebilecekleri problemler yelpazesi insan zihninin uygulanmış olduğu menzil ile aynı seviyeye gelene kadar, bu şeyleri yapma yetenekleri hızla artacaktır.""

Kanıt sistemleri
Boyer-Moore Teoremi Kanıtı
NQTHM (Boyer-Moore Theorem Prover; NQTHM)'nin tasarımı John McCarthy ve Woody Bledsoe'dan etkilenmiştir. 1971'de Edinburgh, İskoçya'da başlatılan bu, Pure Lisp kullanılarak oluşturulmuş tam otomatik bir teorem ispatıdır. NQTHM'nin ana yönleri şunlardı:

Lisp'in çalışma mantığı olarak kullanılması.
Toplam özyinelemeli -fonksiyonlar için bir tanım ilkesine dayanma.
Yeniden yazma ve ""sembolik değerlendirme""nin kapsamlı kullanımı.
Sembolik değerlendirmenin başarısızlığına dayanan bir tümevarım buluşsal yöntemi.

HOL Light
OCaml'de yazılan HOL Light, basit ve temiz bir mantıksal temele ve düzenli bir uygulamaya sahip olacak şekilde tasarlanmıştır. Klasik yüksek mertebeden mantık için başka bir ispat yardımcısıdır.

Coq
Fransa'da geliştirilen Coq, çalıştırılabilir programları spesifikasyonlardan Objective CAML veya Haskell kaynak kodu olarak otomatik olarak çıkarabilen başka bir otomatik prova asistanıdır. Özellikler, programlar ve kanıtlar, Endüktif Yapıların Hesabı (Calculus of Inductive Constructions; CIC) adı verilen aynı dilde resmîleştirilir.

Uygulamalar
Otomatik muhakeme, otomatikleştirilmiş teorem kanıtlayıcıları oluşturmak için en yaygın şekilde kullanılmıştır. Bununla birlikte, çoğu zaman, teorem kanıtlayıcılar, etkili olmak için bazı insan rehberliğini gerektirmektedir. Bu nedenle daha genel olarak kanıt yardımcıları olarak nitelendirilmektedir. Bazı durumlarda bu tür ispatlayıcılar bir teoremi ispatlamak için yeni yaklaşımlar geliştirmiştir. Mantık Teorisi buna iyi bir örnektir. Program, Principia Mathematica'daki teoremlerden biri için Whitehead ve Russell tarafından sağlanan kanıttan daha verimli (daha az adım gerektiren) bir kanıt bulmuştur. Biçimsel (Formel) mantık, matematik ve bilgisayar bilimleri, mantık programlama, yazılım ve donanım doğrulama, devre tasarımı ve diğer birçok alanda artan sayıda sorunu çözmek için otomatik akıl yürütme programları uygulanmaktadır. TPTP, düzenli olarak güncellenen bu tür problemlerin bir kütüphanesidir. CADE konferansında düzenli olarak düzenlenen otomatik teorem kanıtlayıcılar arasında da bir rekabet vardır (Pelletier, Sutcliffe ve Suttner 2002); yarışma için problemler TPTP kütüphanesinden seçilmektedir.


== Kaynakça =="
Bilgi temsili,"Bilgi temsili ve akıl yürütme, dünyayla ilgili bilgilerin bir bilgisayar sistemi tarafından anlaşılıp işlenebilecek şekilde ifade edilmesini hedefleyen yapay zeka alanıdır. Bilgi temsili bir bilgisayar destekli sağlık teşhisi ya da doğal dilde konuşma gibi karmaşık yapay zeka problemlerinin çözülmesinde kilit rol oynar. Bilgi temsili araştırmaları, insanların problemleri nasıl çözdüğünü anlamak için psikolojiden yardım alır ve karmaşık sistemleri basitleştirecek matematiksel biçimler geliştirir. Aynı zamanda mantık bulgularını kullanarak çeşitli akıl yürütme biçimlerini otomatikleştirir.


== Kaynakça =="
Örüntü tanıma,"Örüntü tanıma, sürekli devam ve tekrar eden şekiller olan örüntüleri tanımaya yarayan bir teknolojidir. Bir şeklin orantılı olarak büyütülüp veya küçültülmesine ise fraktal denir.

Uygulama alanları
Optik karakter tanıma
Konuşma ve Konuşmacı tanıma
Parmak izi tanıma
DNA kimliklendirme
Otomatik savunma sistemleri
Fabrika üretim hata denetim sistemleri


== Örnek örüntü tanıma uygulamaları =="
Yapay genel zekâ,"Yapay genel zeka (YGZ), bir insanın yapabileceği herhangi bir zihinsel görevi başarıyla gerçekleştirebilecek bir makinenin zekasıdır. Günümüzdeki bazı yapay zeka araştırmalarının temel amacıdır ve bilimkurgu ve fütüroloji'de de ortak bir konudur. Bazı araştırmacılar Yapay genel zekâyı ""güçlü yapay zekâ"", ""tam yapay zekâ"" veya bir makinenin ""genel akıllı eylem"" gerçekleştirme kabiliyeti olarak adlandırmaktadır; diğerleri ise sadece bilinci deneyimleyen makineler için ""güçlü yapay zekâ"" tabirini kullanmaktadır.
Bazı kaynaklar güçlü yapay zekâ ile ""uygulamalı yapay zekâ"" (""dar yapay zekâ"" veya ""zayıf yapay zekâ"" olarak da bilinir) arasındaki ayrımı şöyle vurgular: yazılım kullanarak özel problem çözmek veya muhakeme görevlerini incelemek. Zayıf yapay zekâ, güçlü yapay zekâ'nın aksine, insan bilişsel yeteneklerinin tamamını gerçekleştirmeye yönelik değildir.
2017 yılı itibarıyla dünya genelinde kırktan fazla kuruluş yapay genel zekâ hakkında aktif araştırmalar yapmaktadır.

Gereksinimler
Zekâ için çeşitli kriterler belirlenmiştir (bunlardan en önemlisi Turing testi) ancak bugüne kadar herkesi tatmin edecek bir tanım belirlenememiştir. Bununla birlikte, yapay zeka araştırmacıları arasında, genel bir zekâ'nın aşağıdakileri yapması gerektiği hususunda anlaşma vardır:

Gerekçe, strateji kullanmak, bulmacaları çözmek ve belirsizlik altında karar vermek;
Bilgiyi temsil etmek, ortak bilgi dahil;
Planlama;
Makine öğrenimi;
Doğal dil ile iletişim kurmak;
ve tüm bu becerileri ortak hedeflere yönlendirmek.
Diğer önemli yetenekler arasında, Duyu ve hareket etme (örneğin nesneleri taşıma ve kullanma) yeteneği de bulunur. Bu, tehlikeyi tespit etme ve bunlara müdahale etme yeteneğini de içerir. Zekâ konusundaki birçok disiplinlerarası yaklaşım (örneğin bilişsel bilim), hayal gücü (programlanamayan zihinsel imgeler ve kavramlar oluşturma kabiliyeti) ve otonomi gibi ek özellikleri göz önünde bulundurma ihtiyacını vurgulamaya meyillidir. Bu yeteneklerin çoğunu sergileyen bilgisayar tabanlı sistemler mevcuttur (örneğin hesaplamalı yaratıcılık, otomatik muhakeme, karar destek sistemi, robot, evrimsel hesaplama), ancak henüz hiçbiri insan seviyesinde değildir.

Yapay genel zekâ'yı onaylayan testler
Turing testi (Turing)
İkinci bir insana görünmeden bir makine ve bir insan konuşmalıdır, ikinci insan bu iki konuşundan hangisinin makine olduğunu anlamalıdır, eğer makine bu ikinci insanı kısıtlı süre içinde kandırıp kendini insan olarak düşündürürse testi geçer. Not: Turing zeka için yeterli bir test değildir, ama akıllı bir makinenin geçmesi gerekli olduğu bir testtir.
Kahve Testi (Wozniak)
Bir makine sıradan bir Amerikan evine girmek ve nasıl kahve yapılacağını bulması gereklidir: kahve makinesini bulmak, kahve bulmak, su eklemek, bir kupa bulmak ve uygun düğmelere basarak kahveyi demlemek.
Robot Kolej Öğrencisi Testi (Goertzel)
Bir makine bir üniversiteye kaydolur, insanların alacağı aynı sınıfları alıp geçer ve bir derece alır.
Görevlendirilme Testi (Nilsson)
Bir makine ekonomik olarak önemli bir işe girer ve o işi bir insan kadar iyi bir şekilde gerçekleştirir.
Düz paket mobilya testi (Tony Severyns)
Bir makine, mobilya paketini açmak ve bir araya getirmek zorundadır. Tüm armatürleri doğru şekilde yerleştirerek talimatları okumalı ve ürünü tanımlandığı şekilde monte etmelidir.

İhtilaflar ve tehlikeler
Yapılabilirlik
Ağustos 2020 itibarıyla, yapılabilirliği henüz ispatlanamadığından dolayı YGZ çok spekülatif kalmaktadır. Yapay genel zekanın yapılıp yapılamayacağı veya ne zaman meydana geleceği hakkında farklı görüşler vardır. Günümüze değin herhangi bir YGZ örneği görülmemiş olup, halen teorik bir çalışma alanıdır.

İnsan varlığına karşı oluşturduğu tehdit
Bu tez YGZ'nin varoluşsal bir tehdit oluşturduğunu ve bu tehdidin gereken ilgiyi görmediğini savunur. Bu görüşü savunmasıyla bilinen isimlerden en ünlüleri Elon Musk, Bill Gates ve Stephen hawking iken en önde gelen yapay zeka araştırmacısı ise Stuart J. Russell'dir.
Varoluşsal risk hakkında kaygılanan akademisyenlerin birçoğuna göre bu konuda mesafe almanın en iyi yolu 'kontrol problemini' çözmeye yönelik büyük çaplı araştırmalar yapmaktır. Burada amaç sürekli biçimde kendini geliştiren yapay zekanın, süper-zeka seviyesine geldiğinde yıkıcı değil dostane davranma ihtimalini maksimize etmek için programcıların ne tür güvenlik önlemlerini, algoritmaları ve mimari yapıları uygulamaları gerektiği sorusuna yanıt bulmaktadır.

Acı riskleri
Ana makale: Acı riskleri
Acı riskleri hakkında araştırma yapan bazı aktivistler, başıbozuk bir YGZ'nin galaksiyi kolonize etmek gibi hedeflerine ulaşmak amacıyla bilim ve mühendislik alanlarında bir dizi ilginç başarılar elde edeceğini fakat bunu yaparken kendinden daha zayıf varlıkların acı çekmelerini önlemek gibi insanı değerlere saygı duymayabileceğine dikkat çekmişlerdir.


== Kaynakça =="
Yapay zekâ güvenliği,"Yapay zekâ güvenliği, yapay zekâ sistemlerinden kaynaklanabilecek kazaları, kötüye kullanımı veya diğer zararlı sonuçları önlemekle ilgilenen disiplinler arası bir alandır. Yapay zekâ sistemlerini ahlaki ve faydalı hale getirmeyi amaçlayan makine etiği ile yapay zekâ uyumunu kapsar ve yapay zekâ güvenliği, riskler için sistemleri izlemek ve onları son derece güvenilir hale getirmek gibi teknik sorunları kapsar. Yapay zekâ araştırmalarının ötesinde, güvenliği teşvik eden normlar ve politikalar geliştirmeyi içerir.

Motivasyonlar
Yapay zekâ araştırmacıları, yapay zekâ teknolojisinin oluşturduğu riskin ciddiyeti ve birincil kaynakları hakkında farklı görüşlere sahiptir. Yine de anketler, uzmanların yüksek sonuçlu riskleri ciddiye aldığını göstermektedir. Yapay zekâ araştırmacılarıyla yapılan iki ankette, katılımcıların ortalamasının genel olarak yapay zekâ konusunda iyimser olduğu, ancak gelişmiş yapay zekânın ""son derece kötü (örneğin insan neslinin tükenmesi)"" bir sonuca yol açma olasılığının %5 olduğu görülmüştür. 2022 yılında Doğal dil işleme (NLP) topluluğunda yapılan bir ankete katılanların %37'si, yapay zekâ kararlarının ""en az topyekün bir nükleer savaş kadar kötü"" bir felakete yol açabileceğinin makul olduğunu kabul etmiş veya zayıf bir şekilde kabul etmiştir. Akademisyenler kritik sistem arızalarından, önyargıdan, ve yapay zekâ destekli gözetimden kaynaklanan mevcut riskleri tartışmaktadır; teknolojik işsizlik, dijital manipülasyon  ve silahlanmadan kaynaklanan riskler; ve gelecekteki yapay genel zekânın kontrolünü kaybetmekten kaynaklanan spekülatif riskleri göze almaktadırlar.
Bazıları, 2015'te bunları ""henüz gezegene ayak basmadığımız halde Mars'ta aşırı nüfus konusunda endişelenmek"" ile karşılaştıran Andrew Ng gibi yapay genel zekâ ile ilgili endişeleri eleştirmiştir. Öte yandan Stuart J. Russell, Diğer taraftan Stuart J. Russell, ""insan yaratıcılığını tahmin edebilmenin onu hafife almaktan daha iyi olduğunu"" savunarak dikkatli olunması çağrısında bulunuyor.

Arka plan
Yapay zekâdan kaynaklanan riskler bilişim çağının başlangıcında ciddi olarak tartışılmaya başlandı:

Dahası, öğrenen ve davranışları deneyimle değiştirilen makineler yapma yönünde ilerlersek, makineye verdiğimiz her bağımsızlık derecesinin, isteklerimize olası bir meydan okuma derecesi olduğu gerçeğiyle yüzleşmeliyiz.
2008-2009 yılları arasında Yapay Zekâ Geliştirme Derneği, yapay zekâ araştırma ve geliştirmesinin potansiyel uzun vadeli toplumsal etkilerini araştırmak ve ele almak üzere bir çalışma yaptırmıştır. Panel, bilimkurgu yazarları tarafından ifade edilen radikal görüşlere genel olarak şüpheyle yaklaşmış, ancak ""beklenmedik sonuçları en aza indirmek için karmaşık hesaplama sistemlerinin davranış yelpazesini anlama ve doğrulama yöntemleri konusunda ek araştırmaların değerli olacağı"" konusunda hemfikir olunmuştur.
Roman Yampolskiy 2011 yılında Yapay Zekâ Felsefesi ve Teorisi konferansında "" Yapay zekâ güvenlik mühendisliği"" terimini ortaya atmış, yapay zekâ sistemlerinin önceki başarısızlıklarını sıralamış ve ""yapay zekâlar daha yetenekli hale geldikçe bu tür olayların sıklığı ve ciddiyetinin giderek artacağını"" savunmuştur.
Filozof Nick Bostrom 2014 senesinde ""Süper Zekâ:Yollar, Tehlikeler, Stratejiler"" adlı kitabını yayınladı. Bostrom, yapay zekânın yükselişinin, işgücünün yapay zekâ tarafından yerinden edilmesinden, siyasi ve askeri yapıların manipüle edilmesine ve hatta insan neslinin tükenme olasılığına kadar çeşitli toplumsal sorunlar yaratma potansiyeline sahip olduğu görüşündedir. Gelecekteki gelişmiş sistemlerin insan varlığına tehdit oluşturabileceği yönündeki argümanı Elon Musk, Bill Gates  ve Stephen Hawking'in de benzer endişeleri dile getirmesine neden olmuştur.
2015 yılında onlarca yapay zekâ uzmanı, yapay zekânın toplumsal etkileri üzerine araştırma yapılması çağrısında bulunan ve somut yönergeler belirleyen bir açık mektuba imza atmıştır. Mektup bugüne kadar Yann LeCun, Shane Legg, Yoshua Bengio ve Stuart Russell'ın da aralarında bulunduğu 8000'den fazla kişi tarafından imzalandı.
Aynı yıl, profesör Stuart Russell liderliğindeki bir grup akademisyen California Berkeley Üniversitesinde İnsan Uyumlu Yapay Zekâ Merkezi'ni kurdu ve Yaşamın Geleceği Enstitüsü ""yapay zekânın güvenli, etik ve faydalı kalmasını sağlamayı"" amaçlayan araştırmalar için 6,5 milyon dolar hibe desteği sağladı.
2016 yılında Beyaz Saray Bilim ve Teknoloji Politikası Ofisi ve Carnegie Mellon Üniversitesi, yapay zekânın ""avantajlarını ve dezavantajlarını"" araştırmayı amaçlayan dört Beyaz Saray çalıştayından biri olan Yapay Zekâ için Güvenlik ve Kontrol Kamu Çalıştayı'nı duyurdu.  Aynı yıl, ilk ve en etkili teknik Yapay Zekâ Güvenliği gündemlerinden biri olan ""Concrete Problems in AI Safety"" yayınlandı.
2017 yılında Yaşamın Geleceği Enstitüsü, 100'den fazla düşünce liderinin aşağıdakiler de dahil olmak üzere faydalı yapay zekâ için ilkeleri formüle ettiği Asilomar Faydalı Yapay Zekâ Konferansı'na sponsor oldu. ""Yarıştan Kaçınma: Yapay zekâ sistemleri geliştiren ekipler, güvenlik standartlarında köşe dönmekten kaçınmak için aktif olarak işbirliği yapmalıdır."" 
2018 yılında DeepMind Safety ekibi, teknik özellik, sağlamlık ve güvence konularında yapay zekâ güvenlik sorunlarının ana hatlarını ortaya koydu. Ertesi yıl, araştırmacılar ICLR'de bu sorun alanlarına odaklanan bir çalıştay düzenlediler.
2021 yılında, sağlamlık, izleme, hizalama ve sistemik güvenlik alanlarındaki araştırma yönlerini özetleyen ""Unsolved Problems in Machine Learning Safety"" yayınlandı.
Rishi Sunak, 2023 yılında Birleşik Krallık'ın ""küresel yapay zekâ güvenlik düzenlemesinin merkezi"" olmasını ve yapay zekâ güvenliği konusundaki ilk küresel zirveye ev sahipliği yapmasını istediğini söyledi.

Araştırma odağı
Yapay zekâ güvenlik araştırma alanları arasında sağlamlık, izleme ve hizalama yer almaktadır.

Sağlamlık
Saldırıya karşı sağlamlık
Yapay zekâ sistemleri genellikle rakip örneklere veya ""bir saldırganın kasten modelin hata yapmasına neden olacak şekilde tasarladığı makine öğrenimi (ML) modellerindeki girdilere"" karşı savunmasız durumdadır. Örneğin, 2013 yılında Szegedy ve arkadaşları, bir görüntüye belirli algılanamaz bozulmalar eklemenin, görüntünün yüksek güvenle yanlış sınıflandırılmasına neden olabileceğini ortaya çıkarmıştır. Bu durum sinir ağları ile ilgili bir sorun olmaya devam etmektedir, ancak son çalışmalarda bozulmalar genellikle algılanabilecek kadar büyük olmaktadır.

Sağdaki tüm görüntülerin bozulma uygulandıktan sonra devekuşu olduğu tahmin edilmektedir. Solda yapay zekâ köpeği doğru tahin ediyor ama tititzlikle eklenen gürültü yüzünden yapay zekâ köpeği bir devekuşu olarak sınıflandırıyor. Bu da bize dikkatli bir şekilde oluşturulmuş gürültülerin yapay zekâyı yanlış sınıflandırmaya itebileceğini göstermektedir.
Saldırıya karşı sağlamlık genellikle güvenlikle ilişkilendirilir. Araştırmacılar, bir ses sinyalinin fark edilmeyecek şekilde değiştirilebileceğini ve böylece konuşmadan metne sistemlerinin bunu saldırganın seçtiği herhangi bir mesaja dönüştürebileceğini göstermiştir. Ağa izinsiz giriş  ve kötü amaçlı yazılım  tespit sistemleri de saldırganlara karşı dayanıklı olmalıdır çünkü saldırganlar saldırılarını bu tespit edicileri kandırmak için tasarlayabilir.
Hedefleri temsil eden modeller (ödül modelleri) de tersine dayanıklı olmalıdır. Örneğin, bir ödül modeli bir metin yanıtının ne kadar yararlı olduğunu tahmin edebilir ve bir dil modeli bu puanı en üst düzeye çıkarmak için eğitilebilir. Araştırmacılar, bir dil modelinin yeterince uzun süre eğitilmesi durumunda, daha iyi bir puan elde etmek ve amaçlanan görevde daha kötü performans göstermek için ödül modelinin zayıflıklarından yararlanacağını göstermiştir. Bu sorun, ödül modelinin düşmana karşı sağlamlığı geliştirilerek ele alınabilir. Daha genel olarak, başka bir yapay zekâ sistemini değerlendirmek için kullanılan herhangi bir yapay zekâ sistemi agresif olarak güvenilir olmalıdır. Bu, izleme araçlarını da içerebilir, çünkü daha yüksek bir ödül üretmek için potansiyel olarak manipüle edilebilirler.

İzleme
Belirsizliğin tahmin edilmesi
İnsan operatörlerin, özellikle tıbbi teşhis gibi yüksek riskli ortamlarda, bir yapay zekâ sistemine ne kadar güvenmeleri gerektiğini ölçmeleri genellikle çok önemlidir. Makine öğrenimi modelleri genellikle olasılık çıktısı vererek güven ifade eder; bununla birlikte, özellikle ele almak üzere eğitildiklerinden farklı durumlarda genellikle aşırı güven duyarlar. Kalibrasyon araştırmaları, model olasılıklarının modelin doğru olma oranına mümkün olduğunca yakın olmasını amaçlamaktadır.
Benzer şekilde, anomali tespiti veya dağılım dışı tespiti, bir yapay zekâ sisteminin olağandışı bir durumda olduğunu tespit etmeyi amaçlamaktadır. Örneğin, otonom bir araçtaki bir sensör arızalıysa veya zorlu bir araziyle karşılaşırsa, sürücüyü kontrolü ele alması veya kenara çekmesi için uyarması gerekmektedir. Anomali tespiti, basitçe anormal ve anormal olmayan girdileri ayırt etmek için bir sınıflandırıcıyı eğiterek uygulanmaktadır, ancak bir dizi ek yöntem de kullanılmaktadır.

Kötü niyetli kullanımın tespiti
Akademisyenler  ve devlet kurumları, yapay zekâ sistemlerinin kötü niyetli aktörlerin silah üretmesine, kamuoyunu manipüle etmesine, veya siber saldırıları otomatikleştirmesine yardımcı olmak için kullanılabileceği yönündeki endişelerini dile getirmiştir. Bu endişeler, güçlü yapay zekâ araçlarını çevrimiçi olarak barındıran OpenAI gibi şirketler için mevcut bir endişe sebebidir. Kötüye kullanımı önlemek için OpenAI, kullanıcıları etkinliklerine göre işaretleyen veya kısıtlayan algılama sistemleri geliştirmiştir.

Şeffaflık
Sinir ağları sıklıkla kara kutular olarak tanımlanır; yani gerçekleştirdikleri çok sayıda hesaplama sonucunda aldıkları kararları neden aldıklarını anlamak oldukça zordur. Bu da arızaları önceden tahmin etmeyi zorlaştırmaktadır. 2018 yılında sürücüsüz bir araç, bir yayayı tespit edemeyerek ölümüne neden oldu. Yapay zekâ yazılımının kara kutu niteliği nedeniyle, arızanın nedeni belirsizliğini korumaktadır.
Şeffaflığın kritik faydalarından biri açıklanabilirliktir. Bazen, örneğin iş başvurularını otomatik olarak filtrelemek veya kredi notu atamak gibi, adaleti sağlamak için bir kararın neden verildiğine dair bir açıklama yapmak yasal bir zorunluluktur.
Bir diğer faydası ise arızaların nedeninin ortaya çıkarılmasıdır. Araştırmacılar, 2020 COVID-19 pandemisinin başlangıcında, tıbbi görüntü sınıflandırıcılarının alakasız hastane etiketlerine 'dikkat ettiğini' göstermek için şeffaflık araçlarını kullandılar.
Şeffaflık yöntemleri hataları düzeltmek için de kullanılabilir. Örneğin, ""Locating and Editing Factual Associations in GPT"" başlıklı makalede yazarlar, Eyfel kulesinin konumuyla ilgili soruları nasıl cevapladığını belirleyen model parametrelerini tespit edebilmişlerdir. Daha sonra bu bilgiyi düzenleyerek modelin sorulara kulenin Fransa yerine Roma'da olduğuna inanıyormuş gibi yanıt vermesini sağlamışlardır. Bu durumda yazarlar bir hataya neden olmuş olsalar da, bu yöntemler potansiyel olarak hataları etkili bir şekilde düzeltmek için kullanılabilir. Model düzenleme teknikleri bilgisayarla görüntülemede de mevcuttur.
Son olarak, bazı kişiler yapay zekâ sistemlerinin şeffaf olmamasının önemli bir risk unsuru olduğunu ve bu sistemlerin nasıl çalıştığının daha iyi anlaşılmasının gelecekte büyük çaplı arızaları önleyebileceğini ileri sürmüştür.""İçsel"" yorumlanabilirlik araştırması makine öğrenimi modellerini şeffaf olmaktan çıkarmayı amaçlamaktadır. Bu araştırmanın bir amacı, iç nöron aktivasyonlarının neyi temsil ettiğini tespit etmektir. Örneğin, araştırmacılar CLIP yapay zekâ sisteminde örümcek adam kostümlü insanların görüntülerine, örümcek adam çizimlerine ve 'örümcek' kelimesine tepki veren bir nöron tanımladılar. Ayrıca bu nöronlar veya 'devreler' arasındaki bağlantıların da izah edilmesini içermektedir. Örneğin, araştırmacılar dil modellerinin bağlamlarından nasıl öğrendiklerinde rol oynayabilecek transformatör dikkatindeki örüntü eşleştirme mekanizmalarını tanımlamışlardır. ""İçsel yorumlanabilirlik"" sinirbilim ile mukayese edilmektedir. Her iki durumda da amaç karmaşık bir sistemde neler olup bittiğini anlamaktır, bununla birlikte makine öğrenimi araştırmacıları mükemmel ölçümler alabilme ve keyfi çıkarımlar yapabilme avantajına sahiptir.

Trojanları tespit etmek
Makine öğrenimi modelleri potansiyel olarak 'trojan' veya 'arka kapı' içerebilmektedir: bunlar kötü niyetli aktörlerin bir yapay zekâ sisteminde oluşturdukları güvenlik açıklarıdır. Örneğin, trojanlı bir yüz tanıma sistemi, belirli bir mücevher parçası göründüğünde erişim izni verebilir; veya trojanlı bir otonom araç, belirli bir hareket tetikleyicisi görünene kadar normal çalışabilir. Bir saldırganın bir trojan yerleştirebilmesi için sistemin eğitim verilerine erişimi olması gerektiğini unutulmamalıdır. CLIP veya GPT-3 gibi bazı büyük modellerde halka açık internet verileriyle eğitildiklerinden bunu yapmak daha zor olmayabilir. Araştırmacılar, 3 milyon eğitim görüntüsünden sadece 300'ünü değiştirerek bir görüntü sınıflandırıcıya trojan yerleştirmeyi başardılar. Araştırmacılar, güvenlik riski oluşturmanın yanı sıra, trojanların daha iyi izleme araçlarının test edilmesi ve geliştirilmesi için somut bir ortam sağladığını ileri sürmektedir.

Sistemik güvenlik ve sosyoteknik faktörler
Yapay zekâ risklerinin (ve daha genel olarak teknolojik risklerin) yanlış kullanım veya kazalar şeklinde sınıflandırılması yaygındır. Bazı akademisyenler bu bakış açısının yetersiz kaldığını öne sürmüşlerdir. Örneğin, Küba Füze Krizi açıkça bir hata ya da yanlış teknoloji kullanımı sonucu ortaya çıkmamıştır. Politika analistleri Zwetsloot ve Dafoe şöyle yazmıştır: ""Kötüye kullanım ve kaza bakış açıları, bir hasara yol açan nedensellik zincirinde yalnızca son adıma odaklanma eğilimi gösterir: Yani, teknolojiyi kötüye kullanan kişiye veya istenmeyen şekilde hareket eden bir sisteme... Ancak çoğu zaman, ilgili nedensellik zinciri çok daha uzun olmaktadır."" Riskler genellikle rekabet baskısı, zararların yayılması, hızlı gelişim, yüksek düzeyde belirsizlik ve yetersiz güvenlik kültürü gibi 'yapısal' veya 'sistematik' etkenlerden kaynaklanmaktadır. Güvenlik mühendisliğinin daha geniş bağlamında, 'kurumsal güvenlik kültürü' gibi yapısal faktörler popüler STAMP risk analizi çerçevesinde merkezi bir öneme sahiptir.
Yapısal bakış açısından esinlenen bazı araştırmacılar, sosyoteknik güvenlik faktörlerini iyileştirmek için makine öğrenimini kullanmanın önemini vurgulamaktadır; örneğin, siber savunma için makine öğrenimini kullanmak, kurumsal karar verme sürecini iyileştirmek ve işbirliğini kolaylaştırmak mümkündür.

Siber savunma
Bazı akademisyenler, yapay zekânın siber saldırganlar ve siber savunmacılar arasında zaten dengesiz olan oyunu daha da kötüleştireceğinden endişe duymaktadır. Bu durum 'ilk saldırı' güdülerini arttıracak olup daha agresif ve istikrarı bozucu saldırılara yol açabilecektir. Bu riski azaltmak için bazıları siber savunma konusuna daha fazla önem verilmesini önermektedir. Buna ek olarak, güçlü yapay zekâ modellerinin çalınıp kötüye kullanılmasını önlemek için yazılım güvenliği de çok önemlidir.

Kurumsal karar alma mekanizmasının iyileştirilmesi
Yapay zekânın ekonomik ve askeri alanlarda ilerlemesi, benzeri görülmemiş siyasi sorunlara yol açacaktır. Bazı akademisyenler, yapay zekâ yarışı dinamiklerini, az sayıda karar merciinin dikkatli kararlarının çoğu zaman istikrar ve felaket arasındaki farkı belirlediği soğuk savaş ile kıyaslamıştır. Yapay zekâ araştırmacıları, yapay zekâ teknolojilerinin karar verme sürecine yardımcı olmak için de kullanılabileceğini ileri sürmüşlerdir. Örneğin, araştırmacılar yapay zekâ tahmin ve danışmanlık sistemleri geliştirmeye başlamıştır.

İşbirliğini kolaylaştırmak
En büyük küresel tehditlerin çoğu (nükleer savaş, iklim değişikliği, vb.) işbirliği yapmanın zor olduğu konulardır. İyi bilinen tutsak ikilemi senaryosunda olduğu gibi, bazı dinamikler, kendi çıkarları doğrultusunda en iyi şekilde hareket etseler bile, tüm oyuncular için kötü sonuçlara yol açabilmektedir. Örneğin, hiç kimse müdahale etmezse sonuçları önemli olsa bile, hiçbir tekil aktörün iklim değişikliğini ele almak için ciddi girişimleri bulunmamaktadır.
Göze çarpan bir yapay zekâ işbirliği sorunu, 'dibe doğru yarıştan' sakınmaktır. Bu durumda ülkeler ya da şirketler daha yetenekli yapay zekâ sistemleri kurmak için yarışırken güvenliği ihmal edecek ve bu da ilgili herkese zarar veren feci bir kazayla sonuçlanacaktır. Bu gibi durumlara ilişkin endişeler, insanlar arasında ve potansiyel olarak yapay zekâ sistemleri arasında işbirliğini kolaylaştırmak için hem siyasi hem de teknik çabalara ilham vermiştir. Çoğu yapay zekâ araştırması, tekil etmenlerin birbirinden ayrı görevler yerine getirecek şekilde tasarlanması konusuna odaklanmaktadır. Akademisyenler, yapay zekâ sistemleri daha otonom hale geldikçe, etkileşim biçimlerini incelemenin ve şekillendirmenin önemli hale gelebileceğini belirtmişlerdir.

Büyük Dil Modellerinin Zorlukları
Son yıllarda, büyük dil modellerinin geliştirilmesi, yapay zekâ güvenliği alanında eşsiz kaygılar ortaya çıkarmıştır. Araştırmacılar Bender ile Gebru ve diğerleri, bu modellerin eğitimiyle ilgili çevresel ve mali maliyetlerin altını çizerek, Transformatör modelleri için olduğu gibi eğitim süreçlerinin enerji tüketimi ve karbon ayak izinin önemli olabileceğini vurgulamışlardır. Dahası, bu modeller genellikle devasa, işlenmemiş İnternet tabanlı veri kümelerine dayanmaktadır; bu da egemen ve önyargılı bakış açılarını kodlayarak yeterince temsil edilmeyen grupları daha da ötekileştirebilir. Büyük ölçekli eğitim verileri çok geniş olsa da çeşitliliği güvence altına almaz ve genellikle ayrıcalıklı demografik grupların dünya görüşlerini yansıtarak mevcut önyargıları ve basmakalıp düşünceleri sürdüren modellere neden olmaktadır. Bu durum, bu modellerin görünüşte tutarlı ve akıcı metinler üretme eğilimi ile daha da şiddetlenmektedir; bu da kullanıcıları, "" tesadüfi papağanlar"" olarak tanımlanan bir fenomen olan, anlam ve niyetin olmadığı yerlerde anlam ve niyet atfetme konusunda yanlış yönlendirebilir. Dolayısıyla bu modeller toplumsal önyargıları güçlendirme, yanlış bilgi yayma ve aşırılık yanlısı propaganda ya da deepfake üretme gibi kötü niyetli amaçlarla kullanılma riski taşımaktadır. Bu zorlukların üstesinden gelmek için araştırmacılar, veri seti oluşturma ve sistem geliştirme konusunda daha dikkatli bir planlama yapılmasını savunmakta ve eşitlikçi bir teknolojik ekosisteme olumlu katkıda bulunan araştırma projelerine duyulan ihtiyacı vurgulamaktadır.

Yönetişimde
Yapay zekâ yönetişimi, genel olarak yapay zekâ sistemlerinin kullanımına ve geliştirilmesine rehberlik edecek normlar, standartlar ve düzenlemeler oluşturmakla ilgilenmektedir.

Araştırma
Yapay zekâ güvenlik yönetişimi araştırmaları, yapay zekânın potansiyel etkilerine ilişkin temel araştırmalardan belirli uygulamalara kadar uzanmaktadır. Temelde araştırmacılar, yapay zekânın geniş uygulanabilirliği nedeniyle toplumun birçok yönünü dönüştürebileceğini savunmuş, onu elektrik ve buhar makinesiyle karşılaştırmışlardır. Bazı çalışmalar, bu etkilerden oluşabilecek belirli riskleri öngörmeye odaklanmıştır; örneğin, kitlesel işsizlik, silahlanma, dezenformasyon, gözetim, ve gücün yoğunlaşmasından kaynaklanan riskler bunlara dahildir. Diğer çalışmalar, hızla gelişen yapay zekâ endüstrisini izlemenin zorluğu, yapay zekâ modellerinin kullanılabilirliği, ve 'dibe doğru yarış' dinamikleri gibi altta yatan risk faktörlerini araştırmaktadır.Askell, Amanda; Brundage, Miles; Hadfield, Gillian (10 Temmuz 2019). ""The Role of Cooperation in Responsible AI Development"". arXiv:1907.04534 $2. </ref> DeepMind'da uzun vadeli yönetişim ve strateji başkanı olan Allan Dafoe, yarışın tehlikelerini ve potansiyel işbirliği ihtiyacını vurgulamıştır: "" Yapay zekâ güvenliği ve uyumu için, gelişmiş güçlü sistemler devreye sokulmadan önce yüksek derecede dikkatli olunması neredeyse gerekli ve yeterli bir koşul haline gelecektir; fakat aktörler, ilk girenlere büyük getiri sağlayacak ya da göreceli avantaj sağlayacak bir alanda rekabet ediyorlarsa, kendilerine idealin altında bir dikkat düzeyi tercih etmeleri yönünde baskı yapılacaktır.""  Bir araştırma akışı, yapay zekâ hesap verebilirliğini değerlendirmek, yapay zekâ tabanlı sistemlerin denetimlerine rehberlik etmek ve desteklemek için yaklaşımlar, çerçeveler ve metotlar geliştirmeye yoğunlaşmaktadır.

Yerel Yapay Zekâ Güvenlik Önlemlerini Küresel Çözümlere Ölçeklendirmek
Yapay zekâ güvenliği sorununu ele alırken, yerel ve küresel çözümler arasındaki ayrımı vurgulamak önemlidir. Yerel çözümler, bireysel yapay zekâ sistemlerine odaklanarak güvenli ve faydalı olmalarını sağlarken, küresel çözümler çeşitli yetki alanlarındaki tüm yapay zekâ sistemleri için güvenlik önlemleri uygulamaya çalışmaktadır. Bazı araştırmacılar  yerel güvenlik önlemlerinin küresel düzeye aktarmanın gerekliliğini savunarak bu küresel çözümler için ayrı bir sınıflandırma önermektedir. Bu yaklaşım, hiçbir kuruluşun yapay zekâ teknolojileriyle ilişkili riskleri etkili bir şekilde yönetemeyeceğini belirterek, yapay zekâ güvenliğinin uluslararası yönetişiminde işbirliğine dayalı çabaların önemini ortaya koymaktadır. Bu bakış açısı, dünya çapında gelişmiş yapay zekâ sistemlerinin ortaya çıkardığı karmaşık zorlukların üstesinden gelmeyi amaçlayan uluslararası politika oluşturma ve düzenleyici çerçevelerde devam eden çabalarla uyumludur.

Hükümet eylemleri
Bazı uzmanlar, düzenlemelerin yeniliği engelleyeceği ve ""cehalet içinde düzenleme yapmak için acele etmenin"" aptallık olacağı yönündeki endişelerini ifade ederek, yapay zekâyı düzenlemek için henüz çok erken olduğunu belirtmiştir. İş adamı Elon Musk, felaket risklerini azaltmak için önleyici eylem konusunda çağrıda bulmaktadır.
Resmi mevzuatın dışında, devlet kurumları etik ve güvenlik önerileri ortaya koymuştur. Mart 2021'de ABD Yapay Zekâ Ulusal Güvenlik Komisyonu, yapay zekâdaki gelişmelerin ""sistemlerin güvenlik, sağlamlık ve güvenilirlik dahil olmak üzere hedef ve değerlerle uyumlu olmasını sağlamayı"" gittikçe daha önemli hale getireceğini rapor etmiştir. Daha sonra, Ulusal Standartlar ve Teknoloji Enstitüsü, ""yıkıcı riskler mevcut olduğu zaman riskler yeterince yönetilene kadar geliştirme ve dağıtım güvenli bir şekilde durdurulmalıdır"" tavsiyesinde bulunan Yapay Zekâ Riskini yönetmeye yönelik bir taslak hazırlamıştır.
Eylül 2021'de Çin Halk Cumhuriyeti, yapay zekâ kararlarının insan kontrolü altında kalması gerektiğini vurgulayarak ve hesap verebilirlik mekanizmaları çağrısında bulunarak Çin'de yapay zekâ kullanımına yönelik etik yönergeleri yayınladı. Aynı ay, Birleşik Krallık 10 yıllık Ulusal Yapay Zekâ Stratejisini  yayınladı; bu stratejide İngiliz hükûmetinin ""bağlantısız Yapay Genel Zekânın uzun vadede oluşturduğu riski ve bunun anlamına geleceği öngörülemeyen değişiklikleri"" belirtiyor. Strateji, yıkıcı riskler de dahil olmak üzere uzun vadeli yapay zekâ risklerini değerlendirmeye yönelik eylemleri açıklamaktadır. İngiliz hükûmeti yapay zekâ güvenliği konusunda ilk büyük küresel zirveyi düzenledi. Bu zirve 1 ve 2 Kasım 2023 tarihlerinde gerçekleşti ve ""politika yapıcıların ve dünya liderlerinin yapay zekânın mevcut ve gelecekteki risklerini ve bu risklerin küresel olarak koordine edilmiş bir yaklaşımla nasıl azaltılabileceğini değerlendirmeleri için bir fırsat"" şeklinde nitelendirildi.
Özellikle Amerika Birleşik Devletleri'ndeki hükûmet kuruluşları da teknik yapay zekâ güvenlik araştırmalarının geliştirilmesini desteklemiştir. İstihbarat İleri Araştırma Projeleri Faaliyeti, yapay zekâ sistemlerine yönelik Trojan saldırılarını tespit etmek ve bunlara karşı koruma sağlamak amacıyla TrojAI projesini başlatmıştır. DARPA, açıklanabilir yapay zekâ ve düşman saldırılarına karşı sağlamlığın artırılması üzerine araştırmalar yapmaktadır. Ulusal Bilim Vakfı da Güvenilir Makine Öğrenimi Merkezi'ni destekliyor ve deneysel yapay zekâ güvenlik araştırmaları için milyonlarca dolar fon ayırıyor.

Kurumsal regülasyonlar
Yapay zekâ laboratuvarları ve şirketleri genellikle resmi mevzuatın dışında kalan güvenlik uygulamalarına veya kurallarına uymaktadır. Yönetişim araştırmacılarının bir amacı da bu standartları şekillendirmektir. Literatürde bulunan güvenlik tavsiyelerine örnek olarak üçüncü taraf denetimi, hataların bulunması için ödüller sunulması, yapay zekâ olaylarının paylaşılması   yönergelerin takip edilmesi. araştırma veya modellerin yayınlanıp yayınlanmayacağına karar vermek, ve yapay zekâ laboratuvarlarında bilgi ve siber güvenliği iyileştirmek  yer almaktadır.
Şirketler de bazı taahhütlerde bulundu. Cohere, OpenAI ve AI21, kötüye kullanımı azaltan ""dil modellerinin dağıtımına yönelik en iyi uygulamalar"" önerisinde bulundu ve üzerinde anlaşmaya vardı. OpenAI, yarış dinamiklerine katkıda bulunmaktan kaçınmak için tüzüğünde ""eğer değerlerle uyumlu, güvenlik bilincine sahip bir proje bizden önce Yapay Genel Zekâ'yı inşa etmeye yaklaşırsa, bu projeyle rekabet etmeyi bırakıp ona yardımcı olmaya başlayacağımızı beyan ediyoruz"" ifadesine yer vermiştir. Ayrıca, DeepMind CEO'su Demis Hassabis, Facebook Yapay Zekâ Direktörü Yann LeCun gibi sektör liderleri Asilomar İlkeleri  ve Otonom Silahlar Açık Mektubu gibi açık mektuplara imza atmışlardır.


== Kaynakça =="
Bilgisayarlı görü,"Bilgisayarlı görü, bilgisayarların dijital görüntülerden veya videolardan nasıl bir anlam kazanabileceğiyle ilgilenen disiplinler arası bilimsel bir alandır. Mühendislik yöntemleriyle, insan görsel sisteminin yapabileceği görevleri anlamaya ve otomatikleştirmeye çalışmaktadır.
Bilgisayarlı görü görevleri, sayısal veya sembolik bilgi üretmek için dijital görüntüleri elde etme, işleme, analiz etme ve anlamayı içermektedir. Aynı zamanda gerçek dünyadan yüksek boyutlu verilerin çıkarılmasına yönelik yöntemleri içermektedir. Buna göre anlamak, görsel imgelerin (retinanın girdisi) düşünce süreçlerin anlamlandıran ve uygun eylemi ortaya çıkarabilen dünya tanımlarına dönüşümü anlamına gelmektedir.
Bilgisayarlı görünün bilimsel disiplini, görüntülerden bilgi çıkaran yapay sistemlerin arkasındaki teori ile ilgilidir. Görüntü verileri; video parçaları, birden çok kameradan gelen görüntüler, bir 3B tarayıcıdan çok boyutlu veriler veya tıbbi tarama cihazından gelen görüntüler gibi birçok biçimde olabilmektedir. Bilgisayarlı görü vizyonunun teknolojik disiplini, teorilerini ve modellerini bilgisayarla görme sistemlerinin yapımına uygulamayı amaçlamaktadır.
Bilgisayarlı görünün alt alanları nesne algılama, olay algılama, video izleme, nesne tanıma, 3D poz tahmini, öğrenme, indeksleme, hareket tahmini, otomatik görsel oluşturma, 3D sahne modelleme ve görüntü onarımı gibi alanlardır.

Tanım
Bilgisayarlı görü, bilgisayarların dijital görüntülerden veya videolardan nasıl bir anlam kazanabileceğiyle ilgilenen disiplinler arası bilimsel bir alandır. Mühendislik yöntemleriyle, insan görsel sisteminin yapabileceği görevleri anlamaya ve otomatikleştirmeye çalışmaktadır. ""Bilgisayarlı görü, tek bir görüntüden veya bir dizi görüntüden yararlı bilgilerin otomatik olarak çıkarılması, analizi ve anlaşılmasıyla ilgilidir. Otomatik görsel anlayışa ulaşmak için teorik ve algoritmadan türetilen bir temelin geliştirilmesini içermektedir"". Bilimsel bir disiplin olarak bilgisayarlı görü, görüntülerden bilgi çıkaran yapay sistemlerin arkasındaki teori ile ilgilenir. Görüntü verileri, video parçaları, birden çok kameradan gelen görüntüler veya bir tıbbi tarayıcıdan gelen çok boyutlu veriler gibi birçok biçimde olabilmektedir. Teknolojik bir disiplin olarak bilgisayarlı görü, teorilerini ve modellerini bilgisayarla görme sistemlerinin inşası için uygulamaya çalışmaktadır.

Tarihçe
1960'ların sonlarında, yapay zekaya öncülük eden üniversitelerden bilgisayarlı görü ile ilgili çalışmalar başlatıldı. Robotlara akıllı davranışlar kazandırmak için bir basamak olan insan görsel sistemini taklit ettirmek amaçlanıyordu. 1966'da bunun, bir bilgisayara bir kamera bağlatılarak ve ""gördüklerini tarif etmesini"" sağlatılarak bir ufak bir proje ile başarılabileceğine inanılıyordu.
Bilgisayarlı görüyü o dönemde yaygın olan dijital görüntü işleme alanından ayıran şey, tarama yapılan alanı tam olarak anlamak amacıyla görüntülerden üç boyutlu yapı çıkarma arzusuydu. 1970'lerde yapılan araştırmalar, görüntülerden kenarların çıkarılması, çizgilerin etiketlenmesi, çok yüzlü olmayan ve çok yüzlü modelleme, nesnelerin daha küçük yapıların ara bağlantıları olarak gösterilmesi, optik akış ve hareket tahmini gibi bugün var olan bilgisayarlı görü algoritmalarının birçoğunun ilk temellerini oluşturdu. Daha sonraki on yıl, bilgisayar vizyonunun daha titiz matematiksel analizine ve nicel yönlerine dayanan çalışmalar görüldü. Bunlar arasında ölçek alanı kavramı, gölgeleme, doku ve odak gibi çeşitli ipuçlarından şekil çıkarımı ve yılanlar olarak bilinen kontur (Resimde nesneyi belirgin gösteren çevre çizgisi.) modelleri yer aldı. Araştırmacılar ayrıca, bu matematiksel kavramların çoğunun, düzenlileştirme ve Markov rastgele alanları ile aynı optimizasyon çerçevesi içinde ele alınabileceğini fark etmişlerdir. 1990'larda, önceki araştırma konularından bazıları diğerlerinden daha aktif hale geldi. Projektif 3-D onarım araştırmaları, kamera kalibrasyonunun daha iyi anlaşılmasına yol açmıştır. Kamera kalibrasyonu için optimizasyon yöntemlerinin ortaya çıkmasıyla birlikte, fotogrametri alanından demet ayarlama teorisinde birçok fikrin keşfedildiği fark edilmiştir. Bu, birden çok görüntüden sahnelerin seyrek 3 boyutlu onarımı için farklı yöntemlere yol açmıştır. Yoğun üç boyutlu uygunluk problemi ve daha fazla çoklu-görüntülü üç boyutlu tekniklerinde ilerleme kaydedildi. Aynı zamanda, görüntü ayırmayı çözmek için grafik kesiminin varyasyonları kullanıldı. Bu on yıl içinde, görsellerdeki yüzleri tanımak için istatistiksel öğrenme tekniklerinin ilk kez pratikte kullanılmıştır (Örneğin Eigenface). 1990'ların sonlarına doğru, bilgisayar grafikleri ve bilgisayar görüşü alanları arasındaki etkileşimin artmasıyla önemli bir değişiklik meydana gelmiştir. Bu, görüntü tabanlı oluşturma, görüntü dönüştürme, görünüm enterpolasyonu, panoramik görüntü birleştirme ve erken ışık alanı oluşturmayı içeriyordu.
Son zamanlarda yapılan çalışmalar, makine öğrenimi teknikleri ve karmaşık optimizasyon çerçeveleri ile birlikte kullanılan özellik tabanlı yöntemlerin yeniden canlandığını gördü. Derin Öğrenme tekniklerinin ilerlemesi, bilgisayarlı görü alanına daha fazla yenilik getirdi. Sınıflandırma, segmentasyon ve optik akış gibi çeşitli görevler için çeşitli karşılaştırmalı bilgisayarlı görü veri setlerinde derin öğrenme algoritmalarının doğruluğu önceki yöntemleri geride bırakmıştır.

İlgili alanlar
Katı hal fiziği
Katı hal fiziği, bilgisayarlı görü ile yakından ilgili olan başka bir alandır. Çoğu bilgisayarlı görü sistemi, tipik olarak görünür veya kızılötesi ışık şeklinde olan elektromanyetik radyasyonu algılayan görüntü sensörleri ile çalışmaktadır. Sensörler, kuantum fiziği kullanılarak tasarlanmıştır. Işığın yüzeylerle etkileşime girdiği süreç fizik kurallar kullanılarak açıklanmaktadır. Fizik, çoğu görüntüleme sisteminin temel bir parçası olan optiğin davranışını açıklar. Gelişmiş görüntü sensörleri, görüntü oluşum sürecini tam olarak anlamak için kuantum mekaniğine bile ihtiyaç duymaktadır. Ayrıca, fizikteki çeşitli ölçüm problemleri, örneğin sıvılarda hareket gibi bilgisayarlı görü kullanılarak ele alınabilmektedir.

Sinir bilimi
Önemli bir rol oynayan ikinci bir alan, sinir bilimi, özellikle biyolojik görü sistemi çalışmasıdır. Geçen yüzyılda, hem insanlarda hem de çeşitli hayvanlarda görsel uyaranların işlenmesi için çalışan gözler, nöronlar ve beyin yapıları üzerinde kapsamlı bir çalışma yapılmıştır. Bu, görme ile ilgili belirli görevleri çözmek için ""gerçek"" görüntü sistemlerinin nasıl çalıştığına dair kaba, ancak karmaşık bir açıklamaya yol açmıştır. Bu sonuçlar, yapay sistemlerin, biyolojik sistemlerden farklı karmaşıklık düzeylerinde işlenmesine yol açmıştır. Aynı zamanda davranışları taklit edecek şekilde tasarlandığı bilgisayarlı görü içinde bir alt alana yol açmıştır. Ayrıca, bilgisayarlı görü içinde geliştirilen öğrenmeye dayalı yöntemlerden bazıları (örneğin; sinir ağı, derin öğrenme tabanlı görüntü ve özellik analizi ve sınıflandırma) biyoloji geçmişine sahiptir.
Bilgisayarlı görü araştırmalarının bazı türleri, biyolojik vizyon araştırmalarıyla yakından ilgilidir. Aslında, yapay zeka araştırmalarının birçok türünün insan bilincine yönelik araştırmalarla ve görsel bilgileri yorumlamak, entegre etmek ve kullanmak için depolanan bilginin kullanımıyla yakından bağlantılı olması gibi örnekler verilebilmektedir. Biyolojik görme alanı, insanlarda ve diğer hayvanlarda görsel algının arkasındaki fizyolojik süreçleri inceler ve modeller. Bilgisayarlı görü ise yapay görme sistemlerinin arkasında yazılım ve donanımda uygulanan süreçleri inceler ve açıklar. Biyolojik ve bilgisayarlı görü arasındaki disiplinler arası alışveriş her iki alan için de verimli olmaktadır.

Sinyal işleme
Bilgisayarlı görü ilgili bir başka alan da sinyal işlemedir. Tek değişkenli sinyallerin, tipik olarak zamansal sinyallerin işlenmesine yönelik birçok yöntem, bilgisayarlı görüde iki değişkenli sinyallerin veya çok değişkenli sinyallerin işlenmesine doğal bir şekilde genişletilebilmektedir. Bununla birlikte, görüntülerin özel doğası nedeniyle, tek değişkenli sinyallerin işlenmesinde karşılığı olmayan, bilgisayarlı görü içerisinde geliştirilen birçok yöntem vardır. Sinyalin çok boyutluluğuyla birlikte, sinyal işlemede bilgisayar görüşünün bir parçası olarak bir alt alanı tanımlanmaktadır.

Robotik navigasyon
Robot navigasyonu, robotik sistemlerin bir ortamda gezinmesi için otonom yol planlaması veya düşüncesiyle ilgilenmektedir. İçlerinde gezinmek için bu ortamların ayrıntılı olarak anlaşılması gerekmektedir. Çevre hakkındaki bilgiler, bir görüntü sensörü görevi gören ve çevre ile robot hakkında üst düzey bilgi sağlayan bir bilgisayar görüntü sistemi tarafından sağlanabilir.

Diğer alanlar
Bilgisayarlı görü ile ilgili yukarıda bahsedilen görüşlerin yanı sıra, ilgili araştırma konularının çoğu tamamen matematiksel bir bakış açısıyla da incelenebilmektedir. Örneğin, bilgisayarlı görüdeki birçok yöntem istatistiklere, optimizasyona veya geometriye dayanmaktadır. Son olarak, alanın önemli bir kısmı bilgisayar vizyonunun uygulama yönüne ayrılmıştır; çeşitli yazılım ve donanım kombinasyonlarında mevcut yöntemlerin nasıl gerçekleştirilebileceği veya çok fazla performans kaybetmeden işlem hızı kazanmak için bu yöntemlerin nasıl değiştirilebileceği gibi alanlarla da ilgilenmektedir. Bilgisayarlı görü ayrıca moda e-ticaretinde, envanter yönetiminde, patent araştırmasında, mobilyada ve güzellik endüstrisinde de kullanılmaktadır.

Ayrımlar
Bilgisayarlı görü ile en yakından ilgili alanlar görüntü işleme, görüntü analizi ve makine görüsüdür. Bunların kapsadığı çeşitli teknikler ve uygulamalar arasında önemli bir benzerlik vardır. Bunun anlamı, bu alanlarda kullanılan ve geliştirilen temel tekniklerin benzer olduğunu, farklı isimlere sahip tek bir alan olduğu şeklinde yorumlanabilecek olduğunu ima etmektedir. Öte yandan, araştırma gruplarının, bilimsel dergilerin, konferansların ve şirketlerin kendilerini özellikle bu alanlardan birine ait olarak sunmaları veya pazarlamaları gerekli görünmektedir. Bu nedenle, her alanı diğerlerinden ayıran çeşitli nitelendirmeler olmuştur. Bilgisayar grafikleri, 3D modellerden görüntü verileri üretmektedir, bilgisayarlı görü genellikle görüntü verilerinden 3D modeller üretmektedir. Örneğin artırılmış gerçeklikte keşfedildiği gibi, iki disiplinin bir kombinasyonuna doğru bir eğilim vardır.
Aşağıdaki nitelendirmeler konuyla ilgili görünmektedir ancak evrensel olarak kabul edildiği gibi alınmamalıdır:

Görüntü işleme ve görüntü analizi genellikle 2D görüntülere, bir görüntünün diğerine nasıl dönüştürüleceğine, örneğin kontrast geliştirme gibi piksel bazlı işlemler, kenar çıkarma veya gürültü giderme gibi yerel işlemler veya görüntüyü döndürme gibi geometrik dönüşümlere odaklanma eğilimindedir. Bu tanımlandırma, görüntü işlemenin veya analizinin, ne varsayımlar gerektirdiğini ne de görüntü içeriği hakkında yorumlar üretmediğini ima etmektedir.
Bilgisayarlı görü, 2D görüntülerden 3D görüntü analizini içermektedir. Bu, bir veya birkaç görüntü üzerine yansıtılan 3D sahneyi analiz eder, örneğin; 3D sahne hakkındaki yapının veya diğer bilgilerin bir veya birkaç görüntüden nasıl yeniden yapılandırılacağı ifade etmektedir. Bilgisayarlı görü genellikle bir görüntüde tasvir edilen sahne hakkında az çok karmaşık varsayımlara dayanmaktadır.
Makine görüsü, endüstriyel uygulamalarda görüntüleme tabanlı otomatik inceleme, süreç kontrolü ve robot rehberliği sağlamak için bir dizi teknolojiyi ve yöntemi uygulama sürecidir. Makine görüsü, temel olarak üretimde, örneğin vizyon tabanlı robotlar ve görsel tabanlı inceleme, ölçüm veya toplama (çöp toplama gibi) sistemleri gibi uygulamalara odaklanma eğilimindedir. Bu, görüntü sensörü teknolojilerinin ve kontrol teorisinin genellikle bir robotu kontrol etmek için görüntü verilerinin işlenmesi ile bütünleştirildiği sonucu çıkarılmaktadır. Ayrıca gerçek zamanlı işlemenin donanım ve yazılımdaki verimli uygulamalarla vurgulandığı anlamına gelmektedir. Aynı zamanda, aydınlatma gibi dış koşulların, makine görüşünde genel bilgisayarlı görüde olduğundan daha fazla kontrol edilebileceğini ve genellikle daha kontrollü olduğunu ifade etmektedir. Bu da farklı algoritmaların kullanılmasını sağlayabileceğini ima etmektedir.
Öncelikle görüntü üretme sürecine odaklanan, ancak bazen görüntülerin işlenmesi ve analizi ile ilgilenen görüntüleme adı verilen bir alan da vardır. Örneğin, tıbbi görüntüleme, tıbbi uygulamalardaki görüntü verilerinin analizine yönelik önemli çalışmaları içermektedir.
Son olarak, örüntü tanıma, temel olarak istatistiksel yaklaşımlara ve yapay sinir ağlarına dayanan, genel olarak sinyallerden bilgi çıkarmak için çeşitli yöntemler kullanan bir alandır. Bu alanın önemli bir kısmı, bu yöntemlerin görüntü verilerine uygulanmasına ayrılmıştır.

Uygulama alanları
Uygulamalar, üretim hattında hızla ilerleyen şişeleri inceleyen endüstriyel makine görme sistemleri gibi görevlerden, yapay zeka ve çevrelerindeki dünyayı kavrayabilen bilgisayarlar veya robotlar üzerine araştırmalara kadar uzanmaktadır. Bilgisayarlı görü ve makine görüsü alanları önemli ölçüde örtüşmektedir. Bilgisayarlı görü, birçok alanda kullanılan otomatik görüntü analizinin temel teknolojisini kapsamaktadır. Makine görüsü genellikle, endüstriyel uygulamalarda otomatik inceleme ve robot rehberliği sağlamak için otomatik görüntü analizini diğer yöntem ve teknolojilerle birleştirme sürecini ifade etmektedir. Pek çok bilgisayarlı görü uygulamasında, bilgisayarlar belirli bir görevi çözmek için önceden programlanmıştır, ancak öğrenmeye dayalı yöntemler artık giderek yaygınlaşmaktadır. Bilgisayarlı görü uygulamalarının örnekleri aşağıdakilere yönelik sistemleri içermektedir:

Otomatik inceleme, örneğin imalat uygulamalarında;
İnsanları tanımlama görevlerinde yardımcı olmak, örneğin bir tür tanımlama sistemi;
Kontrol süreçleri, örneğin bir endüstriyel robot;
Olayları tespit etme, örneğin görsel izleme veya insan sayımı;
Etkileşim, örneğin bilgisayar-insan etkileşimi için bir cihaza girdi olarak;
Nesneleri veya ortamları modelleme, örneğin, tıbbi görüntü analizi veya topografik modelleme;
Navigasyon, örneğin, otonom bir araç veya mobil robot yön bulma;
Bilgilerin düzenlenmesi, örneğin görüntü veritabanılarının ve görüntü dizilerinin indekslenmesi.

Sağlık
En önemli uygulama alanlarından biri, bir hastayı teşhis etmek için görüntü verilerinden bilgilerin çıkarılmasıyla bir sonuç elde eden tıbbi bilgisayarlı görü veya tıbbi görüntü işlemedir. Bunun bir örneği, tümörlerin veya diğer anormal değişikliklerin saptanmasıdır; organ boyutları, kan akışı vb. ölçümleri başka bir örnektir. Aynı zamanda yeni bilgiler sağlayarak tıbbi araştırmaları desteklemektedir: örneğin beynin yapısı veya tıbbi tedavilerin kalitesi hakkında. Tıbbi alandaki bilgisayarlı görü uygulamaları, örneğin gürültünün etkisini azaltmak için insanlar tarafından yorumlanan ultrasonik görüntüler veya X-ışını görüntüleri tarafından yorumlanan görüntülerin geliştirilmesini de içermektedir.

Makine görüsü
Bilgisayarlı görüdeki ikinci bir uygulama alanı, makine görüsü olarak adlandırılan ve bir üretim sürecini desteklemek amacıyla bilginin çıkarıldığı endüstridir. Bir örnek vermek gerekir ise, kusurları bulmak için ayrıntıların veya nihai ürünlerin otomatik olarak incelendiği kalite kontrolüdür. Bir başka örnek, bir robot kol tarafından alınacak detayların konumunun ve yönünün ölçülmesidir. Optik ayırma adı verilen bir işlem olan, istenmeyen gıda maddelerini dökme malzemeden çıkarmak için tarımsal süreçte yoğun bir şekilde makine görüsü kullanılmaktadır.

Askerî
Askerî uygulamalar muhtemelen bilgisayarlı görünün en geniş alanlarından biridir. En belirgin örnekler, düşman askerlerinin veya araçlarının tespiti ve füze rehberliğidir. Füze güdümüne yönelik daha gelişmiş sistemler, füzeyi belirli bir hedef yerine bir bölgeye göndermekte ve yerel olarak elde edilen görüntü verilerine göre füze alana ulaştığında hedef seçimi yapılmaktadır. ""Savaş alanı farkındalığı"" gibi modern askeri kavramlar, görüntü sensörleri de dahil olmak üzere çeşitli sensörlerin, stratejik kararları desteklemek için kullanılabilecek bir savaş sahnesi hakkında zengin bir bilgi kümesi sağladığını ifade etmektedir. Bu durumda, verilerin otomatik olarak işlenmesi, karmaşıklığı azaltmak ve güvenilirliği artırmak için birden çok sensörden gelen bilgileri birleştirmek için kullanılır.

Otonom araçlar
Yeni uygulama alanlarından biri, su altı araçları, kara tabanlı araçları (tekerlekli, arabalı veya kamyonlu küçük robotlar), hava araçları ve insansız hava araçlarını (İHA) içeren otonom araçlardır. Özgürlük seviyesi, tamamen otonom (insansız) araçlardan, bilgisayar destekli sistemlerin çeşitli durumlarda bir sürücüyü veya pilotu desteklediği araçlara kadar uzanmaktadır. Tamamen otonom araçlar genellikle navigasyon için bilgisayarlı görüyü kullanır, örneğin; nerede olduğunu bilmek veya çevresinin bir haritasını oluşturmak ve engelleri tespit etmek için kullanılmaktadır. Ayrıca, orman yangınlarını arayan bir İHA gibi belirli göreve özgü olayları tespit etmek için de kullanılabilmektedir. Destekleyici sistemlere örnek olarak, arabalardaki engel uyarı sistemleri ve uçakların otonom inişi için sistemler verilebilmektedir. Birkaç otomobil üreticisi otomobillerin otonom sürüşü için sistemler gösterdiler, ancak bu teknoloji hala piyasaya sürülebilecek bir seviyeye ulaşılamamıştır. Gelişmiş füzelerden keşif görevleri veya füze rehberliği için İHA'lara kadar geniş askeri otonom araç örnekleri vardır. NASA'nın Curiosity ve CNSA'nın (China National Space Administration, Çin Ulusal Uzay İdaresi) Yutu-2 gezgini gibi bilgisayarlı görü kullanan otonom araçlarla uzay araştırmaları yapılmaktadır.

Özgün görevleri
Uygulama alanlarının her biri bir dizi bilgisayarlı görü görevi kullanır; çeşitli yöntemler kullanılarak çözülebilen az çok iyi tanımlanmış ölçüm problemleri veya işleme problemleri. Özgün bilgisayarlı görü görevlerinin bazı örnekleri aşağıda sunulmuştur.
Bilgisayarlı görü görevleri, sayısal veya sembolik bilgiler, örneğin karar formlarında üretmek için dijital görüntüleri elde etme, işleme, analiz etme ve anlama ve gerçek dünyadan yüksek boyutlu verilerin çıkarılmasına yönelik yöntemleri içermektedir. Bu bağlamda anlamak, görsel imgelerin (retinanın girdisi) diğer düşünce süreçleriyle arayüz oluşturabilen ve uygun eylemi ortaya çıkarabilen dünyanın tanımlarına dönüştürülmesi anlamına gelmektedir. Bu görüntü anlayışı, geometri, fizik, istatistik ve öğrenme teorisi yardımıyla oluşturulan modeller kullanılarak görüntü verilerinden sembolik bilgilerin çözülmesi olarak görülebilmektedir.

Tanıma
Bilgisayarlı görü, görüntü işleme ve makine görüsündeki başlıca sorunlardan biri görüntü verilerinin belirli bir nesne, özellik veya etkinlik içerip içermediğini belirlemektir. Literatürde tanıma sorunu farklı şekillerde ele alınmıştır.

Nesne tanıma (nesne sınıflandırması da denir): Önceden belirlenmiş veya öğrenilmiş bir veya birkaç nesne veya nesne sınıfı, genellikle görüntüdeki 2D konumlarıyla veya sahnedeki 3D pozisyonlarıyla birlikte tanınabilmektedir. Blippar, Google Goggles ve LikeThat, bu işlevi gösteren bağımsız programlar sağlamaktadır.
Kimlik: Bir nesnenin tek bir örneği tanınır. Örnekler arasında belirli bir kişinin yüzünün veya parmak izinin tanımlanması, el yazısı rakamların tanımlanması veya belirli bir aracın tanımlanması yer almaktadır.
Tespit etme: Görüntü verileri belirli bir koşul için taranır. Örnekler arasında, tıbbi görüntülerde olası anormal hücrelerin veya dokuların tespiti veya bir aracın otomatik yol geçiş ücreti sisteminde tespit edilmesi yer almaktadır. Nispeten basit ve hızlı hesaplamalara dayanan tespit etme, bazen doğru bir yorumlama üretmek için daha hesaplama gerektiren tekniklerle daha fazla analiz edilebilen ilginç görüntü verilerinin daha küçük bölgelerini bulmak için kullanılmaktadır.
Şu anda, bu tür görevler için en iyi algoritmalar evrişimli sinir ağlarına dayanmaktadır. Yeteneklerinin bir örneği, ImageNet Büyük Ölçekli Görsel Tanıma Yarışması'nda verilmiştir; Bu, yarışmada kullanılan milyonlarca görüntü ve 1000 nesne sınıfıyla nesne sınıflandırma ve tespitinde bir kriterdir. ImageNet testlerinde evrişimli sinir ağlarının performansı artık insanlarınkine yakındır. En iyi algoritmalar, bir çiçeğin gövdesindeki küçük bir karınca veya elinde bir tüyü tutan bir kişi gibi küçük veya ince nesnelerle hala uğraşmaktadır. Ayrıca, filtrelerle bozulmuş görüntülerle de sorun yaşamaktadır. Bilgisayarların aksine, bu tür görüntüler insanları nadiren rahatsız etmektedir. Bununla birlikte, insanlar başka konularda sorun yaşama eğilimindedir. Örneğin, nesneleri belirli bir köpek türü veya kuş türleri gibi ince taneli sınıflara ayırmada iyi değillerdir, oysa evrişimli sinir ağları bunu kolaylıkla halletmektedir.
Aşağıdakiler gibi, tanımaya dayalı birkaç özel görev mevcuttur:

İçeriğe dayalı görüntü alma: Belirli bir içeriğe sahip daha büyük bir görüntü kümesindeki tüm görüntüleri bulmaktadır. İçerik, örneğin bir hedef görüntüye göre benzerlik açısından veya metin girişi olarak verilen üst düzey arama kriterleri gibi farklı şekillerde belirtilebilmektedir.

Poz tahmini: Kameraya göre belirli bir nesnenin konumunu veya yönünü tahmin edebilmektedir.
Optik karakter tanıma: Genellikle metni düzenleme veya indekslemeye daha uygun bir formatta (örneğin ASCII) kodlamak amacıyla basılı veya el yazısı metnin görüntülerindeki karakterlerin tanımlanmasıdır.
2D kod okuma: Veri matrisi ve QR kodları gibi 2D kodların okumaktadır.
Yüz tanıma
Şekil Tanıma Teknolojisi: İnsanları nesnelerden ayıran teknolojidir.

Hareket analizi
Çeşitli görevleri vardır. Bir görüntü dizisinin, görüntüdeki veya 3D sahnedeki her noktada veya hatta görüntüleri üreten kameranın hızının bir tahminini üretmek için işlendiği hareket tahminiyle ilgilidir. Bu tür görevlerin örnekleri şunlardır:

Egomotion: Kamera tarafından üretilen bir görüntü dizisinden kameranın 3D katı hareketini (döndürme ve öteleme) belirlemedir.
Takip: Görüntü dizisindeki (genellikle) daha küçük bir dizi ilgi noktası veya nesnenin (örneğin araçlar, nesneler, insanlar veya diğer organizmalar) hareketlerini takip etmektir. Bu, yüksek çalışan makinelerin çoğu bu şekilde izlenebildiği için geniş endüstri uygulamalarına sahiptir.
Optik akış: Görüntüdeki her nokta için o noktanın görüntü düzlemine göre nasıl hareket ettiğini belirlemek için kullanılmaktadır. Bu hareket, hem ilgili 3D noktasının sahnede nasıl hareket ettiğinin hem de kameranın sahneye göre nasıl hareket ettiğinin bir sonucudur.

Sahne yapılandırılması
Bir sahnenin veya videonun bir veya (özgün olarak) daha fazla görüntüsü verildiğinde, sahne yeniden yapılandırması sahnenin 3D modelini hesaplamayı amaçlamaktadır. En basit durumda, model bir dizi 3D nokta olabilmektedir. Daha karmaşık yöntemler, eksiksiz bir 3D yüzey modeli üretebilmektedir. Hareket veya tarama gerektirmeyen 3D görüntülemenin ve ilgili işleme algoritmalarının ortaya çıkışı, bu alanda hızlı ilerlemeler sağlamaktadır. Izgara tabanlı 3D algılama, birden çok açıdan 3D görüntüler elde etmek için kullanılabilmektedir. Algoritmalar artık birden fazla 3D görüntüyü nokta bulutları ve 3D modeller halinde birleştirmek için kullanılabilmektedir.

Görüntü onarımı
Görüntü onarımının amacı, görüntülerden gürültünün (sensör gürültüsü, hareket bulanıklığı vb.) giderilmesidir. Gürültünün giderilmesi için mümkün olan en basit yaklaşım, düşük geçişli filtreler veya medyan filtreler gibi çeşitli filtre türleridir. Daha karmaşık yöntemler, onları gürültüden ayırmak için yerel görüntü yapılarının nasıl göründüğüne dair bir model varsaymaktadır. Önce görüntü verilerinin çizgiler veya kenarlar gibi yerel görüntü yapıları açısından analiz edilmesi ve ardından analiz aşamasından gelen yerel bilgilere dayalı olarak filtrelemenin kontrol edilmesiyle, daha basit yaklaşımlara kıyasla genellikle daha iyi bir gürültü giderme seviyesi elde edilmektedir.
Bu alandaki bir örnek de boyamadır (Fotoğraflarda veya videolarda renk değişimi yapılması).

Sistem yöntemleri
Bir bilgisayarlı görü sisteminin organizasyonu büyük ölçüde uygulamaya bağlıdır. Bazı sistemler, belirli bir ölçüm veya algılama problemini çözen bağımsız uygulamalardır. Bir bilgisayarla görme sisteminin özel uygulaması aynı zamanda işlevselliğinin önceden belirlenmiş olmasına veya çalışma sırasında bir kısmının öğrenilip değiştirilemeyeceğine de bağlıdır. Birçok işlev uygulamaya özgüdür. Bununla birlikte, birçok bilgisayarlı görü sisteminde bulunan tipik işlevler vardır.

Görüntü edinme: Bir dijital görüntü, çeşitli ışığa duyarlı kameraların yanı sıra mesafe sensörleri, tomografi cihazları, radar, ultrasonik kameralar ve benzeri araçları içeren bir veya birkaç görüntü sensörü tarafından üretilmektedir. Sensör tipine bağlı olarak, ortaya çıkan görüntü verileri sıradan bir 2D görüntü, 3D hacim veya bir görüntü dizisidir. Piksel değerleri tipik olarak bir veya birkaç spektral banttaki (gri görüntüler veya renkli görüntüler) ışık yoğunluğuna karşılık gelir, ancak derinlik, sonik veya elektromanyetik dalgaların soğurulması veya yansıması veya nükleer manyetik rezonans gibi çeşitli fiziksel ölçülerle de ilgili olabilmektedir.
Ön işleme: Belirli bir bilgi parçasını çıkarmak için görüntü verilerine bir bilgisayarlı görü yöntemi uygulanmadan önce, yöntemin belirlediği belirli varsayımları karşıladığından emin olmak için genellikle verileri işlemek gerekmektedir. Örnekler:
Görüntü koordinat sisteminin doğru olduğundan emin olmak için yeniden örnekleme.
Sensör gürültüsünün yanlış bilgi vermemesini sağlamak için gürültü azaltma.
İlgili bilgilerin tespit edilebilmesini sağlamak için karşıtlık geliştirme.
Görüntü yapılarını yerel olarak uygun ölçeklerde geliştirmek için alan gösterimini ölçeklendirme.
Özellik çıkarma: Çeşitli karmaşıklık düzeylerindeki görüntü özellikleri, görüntü verilerinden çıkarılır. Bu tür özelliklerin özgün örnekleri şunlardır:
Çizgiler, kenarlar ve sırtlar.
Köşeler, lekeler veya noktalar gibi yerelleştirilmiş ilgi noktaları.
Daha karmaşık özellikler doku, şekil veya hareketle ilgili olabilmektedir.

Algılama / bölümleme (segmentation): İşlemenin bir noktasında, görüntünün hangi görüntü noktalarının veya bölgelerinin daha sonraki işlemlerle ilgili olduğuna dair bir karar verilmektedir. Örnekler:
Belirli bir ilgi noktası kümesinin seçilmesi.
Belirli bir ilgi nesnesini içeren bir veya birden çok görüntü bölgesinin bölümlenmesi.
Görüntünün ön plan, nesne grupları, tek nesneler veya göze çarpan nesne parçalarını içeren iç içe geçmiş sahne mimarisine bölünmesi, görsel belirginlik ise genellikle uzamsal ve zamansal dikkat olarak uygulanması.
Üst düzey işleme: Bu adımda, girdi tipik olarak küçük bir veri kümesidir, örneğin belirli bir nesneyi içerdiği varsayılan bir dizi nokta veya bir görüntü bölgesidir. Örneğin aşağıdakilerle ilgilenir:
Verilerin model tabanlı ve uygulamaya özgü varsayımları karşıladığının doğrulanması.
Nesne duruşu veya nesne boyutu gibi uygulamaya özel parametrelerin tahmini.
Görüntü tanıma - tespit edilen bir nesneyi farklı kategorilere ayırmak.
Görüntü kaydı - aynı nesnenin iki farklı görünümünü karşılaştırmak ve birleştirmek.
Karar verme: Başvuru için gerekli olan nihai kararın verilmesidir. Örneğin:
Otomatik denetim uygulamalarında başarılı / başarısız.
Tanıma uygulamalarında eşleşme / eşleşme yok.

Görüntü anlama sistemleri
Görüntü anlama sistemleri (Image-understanding systems: IUS) aşağıdaki gibi üç soyutlama düzeyi içermektedir: düşük düzey; kenarlar, doku öğeleri veya bölgeler gibi görüntü temel öğelerini içermektedir; orta seviye; sınırları, yüzeyleri ve hacimleri içermektedir. Yüksek seviye; nesneleri, sahneleri veya olayları içermektedir. Bu gereksinimlerin çoğu, tamamen daha fazla araştırma yapılması gereken konulardır.
Bu seviyeler için IUS tasarımındaki temsil gereksinimleri şunlardır: Prototipik kavramların temsili, konsept organizasyonu, mekansal bilgi, zamansal bilgi, ölçekleme ve karşılaştırma ve farklılaştırma yoluyla açıklamadır. Çıkarım, şu anda bilinen gerçeklerden açıkça temsil edilmeyen yeni gerçekleri türetme sürecini ifade ederken; kontrol, işlemenin belirli bir aşamasında birçok çıkarım, arama ve eşleştirme tekniklerinden hangisinin uygulanması gerektiğini seçen süreci ifade etmektedir. IUS için çıkarım ve kontrol gereksinimleri şunlardır: Arama ve hipotez aktivasyonu, eşleştirme ve hipotez testi, beklentilerin oluşturulması ve kullanılması, dikkatin değişmesi ve odağı, inancın kesinliği ve gücü, çıkarım ve hedef tatminidir.

Kullanılan araçlar
Bilgisayarlı görü, birçok endüstri ve sektörde çok ihtiyaç duyulan devrimi getirdi. Bulut üzerinden hizmet olarak GPU, Makine Öğrenimi cihazları ve ML platformu gibi donanımlardaki gelişmeler, günümüzde bilgisayarla görmeyi daha etkileyici hale getirdi. Bu yazılım donanımlarından en çok kullanılan 6 tanesi ise:

OpenCV: Bu iyi bilinen kitaplıktır. Görüntü ve video işleme görevlerini yürütmek için temel stratejileri ve algoritmaları kapsayan çok platformlu bir yöntemdir. OpenCV işlevleri C++ ve Python dilleri ile çalışmaktadır.
Tensorflow: Google tarafından geliştirilmiştir. TensorFlow 2.0, resimler, konuşma tanıma, nesne algılama, güçlendirilmiş öğrenme ve öneriler için önceden ayarlanmış ve hazırlanmış modellerin yürütülmesini sağlamaktadır.
Matlab: Görüntü işleme uygulamaları yapmak için en iyi araçlardan bir tanesidir. Hızlı örneklemeye izin verdiği için araştırmalarda kullanılmaktadır. C++ dili ile karşılaştırıldığında çok sadedir ve sorun gidermeyi kolaylaştırmaktadır.
CUDA: NVIDIA'nın bu aracı, paralel hesaplamanın temeli olarak kullanılmaktadır. CUDA, inanılmaz performans sunmak için GPU'ların gücünü kullanmaktadır. Araç kutusu, bir dizi görüntü, sinyal ve video işleme işlevini içeren NVIDIA Performance Primitives kitaplığını içermektedir.
Theano: Bu Python tabanlı sayısal kitaplıktır. CPU veya GPU üzerinde çalışabilmektedir. Kanada'daki Montreal Üniversitesi'ndeki LISA grubu tarafından oluşturulmuştur. Araç, matematiksel ifadeleri kontrol etmek ve değerlendirmek için geliştirici bir derleyici olarak kullanılmaktadır.
Keras: Farklı kitaplıkların en iyisini araçlarını birleştiren başka bir Python tabanlı derin öğrenme kitaplığıdır. TensorFlow, Theano ve CNTK'nın gücünü birleştirerek popülerlik kazanmıştır. TensorFlow, Microsoft Cognitive Toolkit, PlaidML veya Theano üzerinde çalışabilmektedir. Keras genellikle derin sinir ağları ile hızlı sonuçlar çıkarmak için kullanılmaktadır

Donanım
Pek çok tür bilgisayarlı görü sistemi vardır; ancak hepsi şu temel öğeleri içermektedir: bir güç kaynağı, en az bir görüntü edinme cihazı (kamera, ccd, vb.), bir işlemci ve kontrol ve iletişim kabloları veya bir tür kablosuz ara bağlantı mekanizması. Ek olarak, pratik bir görsel denetim sistemi ve sistemi izlemek için yazılımın yanı sıra bir ekran içermektedir. İç mekanlar için görüntü sistemleri, çoğu endüstriyel sistemde olduğu gibi, bir aydınlatma sistemi içerir ve kontrollü bir ortama yerleştirilebilmektedir. Ayrıca, tamamlanmış bir sistem, kamera destekleri, kablolar ve konektörler gibi birçok aksesuarı içermektedir.
Çoğu bilgisayarlı görü sistemi, bir sahneyi saniyede en fazla 60 kare (genellikle çok daha yavaş) kare hızlarında pasif olarak görüntüleyen görünür ışık kameraları kullanılmaktadır. Birkaç bilgisayarlı görü sistemi, yapılandırılmış ışıklı 3D tarayıcılar, termografik kameralar, hiperspektral görüntüleyiciler, radar görüntüleme, lidar tarayıcılar, manyetik rezonans görüntüleri, yandan taramalı sonar gibi aktif aydınlatmalı veya görünür ışıktan başka bir şey veya her ikisine sahip görüntü toplama donanımını kullanmaktadır. Bu tür donanım, görünür ışıklı görüntüleri işlemek için kullanılan aynı bilgisayar görme algoritmaları kullanılarak daha sonra sıklıkla işlenen ""görüntüleri"" yakalamaktadır. Geleneksel yayın ve tüketici video sistemleri saniyede 30 kare hızında çalışırken, dijital sinyal işleme ve tüketici grafik donanımındaki gelişmeler, saniyede binlerce kare ile yüzlerce gerçek zamanlı sistemler için yüksek hızlı görüntü alma, işleme ve görüntülemeyi mümkün kılmıştır. Robotikteki uygulamalar için hızlı, gerçek zamanlı video sistemleri kritik öneme sahiptir ve genellikle belirli algoritmalar için gerekli olan işlemeyi basitleştirebilmiştir. Yüksek hızlı bir projektörle birleştirildiğinde, hızlı görüntü elde etme, 3D ölçümün ve özellik izlemenin gerçekleştirilmesine olanak tanımaktadır.
2016 itibarıyla, görüntü işleme birimleri, bu roldeki CPU'ları ve grafik işleme birimlerini (GPU) tamamlamak için yeni bir işlemci sınıfı olarak ortaya çıkarmaktadır.

Geleceği
Teknolojinin daha fazla araştırılması ve iyileştirilmesiyle, bilgisayarlı görünün geleceği için daha geniş bir işlev yelpazesi gerçekleştirdiğini gösterecektir. Yalnızca bilgisayarlı görü teknolojilerinin eğitilmesi daha kolay olmayacak, aynı zamanda görüntülerden şu anda olduğundan daha fazlasını ayırt edebilecektir. Bilgisayarlı görü, daha güçlü uygulamalar oluşturmak için diğer teknolojilerle veya diğer yapay zeka alt kümeleriyle birlikte kullanılabileceği anlamına gelmektedir. Örneğin, resim yazısı oluşturma uygulamaları, çevredeki nesneleri görme engelli kişiler için yorumlamak için doğal dil işleme ile birleştirilebilecektir. Aynı zamanda yapay genel zeka ve yapay süper zekanın geliştirilmesinde, onlara bilgiyi insan görsel sistemi kadar hatta daha iyi işleme yeteneği vererek hayati bir rol oynayacaktır. Günümüzün teknoloji yetenekleri düşünüldüğünde, keşfedilmemiş kalan teknolojinin daha fazla faydası ve uygulaması olduğuna inanmak zor olabilmektedir. Bilgisayarlı görünün geleceği, bizim kadar insan olan yapay zeka sistemlerinin önünü açacaktır. Bununla birlikte, üstesinden gelinmesi gereken birkaç zorluk var, bunların en büyüğü yapay zekanın kara kutusunun gizemini çözmektir. Bunun nedeni, tıpkı diğer derin öğrenme uygulamaları gibi, işlevsel olarak etkili olmasına rağmen, iç işleyişi söz konusu olduğunda bu teknolojinin çözülemez olmasıdır.

Ayrıca bakınız
Görsel gradyan


== Kaynakça =="
Robotik,"Robotik, robotların tasarımı, yapımı, işletimi ve kullanımına ilişkin disiplinler arası bir çalışma ve uygulamadır. Dördüncü Sanayi Devrimi'nin en yaygın özelliklerinden biridir. Makine mühendisliği, uçak mühendisliği, uzay mühendisliği, elektronik mühendisliği, bilgisayar mühendisliği, mekatronik ve kontrol mühendisliği dallarının ortak çalışma alanıdır. Robotlar bir yazılım aracılığıyla yönetilen ve yararlı bir amaç için iş ve değer üreten karmaşık makinelerdir.
Robotik, insanın yerinde geçebilecek ya da insanın eylemlerini taklit edebilecek makineler yapmayı hedefler. Robotların birçok farklı durumda kullanılması amaçlansa da, günümüzde daha çok tehlikeli ortamlarda (örn. bomba imhası), üretim süreçlerinde veya insanın yaşayamadığı uzay, sualtı, yüksek sıcaklık ve radyasyonlu ortamlarda kullanılmaktadır. Robotlar her biçimde yapılabileceği halde, bazı robotlar insana benzer olarak yapılmaktadır. Bunun, robotların insanlar tarafından kabulünü kolaylaştıracağı düşünülmektedir. Birçok robot doğadan esinlenerek yapılmıştır.
Türkiye'deki üniversitelerde açılan mekatronik bölümleri robotikle yakından ilişkilidir.

Tarihçe
Tarihte robotikle ilintili en erken atıf MÖ 3. yüzyılda Çin'de yazılmış bir Lie Zi yazmasında bulunmuştur. Bu yazma MÖ 1000 yıllarında yaşamış Zhou Kralı Mu'ya sunulan mekanik bir insan mankeninden bahseder. Antik Yunanda, MÖ 3. yüzyılda yaşamış Ktesibios çağını aşan pek çok çalışmalar yapmış, yüzden fazla mekanik otomata tasarlamıştır. Onun çalışmaları Bizantiyonlu Filon ve İskenderiyeli Heron tarafından devam ettirilmiştir.
Onlardan sonra bilinen en önemli robotik öncüsü El-Cezeri'dir. Çağının çok ilerisinde[kime göre?] mekanik otomatalar yapmıştır. Eski tarihlerde yaşamış olan meslektaşlarının icatlarını geliştirmiş ve kendine ait olan birçok tasarım yapmıştır. El Cezeri, Otomatik Makineler tarihinde Çağın Doruğuna Erişmiş Büyük Mühendis İbn-i Razzaz Cezeri adıyla anılır. Yazdığı kitabındaki tüm buluşlar insanımsı, estetik değerlere sahiptir ve hiçbiri hayal ürünü değildir.[kime göre?] Alman Profesörü Widemann, tarafından tekrar üretilip çalıştırılmışlardır. El Cezeri'nin kaleme aldığı orijinal ismi Kitab-ül Camii Beyn-el ilmi vel-amel En Nafi-i fi Sınaat-il hiyel kitabı, Kültür Bakanlığı 1990 yılında Olağanüstü Mekanik Araçların Bilgisi Hakkında Kitap adında basmıştır. Kitabın Türkçe çevirisi ise Sevim Tekeli tarafından hazırlanarak Türk Tarih Kurumu Yayınları tarafından basılmıştır. El-Cezeri'nin mezarı hâlen Cizre'de Nuh Peygamber Camii'nin avlusunda bulunuyor. Avrupalılar tarafından Al-Jasar olarak bilinmektedir.
İtalya Floransa'da yaşamış Rönesansın en büyük ressam ve heykeltıraşlarından kabul edilen Leonardo Da Vinci'ye ait 1495 yılında tasarlandığı sanılan savaşçı makine kayıt altına alınmış bir başka örnektir. Resimdeki model Leonardo'nun orijinal çizimlerinden yararlanılarak 1950 yılında yeniden yapılmıştır. Robot kollarını çenesini ve başını hareket ettirebilmektedir.

18. ve 19. Yüzyılda Avrupa'da Robotik
Bu yüzyıllarda daha çok eğlence amaçlı gerçekleştirilen Robot - Otomatlar zengin sarayların gözdesiydi.
Yanda: 1776 yılında Fransız mekanikçi Pierre Jaquet Droz tarafından yapılan org çalan müzisyen
Osmanlı Sarayı için geliştirilen otomatlardan biri de 1769 yılında [Baron Von Kempelen] tarafından yapılan satranç oynayan adamdı. Bu otomat Viyana ve Moskova fuarlarında sergilenmişti. Ancak daha sonraları bu otomatın içinde insan gizlendiği iddia edilmiştir. Bir Zemberekten güç alan metal silindir ve üzerindeki kamlar sayesinde olasılıkları hesaplayabilen karmaşık bir mekanizması vardı. O yıllarda Laterna mekaniğinin benzeri olan bu sistemler daha sonraları Thomas Alva Edison'a da ilham kaynağı olacak ve Edison üzerinde sabitlenmiş kamları bulunan silindirin yerine üzerine yazılabilir balmumu silindiri koyarak gramafonu icat edecekti. Bu örnek tarihte icatların öyle gökten düşmediğine ilişkin çarpıcı bir örnektir.
1785 yılında Pierre Kintzing tarafından yapılan Müzisyen. Dönemin değer yargılarına göre oldukça estetik bir görünümü olan ve bir tür vurmalı akustik çalgı olan Harpsicord çalan kadın döneminin androidi sayılabilirdi. Bu gün Fransada müzede bulunan bu örnek de kurulan bir zemberekten güç almaktaydı. Bu otomatlar gerçekten de birçok müzik parçasını çalabilen karmaşık makinelerdi. Avrupa'nın bilgi birikimi, o çağda doruğa çıkmış olan saat yapımcılığı ve mekanik ustalığından ileri gelmekteydi. Çok küçük parçalar yapmakta ustalaşmış saat yapımcıları ve mekanik ustaları için otomat yapımı sarayda ve soylu çevrelerde kendilerini gösterebilecekleri eşsiz fırsatlardı.
Charles Roberts adlı bir mekanik ustası tarafından geliştirilen bu örnekteki resim çizen otomatların tarihi bilinmemektedir. Ancak 19. yüzyılda yayınlanan bir kitapta resimleri yer almaktadır. Fransa ulusal müzesinde sergilenen otomatlar son derece karmaşık çizimleri ustalıkla yapmaktadır. Ayrıca şiirde yazabilen otomatlar zemberek - kam prensibiyle çalışmaktaydı.

Endüstriyel robotik
ISO 8373 Standardına göre belirlenmiş endüstriyel robot tanımı ve robot tiplerinin sınıflandırılması şöyledir:
""Endüstriyel uygulamalarda kullanılan, üç veya daha fazla programlanabilir ekseni olan, otomatik kontrollü, yeniden programlanabilir, çok amaçlı, uzayda sabitlenmiş veya hareketli manipülatördür.""

Robotların Sınıflandırılması
Günümüzde kullanılan robotlar çeşitli sınıflara ayrılabilirler. Bunlar kullanılan eksen
takımlarına göre, tiplerine göre, kullanılan tahrik elemanının çesidine göre vb. Bunlardan en önemli olan sınıflandırma yöntemleri aşağıda verilmiştir;

Koordinat Sistemlerine Göre Robotların Sınıflandırılması
Kartezyen koordinat sistemi,
Silindirik koordinat sistemi,
Küresel koordinat sistemi,
Döner koordinat sistemi.

Robot Tiplerine Göre Sınıflandırma
Kartezyen robotlar,
Mafsallı robotlar,
Scara robotlar.

Scara Robotlar
Scara, İngilizce: Selective Compliance Assembly Robotic Arm kelimelerinin baş harflerinden oluşmuştur. Yani seçimlere uyan (faaliyet yerine getirme) montaj robot koludur. Bu robot 1970'ten sonra Japon Endüstriyel Konsorsiyomu ve bir grup araştırmacı tarafından Japonya'da Yamanashi Üniversitesinde geliştirilmiştir.
Scara tipi robot, çok yüksek hızlara, en iyi tekrarlama kabiliyetine, yüksek hassasiyet ve doğruluk oranlarına sahip bir robot çeşididir.

Scara Tipi Robotun Özellikleri
Scara tipi bir robota ait şematik çizim verilmiştir. Scara robotun genel özellikleri şöyledir:

1. Doğruluk
2. Yüksek hız
3. Kolay montaj
4. Hassasiyet
5. Yüksek Verim

Scara Tipi Robotun Yapısı
Bu robot genellikle dikey eksen çevresinde dönen 2 veya 3 kol bölümünden meydana gelmiştir. Şekil 16'de görülen 1 numaralı eksen robota ana dönmeyi veren eksendir. Bu eksen en çok montaj robotlarında kullanılmaktadır. 2 numaralı eksen doğrusal dikey eksendir.
Bu eksende sadece dikey hareket yapılabilmektedir. Bu özellik montaj robotlarında istenildiğinden dolayı, montaj robotlarının büyük bir kısmı aşağıya doğru dikey hareket yapar. 
Dikey eksen hareketleri koordinat hareket eksenleri içinde aşağıya doğru yapılan en çabuk ve düzgün hareketlerdir. 3 numaralı eksende robot kolunun erişebileceği uzaklık değiştirilebilir. 4 numaralı eksende ise dönen kol bileği hareket eder. robotun çalışma alanına ait çizdiği hacim verilmiştir.

Scara Tipi Robotun Kullanım Alanları
Günümüzde Scara tipi robotlar yaygın olarak birçok alanda kullanılmaktadır. Elektronik devre elemanlarının baskılı devre üzerine yerleştirilmesinde, elektromekanik olarak çalışan küçük cihazların ve bilgisayar disk sürücülerinin montajında bu robotlardan faydalanılmaktadır.
Elektronik devre elemanlarının yerleştirilmesi sırasında robotun tutucu kolu kullanılır.
Bu kola alınan parça bakırlı plaket üzerinde önceden belirlenen yere yerleştirilir. Yerleştirme işlemi ve öncesi bilgisayar tarafından kontrol edildiği için hata meydana gelmeyecektir.
Robotların elektronik sanayiinde kullanılmasıyla birlikte seri üretim yapılmaya başlanmış ve kalite artmıştır.

Uygulamalar
Dizme, yerleştirme, taşıma, paketleme, silikon çekme, delme, kesme, yapıştırma, kalite kontrol, ölçüm, test işlemleri, yükleme ve boşaltma gibi birçok üretim sürecine kullanılmaktadır.
Otomotiv, beyaz eşya, kimya, cam, mobilya, gıda, elektronik, metal, seramik, kâğıt gibi birçok endüstriyel sektörde kullanıma uygundur.

Operasyonel robotik
İnsanın yaşamasına elverişli olmayan ortamlarda çalışırlar. Örnek: Radyasyon ortamı, su altı, uzay vb. sistemler programlanabilir ve kendi kendine çalışan bir olmaktan çok uzaktan kontrollüdür. Servo DC motor, hidrolik ve pnömatik sistemler tercih edilebilir. Yüksek teknoloji gerektirir. Özel amaçlara göre özel yaklaşımlar geliştirilir. Uzaktan yönetim için güç aktarım sistemleri (hidrolik veya pnömatik) veya radyo frekansı kullanılır.

Tıp ve sağlıkta robotik
Robotik protezler  (ortopedi)
Gelişmiş protezler piezo elektrik sensörlerle tendonlardaki gerilimleri (beyin komutlarını) algılayabiliyorlar ve parmaklara veya eksenlere gerilimin şiddetine göre güç gönderebiliyorlar. Güç aktarımı servo motorlar ve yapay tendon sistemleriyle yapılıyor. Bu protezler çok pahalıya malolduğundan çok yaygın olarak şimdilik kullanılamıyor. Mâliyeti düşürmek için son zamanlarda bellekli metâller üzerinde çalışılıyor.

Ameliyat Robotları (tıbbî operasyonlar)
Tamamen adımlı motorlar ve hassas kontrollerle yapılan sistemler, kıtalar arası iletişimle cerrahların ameliyatlara katılmasını sağlıyabilmektedir.

Da Vinci Ameliyat Robotu Yakın zamanda Einstein isimli yeni bir cerrahi robot doktorların ve hastaların hizmetine Medtronic tarafından sunulacaktır.

Sibernetik
Konstruktif mimari (dış görünüm ve beden)
Amaç sistemi canlı dokuya benzetmek olduğu için elektronik, malzeme bilimi, sibernetik ve tıp konunun içine girmiştir. Ayrıca konstruktif fizik, pnomatik, hidrolik ve makine gibi geleneksel mühendislik ve bilim kategorilerinide ilgilendirmektedir. Plastik döküm yöntemleri, Üç boyutlu yaratım yeteneği ve sanatsal görüş gibi soyut yeteneklerde gerektirmektedir. Bazı sibernetikçi bilim insanları plâstik ve metal yerine kalsiyum ve doğal dokuları kullanmak için araştırmalar yapmaktadırlar.

Zekâ ve Denetim Sistemi
Yapay zekâ araştırmaları, programcılık ve veritabanı sorgu dillerini bilmeyi ve yeni algoritmalar geliştirebilmeyi gerektiriyor. Araştırmalar, mevcut ikili bilgi sisteminin (0 ve 1 (Boole cebiri)) sınırlarını zorluyor. İnsan beyni kadar esnek ve yetenekli bir yapay zekâ, slikon teknolojisiyle mümkün görünmüyor. Bu yüzden bazı bilim insanları moleküler ve biyolojik bilgisayar sistemleri, üzerinde çalışıyorlar.

Antropomorfik Robotik (insan ve canlı benzeşimli robotlar) da sibernetiğin alt koludur.

Oyuncak robotlar
Elektronik ve mekanik sistemler içeren Robotik oyuncaklar çok karmaşık olabiliyor. Sibernetiğin teorik araştırmaları, ilk ticârî ürünlerini bu alanda veriyor. Furby, Sony'nin AIBO robot köpeği, ünlü robot araştırmacısı Mark Tilden'in Robosapien'i bu alandaki öncü ürünlerden bazıları.

Hobi Amaçlı Robotik
Robot hobisi Dünya'da çok sayıda kişinin uğraş alanıdır. Bu kategori herkesin değişik eğilimlerine göre şekillenebilmektedir. Örnegin Japonya'da her yıl hobi robotlarının yarıştırıldığı gösteriler düzenlenmektedir. Hobi tutkunlarının kurduğu birçok topluluk mevcuttur. Bu alana yönelik çok sayıda kitap ve yayın vardır. Ulusal ve uluslararası birçok yarışma düzenlenmektedir.

FeTeMM Eğitimi'nde Robotik Etkinlikleri
Robotik etkinlikleri özellikle FeTeMM Eğitimi'nde ogrenmenin bir parçası olarak kabul ediliyor. Bu etkinlikler okul sonrasi programlar veya daha genel bir ifade ile okul disi ogrenmenin kapsaminda degerlendirilebiliyor.

Robotik alt dalları
Ayrıca bakınız
Sina Robotik Sistemi
Microsoft Robotics Studio


== Kaynakça =="
Duygu analizi,"Duygu analizi ya da görüş madenciliği, duygusal durumları ve öznel bilgileri sistematik olarak tanımlamak, çıkarmak, ölçmek ve incelemek için doğal dil işleme, metin analizi, hesaplamalı dilbilim ve biyometrinin kullanılmasıdır. Duygu analizi, inceleme ve anket yanıtları gibi müşteri materyallerinde, online ve sosyal medyaya ve müşteri hizmetlerinden klinik tıba ve pazarlamaya kadar değişen uygulamalarda, sağlık materyallerinde uygulanır. RoBERTa gibi derin dil modellerinin yükselişiyle birlikte daha zor veri alanları da analiz edilebilir; yazarların genellikle fikirlerini/duygularını daha az açıkça ifade ettikleri haber metinleri buna örnek verilebilir.

Ayrıca bakınız
Bilişsel dilbilim
Yapay zeka


== Kaynakça =="
Veri madenciliği,"Veri madenciliği, büyük ölçekli veriler arasından faydalı bilgiye ulaşma, bilgiyi madenleme işidir. Büyük veri yığınları içerisinden gelecekle ilgili tahminde bulunabilmemizi sağlayabilecek bağıntıların bilgisayar programı kullanarak aranması olarak da tanımlanabilir.

Kavram
Veri madenciliği deyimi yanlış kullanılan bir kavram olabileceğinden buna eş değer başka kullanımlar da literatüre geçmiştir. Veritabanlarında bilgi madenciliği (İng. knowledge mining in databases), bilgi çıkarımı (İng. knowledge extraction), veri ve örüntü analizi (İng. data/pattern analysis), veri arkeolojisi gibi. 
Bu terimler arasında ""Veritabanlarında Bilgi Keşfi"" (İng. VBK - knowledge discovery in databases - KDD) en yaygınıdır. Alternatif olarak veri madenciliği aslında bilgi keşfi sürecinin bir parçası şeklinde kabul görmektedir. Bu adımlar:

Veri temizleme (gürültülü ve tutarsız verileri çıkarmak)
Veri bütünleştirme (birçok veri kaynağını birleştirebilmek)
Veri seçme (yapılacak olan analizle ilgili olan verileri belirlemek)
Veri dönüşümü (verinin veri madenciliği tekniğinden kullanılabilecek hale dönüşümünü gerçekleştirmek)
Veri madenciliği (veri örüntülerini yakalayabilmek için akıllı metotları uygulamak)
Örüntü değerlendirme (bazı ölçümlere göre elde edilmiş bilgiyi temsil eden ilginç örüntüleri tanımlamak)
Bilgi sunumu (mâdenciliği yapılmış olan elde edilmiş bilginin kullanıcıya sunumunu gerçekleştirmek).

Yöntem
Veri madenciliği adımı, kullanıcı ve bilgi tabanıyla etkileşim halindedir. İlginç örüntüler kullanıcıya gösterilir ve bunun ötesinde istenirse bilgi tabanına da kaydedilebilir. Buna göre, veri madenciliği işlemi, gizli kalmış örüntüler bulunana kadar devam eder.
Bir veri madenciliği sistemi, aşağıdaki temel bileşenlere sahiptir:

Veritabanı, veri ambarı ve diğer depolama teknikleri
Veritabanı ya da Veri Ambarı Sunucusu
Bilgi Tabanı
Veri Madenciliği Motoru
Örüntü Değerlendirme
Kullanıcı Arayüzü
Veri madenciliği, eldeki verilerden üstü kapalı, çok net olmayan, önceden bilinmeyen ancak potansiyel olarak kullanışlı bilginin çıkarılmasıdır. Bu da; kümeleme, veri özetleme, değişikliklerin analizi, sapmaların tespiti gibi belirli sayıda teknik yaklaşımları içerir.
Başka bir deyişle, veri madenciliği, verilerin içerisindeki desenlerin, ilişkilerin, değişimlerin, düzensizliklerin, kuralların ve istatistiksel olarak önemli olan yapıların yarı otomatik olarak keşfedilmesidir.
Temel olarak veri madenciliği, veri setleri arasındaki desenlerin ya da düzenin, verinin analizi ve yazılım tekniklerinin kullanılmasıyla ilgilidir. Veriler arasındaki ilişkiyi, kuralları ve özellikleri belirlemekten bilgisayar sorumludur. Amaç, daha önceden fark edilmemiş veri desenlerini tespit edebilmektir.
Veri madenciliğini istatistiksel bir yöntemler serisi olarak görmek mümkün olabilir. Ancak veri madenciliği, geleneksel istatistikten birkaç yönde farklılık gösterir. Veri madenciliğinde amaç, kolaylıkla mantıksal kurallara ya da görsel sunumlara çevrilebilecek nitel modellerin çıkarılmasıdır. Bu bağlamda, veri madenciliği insan merkezlidir ve bazen insan – bilgisayar arayüzü birleştirilir.
Veri madenciliği sahası, istatistik, makine bilgisi, veritabanları ve yüksek performanslı işlem gibi temelleri de içerir.

Veri sınıflandırma
Veri madenciliğinde üzerinde çalışılan veri farklı terimlerle sınıflandırılır. Geniş veri tek bir iş istasyonunun belleğine sığamayacak kadar büyük veri kümelerini ifade etmektedir. Yüksek hacimli veri ise, tek bir iş istasyonundaki ya da bir grup iş istasyonundaki disklere sığamayacak kadar fazla veri anlamındadır. Dağıtık veri ise, farklı coğrafi konumlarda bulunan verileri anlatır.

Ayrıca bakınız
Veritabanı
Makine öğrenimi


== Kaynakça =="
Veri analizi,"Veri analizi, faydalı bilgiler bulma, sonuçları bilgilendirme ve karar vermeyi destekleme amacı ile verileri inceleme, temizleme, dönüştürme ve modelleme işlemidir. Veri analizi, farklı isimler altında çeşitli teknikleri bünyesinde bulunduran, işletme, bilim ve sosyal bilimler gibi farklı alanlarda kullanılan çok çeşitli görünüş ve yaklaşımlara sahiptir. Günümüzün iş dünyasında, veri analizi karar verme  işlemlerinin daha bilimsel hale getirilmesine ve işletmelerin daha etkin çalışmalarına yardımcı olmaktadır.
Veri madenciliği, modellemeye ve tamamen tanımlayıcı amaçtan ziyade tahmin edici bilgi keşiflerine odaklanan özel bir veri analiz tekniğidir. Halbuki iş zekası, ağırlıklı olarak iş bilgilerine odaklanan ve büyük ölçüde veri birleştirmeye dayanan veri analizini kapsar. İstatistiksel uygulamalarda veri analizi, tanımlayıcı istatistikler, keşifsel veri analizi (EDA) ve doğrulayıcı veri analizi (CDA)  gibi kısımlara ayrılabilir. EDA, verilerdeki yeni özellikleri keşfetmeye odaklanırken, CDA mevcut hipotezleri onaylamaya veya tahrif etmeye odaklanır. Tahmine dayalı analitik, tahmin veya sınıflandırma için istatistiksel modellerin uygulanmasına odaklanırken, metin analizi, yapılandırılmamış veri türü olan metin kaynaklarından bilgi çıkarmak ve sınıflandırmak için istatistiksel, dilsel ve yapısal teknikler uygular. Yukarıdakilerin hepsi veri analizi çeşitleridir.
Veri entegrasyonu veri analizinin öncüsüdür ve veri analizi, veri görselleştirme ve veri yayma ile yakından bağlantılıdır. Veri analizi terimi bazen veri modelleme ile eş anlamlı olarak kullanılır.

Ayrıca bakınız
Konu modelleme


== Kaynakça =="
Optimizasyon,"Matematikte matematiksel programlama, eniyileme ya da optimizasyon terimi; bir gerçel fonksiyonu minimize ya da maksimize etmek amacı ile gerçek ya da tam sayı değerlerini tanımlı bir aralıkta seçip fonksiyona yerleştirerek sistematik olarak bir problemi incelemek ya da çözmek işlemlerini ifade eder. Örneğin bu problem şöyle olabilir:
Verilen: f fonksiyonu: A'dan R ye tanımlı. (R:Reel Sayılar)
Aranan : A'da öyle bir x0 var mı ki; tüm x değerleri için f(x0)≤ f(x)ifadesini sağlasın (""minimizasyon"") veya f(x0)≥ f(x) ifadesini sağlasın (""maksimizasyon"").
Böylesi bir formulasyona optimizasyon problemi ya da matematiksel programlama problemi denir (terimin bilgisayar programlama ile direkt bir ilgisi yoktur, ama yine de lineer programlamada kullanılan bir ifadedir). Pek çok gerçek ve teorik problemler bu genel çerçevede modellenebilir.Bu teknik kullanılarak formüle edilen problemlere fizik bilminin ilgi alanından bir örnek verilecek olursa, bilgisayar monitörlerinin enerji minimizasyonundan söz edilebilir. O halde, yukarıdaki f fonksiyonu modellenen sistemdeki enerjiyi temsil edecekti.
Bu tür problemlerde ""A kümesi"" genellikle, bir takım daraltıcı kısıtlar, eşitlikler ve eşitsizlikler ile yerine verilecek(denenecek) değerleri sağlayan öklidyen uzayın (Rn) bir alt kümesidir. f fonksiyonundaki A'nın tanım aralığına ""arama uzayı"", A'nın alacağı değerlerin kümesine ise çözüm adayları ya da olası çözümler denir.
f fonksiyouna objektif (nesnel) ya da paha(maliyet) fonksiyonu denir. İstenilen objeyi minimize ya da maksimize eden(amaca göre) olası A çözümüne ise ""optimal çözüm"" denir.
Genellikle, problemin olası çözümü ve objektif fonksiyonu dışbükeylik göstermez, birden çok yerel ""minimum"" ve ""maksimum"" noktalarına rastlanabilir.
x* da bulunan ve tüm x'ler için δ > 0 iken

  
    
      
        ‖
        
          x
        
        −
        
          
            x
          
          
            ∗
          
        
        ‖
        ≤
        δ
        ;
        
      
    
    {\displaystyle \|\mathbf {x} -\mathbf {x} ^{*}\|\leq \delta ;\,}
  
 epsilon
ifadesi ile

  
    
      
        f
        (
        
          
            x
          
          
            ∗
          
        
        )
        ≤
        f
        (
        
          x
        
        )
      
    
    {\displaystyle f(\mathbf {x} ^{*})\leq f(\mathbf {x} )}
  

sağlanıyorsa; x* civarında bir yerlerde fonksiyonun tüm değerlerinin, bu noktadaki değerden büyük veya o'na eşit olduğu söylenebilir, o halde bu nokta bir ""Yerel Minimum"" noktasıdır. (Yerel Maksimum da benzer şekilde ifade edilebilir).
Dışbükey olmayan problemlerin çözümünde pek çok algoritma kullanılmasına rağmen (çoğunlukla ticari amaca yönelik çözüm üreten algoritmalar) yine de yerel optimal noktalar ve gerçek optimal noktaral arasındaki farkların ayırt ve tespit edilmesinde yetersiz kalınmakta ve orijinal probleme bir adım geriden yaklaşılmaktadır. Deterministik algoritmaların gelişimi ile ilgilenip, yakınsayan ve dışbükey olmayan ifadeleri -sınırlı bir zamanda- gerçek bir optimal ifadeye ayrıştırabilen uygulamalı matematik ve nümerik analiz branşlarına global optimizasyon denir.

Başlıca alt dalları
Doğrusal Programlama : f objektif fonksiyonun doğrusal olduğu ve A kümesinin yalnızca doğrusal eşitlik veya eşitsizlikler ile ulaşılabilir olduğu durumlarda bu isim kullanılır. Böylesi bir A kümesine, sınırlandırılmamış ise polihedron, sınırlı ile politop denir.
Tamsayı Programlama : Doğrusal programların bir ya da daha çok değişkeni tam sayı ile ifade edildiği durumlarda kullanılır.
Kuadratik Programlama: A değeri doğrusal eşitlik veya eşitsizlikler ile gösterilmek kaidesi ile; objektif fonksiyonun kuadratik değerler almasına müsaade eder.
Doğrusal olmayan Programlama: Objektif fonksiyon ya da kısıtların doğrusal olmadığı genel durumları inceler.
Konveks Programlama objektif fonksiyon ve kısıtın bükey bir fonksiyon biçiminde ifade edilebileceği durumları inceler. Non-Lineer programlaman özel bir hâli olarak düşünülebilir.
İkinci Derece Koni Programlama (SOCP).
Yarı-Belirli Programlama (SDP) Değişkenleri yarı tanımlı olacak şekilde, Bükey optimizasyonun bir alt dalıdır. Lineer ve Konveks Kuadratik programlaman genel bir hâlidir.
Stokastik Programlama: Kısıt ve parametrelerin rastgele değişkenler'e bağlı olduğu durumları inceler.
Robust Optimizasyonu/Programlama: Optimizasyon problemindeki belirsiz bilgiyi yakalamaya çalışan bir tür stokastik programlamatır.Stokastik programlama gibi, belirsiz bilgiye dayalı olarak çalışmaz ancak problemi girilen bilginin rastgele etkinliği ve şans temelinden kopmadan çözemez.
Tümleşik optimizasyon : Olası çözüm kümesi içeren problemlerin mümkün ise daha kolay çözülebilir bir şekle indirgenmesi esasına dayanır.
Sonsuz-boyutlu optimizasyon : Olası çözüm kümesinin (Fonksiyonlar kümesi gibi) sonsuz boyutlu bir uzaya ait olup, olmadığını araştırır.
Kısıt Sağlaması f fonskiyonun sabit olup, olamayacağını araştırır (Bu metot yapay zekâ alanında kullanılır, özellikle de otomatikleşmiş ilişkilendirme konusunda yardımcı olur).Ayırıcı Programlama kullanularak, seçilen ve önem adledilen (en az) bir kısıtın sağlanması temin edilmesi esasına dayanır.
Yörünge optimizasyonu: Hava ve uzay araçları için kullanılan özel bir optimizasyon türüdür.
Altdallara göre farklılık gösterecek şekilde, çeşitli teknikler dizayn edilmiştir:

Değişkenler hesabı Objektif fonksiyonun zaman aralıklarından seçilen değişik noktalara nasıl reaksiyon verdiğinin incelenmesi ile kullanılan yöntemdir.
Optimal Kontrol Teorisi değişkenler hesabının çeşitli genellemelerinin toplanmış halidir.
Dinamik programlama Büyük parçaların daha küçük boyutlara indirgenmesi optimizasyon stratejisini yöneten metottur.Bu tür alt problemler ile ilgili olan eşitliklere Bellman eşitliği denir.

Teknikler
İki kez diferansiyeli alınabilen fonksiyonlar için, kısıt bulundurmayan problemler objektif fonksiyonun gradyan'ının sıfır'a eşit olduğu noktaların (istasyon noktaların) yeri tespit edilip, Hesse matrisi ile her noktanın sınıfı belirlenerek çözülebilir.Eğer Hesse pozitif tanımlı ise bu nokta ""Yerel Minimum"", negatif tanımlı ise ""Yerel Maksimum""'dur.Şayet tanımsız ise de bir tür saddle point olduğu söylenebilir.
Ancak, her zaman türev almak olası değildir.Objektif fonksiyonun düzgünlüğüne göre metotların ana sınıflandırması şöyle yapılabilir:

Tümleşik metotlar
Türeve-serbest metotlar
Birinci derece metotlar
İkinci derece metotlar
Bazı metotlar özel isimleri ile de yukarıdaki dört gruptan birine denk gelecek şekilde listenebilir:

Gradyan iniş ya da Dik iniş metodu.
Nelder-Mead metodu ya da the Amoeba metodu.
Alt-gradyan metodu - Gradyan metodunun, gradyan bulunmayan durumlar için kullanılan hali.
Tekyönlü metot
Elipsoid metot
Yığın metodu
Newton metodu
Kazi-Newton metodu
Birleşik gradyan metodu
Hat araması - tek boyulu optimizasyon için kullanılan bir teknik, genellikle başka bir tekniğe yardımcı olması için kullanılır.
Kısıt problemleri genellikle Lagrange Çarpanı ile kısıttan bağımsız bir forma getirilir.
Birkaç popüler metot daha:

Tepe tırmanışı
Benzetimli tavlama
Kuantum benzetimli tavlama
Tabu araması
Kiriş araması
Genetik algoritmalar
Karınca sürüsü optimizasyonu
Evrim stratejisi
Stokastik tünel
Diferansiyel evrim
Sürü parçacıkları
Armoni araması
Arı algoritması

Kullanım alanları
Yapı-Araç İskeleti dinamiği'ne ilişkin problemler sıklık ile matematiksel programlama teknikleri gerektirmektedir.Yapı-Araç İskeleti, manifold ile kısıtlanmış bir basit diferansiyel denklem'in çözümüne ihtiyaç duyan bir yönelim olarak değerlendirilebilir.Bu durumda kısıtlar non-lineer olmayan gemotetrik çeşitliliktedir, öneğin ""bu iki nokta daima temas etmeli"", ""bu alan diğerine etki etmemeli"" ya da ""bu nokta her zaman bu eğri üzerinde olmalı"" gibi.Ayrıca temas halindeki kuvvetlere ilişkin problemler de lineer uyumluluk çatısı altında çözüldüğünden, buna da bir tür QP (Kuadratir Programlama) Problemi gözüyle bakılabilir.
Pek çok dizayn problemi de optimizasyon programları ile çözülmektedir.Bu tür uygulamalara dizayn optimizasyonu denir.Bu alanda bilinen ve büyümekte olan bir alt kol çok disiplinli dizayn optimizasyonu'dur.Bu tür, pek çok problemde kullanışlı olduğu gibi aynı zamanda da uzay mühendisliği sahasına uyarlanabilmektedir.
Ekonomi de matematiksel programlamaya ağır bir bağımlılık duyar.Mikroiktisat'da sık karşılaşılan bir problem olan marjinal fayda ve bundan kaynaklanan ikilik olan harcamaları minimize etme problemi iktisadî bir optimizasyon problemidir. 
Tüketiciler ve firmalar fayda/kar oranlarını maksimize etmek durumundadırlar.Ticaret teorisi de milletler arası ticari ortaklığın izahında optimizasyona sık sık başvurur.
Sabit genel giderli zaman maliyet problemleride önemli bir optimizasyon problemidir. Özellikle inşaat ve endüstri mühendisliğinde bu problemle sıkça karşılaşılır. Doğrusal programlama, sezgisel ve üst sezgisel yöntemler bu problem türünün optimizasyonu için kullanılmaktadır.
Optimizasyon tekniklerinin sıkça kullanıldığı bir diğer alan da operasyon araştırması'dır.

Tarihçe
Dik İniş adıyla bilinen ilk optimizasyon tekniğinin tarihi Gauss'a dek uzanır.
Tarihi olarak, 1940'larda George Dantzig tarafından ortaya atılan lineer programlama kuramı en yaşlı optimizasyon terimidir. Programlama terimi bu bağlamda Bilgisayar Programcılığı'nı ifade etmez.Program teriminin kullanımı ABD Ordusunun, kendi içtimai ve lojistik takvimini belirlemede konteyner kullandığı ""program"" terimi ile ilişkilidir. (Daha sonra ise ""program"" terimi devlet bütçesinin düzenlenmesinde kullanılmış ve günümüzde de yüksek teknolojik araştırmaların sahasına da geçmiştir.)
Optimizasyon Alanına Katkı Sağlayan Diğer Önemli Matematikçiler:

John von Neumann
Leonid Vitalyevich Kantorovich
Naum Z. Shor
Leonid Khachian
Boris Polyak
Yurii Nesterov
Arkadii Nemirovskii
Michael J. Todd
Richard Bellman
Hoang Tuy

Ayrıca bakınız
Oyun teorisi
Yöneylem araştırması
Bulanık mantık
Dinamik programlama

Problem Çözücüler
NAG Numerical Libraries-NAG kütüphanesi çeşitli problem ve durumlardan oluşan optimizasyona dair geniş bir arşiv sunuyor.http://www.nag.co.uk/optimization/index.asp9 Şubat 2008 tarihinde Wayback Machine sitesinde arşivlendi.
NPSOL - Non-Lineer progralama problemlerinin çözümü için geliştirilmiş bir Fortran paketi.http://www.sbsi-sol-optimize.com/asp/sol_product_npsol.htm10 Mart 2008 tarihinde Wayback Machine sitesinde arşivlendi.
OpenOpt - Pyhton dilinde yazılmış ücretsiz çözücüleri sağlayan bir araç-kutusu
IPOPT - Açık kaynak kodlu ilkel-ikincil dahli metotlar ile çalışan bir NLP çözücü.
KNITRO - Non-Lineet problemler için bir çözücü.
Mathematica
OpenSolver - Açık kaynaklı MS Excel eklentisi olarak çalışan bir çözücü

Kaynakça
Mordecai Avriel (2003). Nonlinear Programming: Analysis and Methods. Dover Publishing. ISBN 0-486-43227-0.
Stephen Boyd and Lieven Vandenberghe (2004). Convex Optimization28 Şubat 2008 tarihinde Wayback Machine sitesinde arşivlendi., Cambridge University Press. ISBN 0-521-83378-7.
Panos Y. Papalambros and Douglass J. Wilde (2000). Principles of Optimal Design : Modeling and Computation30 Ocak 2008 tarihinde Wayback Machine sitesinde arşivlendi., Cambridge University Press. ISBN 0-521-62727-3.
Jorge Nocedal and Stephen J. Wright (2006). Numerical Optimization30 Mayıs 2008 tarihinde Wayback Machine sitesinde arşivlendi., Springer. ISBN 0-387-30303-0.

Dış bağlantılar
NEOS Guide şu anda yerine NEOS Wiki gelmiş durumda.
Mathematical Programming Society7 Mart 2006 tarihinde Wayback Machine sitesinde arşivlendi.
COIN-OR19 Aralık 2008 tarihinde Wayback Machine sitesinde arşivlendi.
Mathematical Programming Glossary28 Mart 2010 tarihinde Wayback Machine sitesinde arşivlendi.
Mathematical optimization
Global optimization28 Aralık 2008 tarihinde Wayback Machine sitesinde arşivlendi.
Optimization Related Links9 Aralık 2008 tarihinde Wayback Machine sitesinde arşivlendi.
Decision Tree for Optimization Software17 Mart 2007 tarihinde Wayback Machine sitesinde arşivlendi. Optimizasyon kaynak kodlarına linkler içeriyor.
Optimization Online27 Eylül 2019 tarihinde Wayback Machine sitesinde arşivlendi.
Modeling languages:

AIMMS23 Temmuz 2019 tarihinde Wayback Machine sitesinde arşivlendi.
AMPL17 Eylül 2009 tarihinde Wayback Machine sitesinde arşivlendi.
GAMS20 Ağustos 2009 tarihinde Wayback Machine sitesinde arşivlendi.
OPL
Solvers:

CONOPT23 Aralık 2008 tarihinde Wayback Machine sitesinde arşivlendi.
JOpt
Moocho - Çok esnek ve açık-kaynak kodlu bir NLP çözücü.
SAS OR6 Aralık 2008 tarihinde Wayback Machine sitesinde arşivlendi.
[1]17 Ocak 2008 tarihinde Wayback Machine sitesinde arşivlendi. Stanford Üniversitesi'nin Sistem Optimizasyon Labarotuarı tarafından sunulan ücretsiz optimizasyon yazılımı.
TANGO Project19 Aralık 2008 tarihinde Wayback Machine sitesinde arşivlendi. Genel Non-Lineer algoritmalar için güvenilir bir yazılım.
SmartDO22 Ocak 2009 tarihinde Wayback Machine sitesinde arşivlendi. - Mühendislik ile ilgili global optimizasyon yazılımı (ticari)
Kütüphaneler:

OOL (Open Optimization library)21 Aralık 2008 tarihinde Wayback Machine sitesinde arşivlendi. -
IOptLib (Investigative Optimization Library)21 Nisan 2008 tarihinde Wayback Machine sitesinde arşivlendi. - (ANSI C).
ALGLIB7 Aralık 2008 tarihinde Wayback Machine sitesinde arşivlendi. Optimizasyon kaynakları. C++, C#, Delphi, Visual Basic."
Algoritma,"Algoritma, belli bir problemi çözmek veya belirli bir amaca ulaşmak için tasarlanan yol. Matematikte ve bilgisayar biliminde bir işi yapmak için tanımlanan, bir başlangıç durumundan başladığında, açıkça belirlenmiş bir son durumunda sonlanan, sonlu işlemler kümesidir. Genellikle bilgisayar programlamada kullanılır ve tüm programlama dillerinin temeli algoritmaya dayanır. Aynı zamanda algoritma tek bir problemi çözecek davranışın, temel işleri yapan komutların veya deyimlerin adım adım ortaya konulmasıdır ve bu adımların sıralamasına dikkat edilmelidir. Bir problem çözülürken algoritmik ve sezgisel (herustic) olmak üzere iki yaklaşım vardır. Algoritmik yaklaşımda da çözüm için olası yöntemlerden en uygun olan seçilir ve yapılması gerekenler adım adım ortaya konulur. Algoritmayı belirtmek için; metinsel olarak düz ifade ve akış diyagramı olmak üzere 2 yöntem kullanılır. Algoritmalar bir programlama dili vasıtasıyla bilgisayarlar tarafından işletilebilirler.
İlk algoritma, el-Hârizmî tarafından ""Hisab el-cebir ve el-mukabala"" kitabında sunulmuştur. Algoritma sözcüğü de el-Hârizmî'nin isminin Avrupalılarca telaffuzundan doğmuştur.

Tarihi
Algoritma sözcüğü, Özbekistan'ın Harezm bölgesindeki Hive kentinde doğmuş olan Ebu Abdullah Muhammed İbn Musa el Harezmi'den gelir. Bu alim 9. yüzyılda cebir alanındaki algoritmik çalışmalarını kitaba dökerek matematiğe çok büyük bir katkı sağlamıştır. ""Hisab el-cebir ve el-mukabala (حساب الجبر و المقابلة)"" kitabı dünyanın ilk cebir kitabı ve aynı zamanda ilk algoritma koleksiyonunu oluşturur. Latince çevirisi Avrupa'da çok ilgi görür. Alimin ismini telaffuz edemeyen Avrupalılar ""algorizm"" sözcüğünü ""Arap sayıları kullanarak aritmetik problemler çözme kuralları"" manasında kullanırlar. Bu sözcük daha sonra ""algoritma""ya dönüşür ve genel kapsamda kullanılır.

Uygulama
Çoğu algoritmalar bilgisayar olarak uygulanmak üzere tasarlanmıştır. Bununla birlikte, başka yöntemlerle de uygulanmaktadır, biyolojik sinir ağı (örneğin insan beyninin hesap yapması veya bir böceğin yemek araması), elektrik devresi veya mekanik cihazlar gibi.
Bilgisayar algoritmasına örnek verelim. Kullanıcının girdiği dört sayının ortalamasını görüntüleyen algoritmayı yazalım:

 A0 --> Başla
 A1 --> Sayaç=0 (Sayaç'ın ilk sayısı 0 olarak başlar.)
 A2 --> Sayı=? : T=T+Sayı (Sayıyı giriniz. T'ye sayıyı ekle ve T'yi göster.)
 A3 --> Sayaç=Sayaç+1 (Sayaç'a 1 ekle ve sayacı göster.)
 A4 --> Sayaç<4 ise A2'ye git. (Eğer sayaç 4'ten küçükse Adım 2'ye git.)
 A5 --> O=T/4 (Ortalama için T değerini 4'e böl)
 A6 --> O'yu göster. (Ortalamayı göster.)
 A7 --> Dur

İkinci dereceden ax² + bx + c = 0 biçiminde bir denklemin tüm köklerini bulmak için algoritma yazalım:

Adım 1: Başla.
Adım 2: a, b, c, D, x1, x2, rp ve ip değişkenlerini tanımla.
Adım 3: Diskriminant değerini hesapla.
D ← b2-4ac
Adım 4: Eğer D≥0
x1 ← (-b+√D) / 2a
x2 ← (-b-√D) / 2a
değerlerini hesapla ve x1,x2 değişkenleri göster.
Eğer D≥0 değilse,
Gerçek kısım(rp) ve sanal kısmını(ip) hesapla.
rp ← b / 2a
ip ← √ (D) / 2a
Adım 5: ""rp + j(ip)"" ve ""rp - j(ip)"" değerlerini göster.
Adım 6: Dur.

Kullanıcı tarafından girilen bir sayının faktöriyel değerini bulmak için bir algoritma yazalım:

Adım 1: Başla.
Adım 2: factorial, i ve n değişkenlerini tanımla.
Adım 3: Değişkenlerin başlangıç değerlerini tanımla.
factorial ← 1
i ← 1
Adım 4: Ekrandan girilen n değerini oku.
Adım 5: (i=n) eşitliği sağlanana kadar tekrarla.
5.1: factorial←factorial*i
5.2: i←i+1
Adım 6: factorial değişkeninin değerini göster.
Adım 7: Dur.

Hukuki Konular
Algoritmalar, tek başlarına, genellikle patent verilebilir değildirler. Amerika Birleşik Devletleri'nde soyut kavramların, sayıların ve işaretlerin yalnızca basit yönlendirmelerinden oluşan bir iddia ""süreç"" oluşturmaz (USPTO 2006) ve bundan dolayı algoritmalar patent verilebilir değildir (Gottschalk v.Benson'da olduğu gibi). Bununla birlikte, algoritmanın pratik uygulamaları zaman zaman patent verilebilirdir. Örneğin, Diamond v.Diehr'da, sentetik kauçuğun muhafaza edilmesine yardımcı olmak için kullanılan basit geri bildirim algoritmasının uygulaması patent verilebilir sayılmıştır. Yazılım patenti son derece tartışmalıdır ve algoritmaları içeren birçok eleştirilmiş patent vardır, özellikle veri sıkıştırma algoritmaları, Unisys' LZW patentinde olduğu gibi.
Ek olarak, bazı kriptografik algoritmaların ihracat kısıtlamaları vardır.

1950'den Sonraki Tarihi
Faaliyetlerin birçoğu algoritmanın tanımının geliştirilmesine yönlendirilmiştir ve aktifliği çevredeki sorunlar nedeniyle, özellikle matematiğin temelleri (özellikle Church-Turing tezi) ve akıl felsefesi (özellikle yapay zeka konusundaki tartışmalar) sebebiyle devam etmiştir.

Algoritmalara eleştirel yaklaşımlar
Algoritmaların kullanımı hayatın her alanında giderek yaygınlaşmaktadır. İş yerlerindeki performans değerlendirmelerinden bankaların kime kredi vereceğine, güvenlik sistemlerinden sosyal medya platformlarındaki önerilere kadar hayatın her alanında etkili olan algoritmalara özellikle de sosyal bilimci akademisyenler çeşitli eleştiriler yöneltmektedir. Algoritmaların teknolojik etkilerinin yanı sıra toplumsal bir güce de sahip oldukları, toplumsal gruplar arasındaki eşitsizlikleri derinleştirdikleri, yeni güç dengeleri oluşturdukları ve farklı otoriteler tarafından toplumun belli kesimlerini baskılamak için kullanıldıklarının altı çizilmektedir.

Önemli algoritma türleri
Arama algoritmaları
Bellek yönetimi algoritmaları
Bilgisayar grafiği algoritmaları
Birleşimsel algoritmalar
Çizge algoritmaları
Evrimsel algoritmalar
Genetik algoritmalar
Kripto algoritmaları veya kriptografik algoritmalar
Kök bulma algoritmaları
Optimizasyon algoritmaları
Sıralama algoritmaları
Veri sıkıştırma algoritmaları

Ayrıca bakınız
Algoritmalar listesi
Algoritmaların tarihsel sıralaması
Algokrasi


== Kaynakça =="
